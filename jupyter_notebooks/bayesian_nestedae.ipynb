{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module, ModuleList, Linear, Tanh, MSELoss, GaussianNLLLoss\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import SGD, Adam, Optimizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device='cpu'\n",
    "dtype=torch.float32\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Custom libraries\n",
    "from utils.custom_utils import set_global_random_seed\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "global_seed = 0\n",
    "set_global_random_seed(global_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the perovskite dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Found Run directory already exists.\n",
      " --> nn directory already exists.\n",
      "Train Dataset shape (444, 16)\n",
      "Train Dataset variables ['all_props', 'bg']\n",
      "Train Dataset variable shapes {'all_props': (444, 15), 'bg': (444, 1)}\n"
     ]
    }
   ],
   "source": [
    "inputs_dir = 'inputs_perov_data'\n",
    "run_dir = 'bayesian_nestedae'\n",
    "nn_save_dir = 'ae1'\n",
    "nn = 1\n",
    "mode = 'train'\n",
    "kfolds = 0\n",
    "X_variable_name = 'all_props' \n",
    "y_variable_name = 'bg'\n",
    "\n",
    "# Run the command to preprocess the data\n",
    "os.system('rm -rf inputs')\n",
    "os.system(f'cp -rf {inputs_dir} inputs')\n",
    "os.system(f'python3 preprocess_data.py --run_dir {run_dir} --nn_save_dir {nn_save_dir} --nn {nn} --mode {mode} --kfolds {kfolds} &')\n",
    "time.sleep(5)\n",
    "os.system('rm -rf inputs')\n",
    "\n",
    "train_dataset_path = f'../runs/{run_dir}/{nn_save_dir}/datasets/train_dataset.pt'\n",
    "val_dataset_path = f'../runs/{run_dir}/{nn_save_dir}/datasets/val_dataset.pt'\n",
    "\n",
    "train_dataset = torch.load(train_dataset_path)\n",
    "val_dataset = torch.load(val_dataset_path)\n",
    "\n",
    "print(f'Train Dataset shape {train_dataset.shape}')\n",
    "print(f'Train Dataset variables {train_dataset.variable_names}')\n",
    "print(f'Train Dataset variable shapes {train_dataset.variable_shapes}')\n",
    "\n",
    "train_X_data = train_dataset.dataset['all_props']\n",
    "train_y_data = train_dataset.dataset['bg']\n",
    "val_X_data = val_dataset.dataset['all_props']\n",
    "val_y_data = val_dataset.dataset['bg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesLinear(Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, prior_mu: float = None, prior_sigma: float = None,  bias: bool = True) -> None:\n",
    "        super(BayesLinear, self).__init__()\n",
    "        # User provides the prior mean and prior log sigma for describing the prior distribution to sample the weights from\n",
    "        self.prior_mu = prior_mu\n",
    "        self.prior_sigma = prior_sigma\n",
    "        self.prior_log_sigma = math.log(prior_sigma)\n",
    "        # self.prior_log_sigma = math.log(prior_sigma)\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.bias = bias\n",
    "        self.weight_mu = Parameter(torch.empty(out_dim, in_dim))    \n",
    "        self.weight_log_sigma = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        if self.bias:\n",
    "            self.bias_mu = Parameter(torch.Tensor(out_dim))\n",
    "            self.bias_log_sigma = Parameter(torch.Tensor(out_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias_mu', None)\n",
    "            self.register_parameter('bias_log_sigma', None)\n",
    "        # Reset the parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Uniform Initialization\n",
    "        # From : https://github.com/JavierAntoran/Bayesian-Neural-Networks\n",
    "        scale1 = (2/math.sqrt(self.weight_mu.size(1)))**0.5\n",
    "        # From : https://github.com/Harry24k/bayesian-neural-network-pytorch\n",
    "        scale2 = 1. / math.sqrt(self.weight_mu.size(1))        \n",
    "        self.weight_mu.data.uniform_(-scale2, scale2)\n",
    "        self.weight_log_sigma.data.uniform_(-3, -1)\n",
    "\n",
    "        if self.bias:\n",
    "            self.bias_mu.data.uniform_(-scale2, scale2)\n",
    "            self.bias_log_sigma.data.uniform_(-3, -1)\n",
    "\n",
    "    def sample_parameter(self, mu, log_sigma):\n",
    "        eps = torch.randn_like(log_sigma)\n",
    "        # From : https://github.com/JavierAntoran/Bayesian-Neural-Networks\n",
    "        # std = torch.log(1 + torch.exp(rho))\n",
    "        std = torch.exp(log_sigma)\n",
    "        sample = mu + std*eps\n",
    "        return sample\n",
    "\n",
    "    def forward(self, input: torch.Tensor, sample: bool = True) -> torch.Tensor:\n",
    "        if sample :\n",
    "            weight_sample = self.sample_parameter(self.weight_mu, self.weight_log_sigma)\n",
    "            if self.bias:\n",
    "                bias_sample = self.sample_parameter(self.bias_mu, self.bias_log_sigma)\n",
    "                return F.linear(input, weight_sample, bias_sample)\n",
    "            else:\n",
    "                return F.linear(input, weight_sample)\n",
    "        else:\n",
    "            if self.bias:\n",
    "                return F.linear(input, self.weight_mu, self.bias_mu)\n",
    "            else:\n",
    "                return F.linear(input, self.weight_mu)\n",
    "            \n",
    "    def sample_layer(self, num_samples):\n",
    "        weight_samples = []\n",
    "        for i in range(num_samples):\n",
    "            weight_sample = self.sample_parameter(self.weight_mu, self.weight_log_sigma)\n",
    "            weight_samples += weight_sample.view(-1).cpu().data.numpy().tolist()\n",
    "        if self.bias:\n",
    "            bias_samples = []\n",
    "            for i in range(num_samples):\n",
    "                bias_sample = self.sample_parameter(self.bias_mu, self.bias_log_sigma)\n",
    "                bias_samples += bias_sample.view(-1).cpu().data.numpy().tolist()\n",
    "            return weight_samples, bias_samples\n",
    "        else:\n",
    "            return weight_samples\n",
    "        \n",
    "class AppendLogVar(Module):\n",
    "    def __init__(self, noise=1e-3, dtype=torch.float32, device='cpu', *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # The log variance is the learnable parameter in this layer\n",
    "        # self.log_var is a tensor with the following shape [[0]]\n",
    "        self.log_var = torch.nn.Parameter(torch.empty((1, 1), dtype=dtype, device=device))\n",
    "        # self.log_var is initialized with log(noise) with the following shape [[log(noise)]]\n",
    "        torch.nn.init.constant_(self.log_var, val=math.log(noise))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [[sample1], [sample2], ..] -> [[sample1, log(noise)], [sample2, log(noise)], ..]\n",
    "        return torch.cat((x, self.log_var * torch.ones_like(x)), dim=1)\n",
    "\n",
    "class BNN(Module):\n",
    "    def __init__(self, in_dim, out_dim, \n",
    "                 hidden_layers, hidden_activations,\n",
    "                 out_activation,\n",
    "                 bias=True,\n",
    "                 model_type=1,\n",
    "                 noise_var=0.1,\n",
    "                 dtype=torch.float32,\n",
    "                 device='cpu'):\n",
    "        super(BNN, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_activations = hidden_activations\n",
    "        self.out_activation = out_activation\n",
    "        self.layer_stack = torch.nn.Sequential()\n",
    "        self.model_type = model_type\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.noise_var = noise_var\n",
    "        self.bias = bias\n",
    "        if not (self.model_type == 1 or self.model_type == 2 or self.model_type == 3):\n",
    "            raise ValueError('Provided model_type {model}. model_type can only be 1, 2 or 3.')\n",
    "        \n",
    "        if self.hidden_layers: # Output + Hidden layer model\n",
    "            for i, hidden_dim, hidden_act in enumerate(zip(self.hidden_layers, self.hidden_activations)):\n",
    "                if i == 0:\n",
    "                    if self.model_type == 1:\n",
    "                        self.layer_stack.append(BayesLinear(self.in_dim, hidden_dim, 0, 0.1))\n",
    "                    else:\n",
    "                        self.layer_stack.append(torch.nn.Linear(self.in_dim, hidden_dim, bias=self.bias, dtype=self.dtype, device=self.device))\n",
    "                    self.layer_stack.append(self.hidden_activation)\n",
    "                elif i == len(self.hidden_layers) - 1:\n",
    "                    if self.model_type == 1:\n",
    "                        self.layer_stack.append(BayesLinear(hidden_dim, self.out_dim, 0, 0.1))\n",
    "                        if self.out_activation:\n",
    "                            self.layer_stack.append(self.out_activation)\n",
    "                    else:\n",
    "                        self.layer_stack.append(torch.nn.Linear(hidden_dim, self.out_dim, bias=self.bias, dtype=self.dtype, device=self.device))\n",
    "                        if self.out_activation:\n",
    "                            self.layer_stack.append(self.out_activation)\n",
    "                        if self.model_type == 2:\n",
    "                            self.layer_stack.append(AppendLogVar(noise=self.noise_var))\n",
    "                else:\n",
    "                    if self.model_type == 1:\n",
    "                        self.layer_stack.append(BayesLinear(hidden_dim, hidden_dim, 0, 0.1))\n",
    "                    else:\n",
    "                        self.layer_stack.append(torch.nn.Linear(hidden_dim, hidden_dim, bias=self.bias, dtype=self.dtype, device=self.device))\n",
    "                    self.layer_stack.append(self.hidden_activation)\n",
    "        else: # Only output layer model\n",
    "            if self.model_type == 1:\n",
    "                self.layer_stack.append(BayesLinear(self.in_dim, self.out_dim, 0, 0.1))\n",
    "                if self.out_activation:\n",
    "                    self.layer_stack.append(self.out_activation)\n",
    "            else:\n",
    "                self.layer_stack.append(torch.nn.Linear(self.in_dim, self.out_dim, bias=self.bias, dtype=self.dtype, device=self.device))\n",
    "                if self.out_activation:\n",
    "                    self.layer_stack.append(self.out_activation)\n",
    "                if self.model_type == 2:\n",
    "                    self.layer_stack.append(AppendLogVar(noise=self.noise_var))\n",
    "            \n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layer_stack(X)\n",
    "    \n",
    "def kl_loss_fn(model, reduction='mean'):\n",
    "\n",
    "    def kl_div(mu1, log_sigma1, mu2, log_sigma2):\n",
    "        \"\"\"\n",
    "            Gets the KL divergence between two gaussian distributions\n",
    "        \"\"\"\n",
    "        return (log_sigma2 - log_sigma1).sum() + 0.5*((torch.exp(log_sigma1)**2)/(math.exp(log_sigma2)**2)).sum() + 0.5*(((mu1 - mu2)**2)/(math.exp(log_sigma2)**2)).sum() - 0.5*log_sigma1.numel()\n",
    "    \n",
    "    kl_sum = torch.tensor(0.0, dtype=torch.float32)\n",
    "    n = torch.tensor(0.0, dtype=torch.float32)\n",
    "    for layer in model.layer_stack:\n",
    "        if isinstance(layer, BayesLinear):\n",
    "            kl_sum += kl_div(layer.weight_mu, layer.weight_log_sigma, layer.prior_mu, layer.prior_log_sigma)\n",
    "            n += len(layer.weight_mu.view(-1))\n",
    "            if layer.bias:\n",
    "                kl_sum += kl_div(layer.weight_mu, layer.weight_log_sigma, layer.prior_mu, layer.prior_log_sigma)\n",
    "                n += len(layer.bias_mu.view(-1))\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        return kl_sum/n\n",
    "    elif reduction == 'sum':\n",
    "        return kl_sum\n",
    "    else:\n",
    "        raise ValueError(reduction + \" is not valid\")\n",
    "        \n",
    "def train_loop(dataloader, encoder, decoder, predictor, loss_fn_list, loss_wts, loss_names, optimizer, scheduler=None, print_every=None):\n",
    "    \"\"\"\n",
    "        num_burn_in_steps : for use with BNN trained with SGHMC\n",
    "        keep_every : Sa\n",
    "    \"\"\"\n",
    "    # Set model into training mode\n",
    "    encoder.train()\n",
    "    predictor.train()\n",
    "    decoder.train()\n",
    "    dataset_size = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    num_batches = dataset_size/batch_size\n",
    "    num_losses = len(loss_fn_list)\n",
    "    loss_per_batch_list = [0.0]*num_losses\n",
    "    loss_per_epoch_list = [0.0]*num_losses\n",
    "\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        X, y = data\n",
    "\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        l = encoder(X)\n",
    "        y_pred = predictor(l)\n",
    "        X_pred = decoder(l)\n",
    "\n",
    "        # This stores all the running loss values\n",
    "        loss_list = [torch.tensor(0.0, dtype=torch.float32)]*num_losses\n",
    "        total_loss = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "        if predictor.model_type == 2:\n",
    "\n",
    "        # Calculate prediction losses\n",
    "        for i, (loss_name, loss_fn) in enumerate(zip(loss_names, loss_fn_list)):\n",
    "            if loss_name == 'kl':\n",
    "                loss_list[i] = loss_fn(model, reduction='mean')\n",
    "            elif loss_name == 'neg_log_joint':\n",
    "                if model.model_type == 2:\n",
    "                    gnll, neg_log_var_prior, neg_log_params_prior = loss_fn(pred, y, model.parameters())\n",
    "                    loss_list[i] = gnll + neg_log_var_prior + neg_log_params_prior\n",
    "                else:\n",
    "                    gnll, neg_log_params_prior = loss_fn(pred, y, model.parameters())\n",
    "                    # print(gnll, neg_log_params_prior)\n",
    "                    # print(type(gnll), type(neg_log_params_prior))\n",
    "                    loss_list[i] = gnll + neg_log_params_prior\n",
    "            loss_per_batch_list[i] += loss_list[i].item()\n",
    "        \n",
    "        # print('Model parameters :')\n",
    "        # for param in model.parameters():\n",
    "        #     print(param)\n",
    "\n",
    "        for loss_wt, loss in zip(loss_wts, loss_list):\n",
    "            total_loss += loss_wt*loss\n",
    "\n",
    "        # Calculate the gradients for each trainable param\n",
    "        total_loss.backward()\n",
    "        # Update the trainable params\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if print_every is not None:\n",
    "            if batch % print_every == 0:\n",
    "                current = batch*batch_size + len(X)\n",
    "                for loss_name, loss in zip(loss_names, loss_list):\n",
    "                    print(f' Train {loss_name} @ {current}/{dataset_size} is {loss.item()}. ')\n",
    "                print(f' Train Total loss @ {current}/{dataset_size} is {total_loss.item()}. ')\n",
    "\n",
    "    # Average the stats across all batches to get stat for 1 epoch\n",
    "    for i, loss_per_batch in enumerate(loss_per_batch_list):\n",
    "        loss_per_epoch_list[i] = loss_per_batch/num_batches\n",
    "\n",
    "    return loss_per_epoch_list, [gnll.item()], [neg_log_params_prior.item()]\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn_list, loss_wts, loss_names):\n",
    "    # Set model into evaluation mode\n",
    "    model.eval()\n",
    "    dataset_size = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    num_batches = dataset_size/batch_size\n",
    "    num_losses = len(loss_fn_list)\n",
    "    loss_per_batch_list = [0.0]*num_losses\n",
    "    loss_per_epoch_list = [0.0]*num_losses\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            X, y = data\n",
    "\n",
    "            pred = model(X)\n",
    "\n",
    "            loss_list = [torch.tensor(0.0, dtype=torch.float32)]*num_losses\n",
    "            total_loss = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "            for i, (loss_name, loss_fn) in enumerate(zip(loss_names, loss_fn_list)):\n",
    "                if loss_name == 'kl':\n",
    "                    loss_list[i] = loss_fn(model, reduction='mean')\n",
    "                elif loss_name == 'neg_log_joint':\n",
    "                    if model.model_type == 2:\n",
    "                        gnll, neg_log_var_prior, neg_log_params_prior = loss_fn(pred, y, model.parameters())\n",
    "                        loss_list[i] = gnll + neg_log_var_prior + neg_log_params_prior\n",
    "                    else:\n",
    "                        gnll, neg_log_params_prior = loss_fn(pred, y, model.parameters())\n",
    "                        loss_list[i] = gnll + neg_log_params_prior\n",
    "                else:\n",
    "                    loss_list[i] = loss_fn(pred, y)\n",
    "                loss_per_batch_list[i] += loss_list[i].item()\n",
    "        \n",
    "            for loss_wt, loss in zip(loss_wts, loss_list):\n",
    "                total_loss += loss_wt*loss\n",
    "    \n",
    "    for i, loss_per_batch in enumerate(loss_per_batch_list):\n",
    "        loss_per_epoch_list[i] = loss_per_batch/num_batches\n",
    "\n",
    "    return loss_per_epoch_list, [gnll.item()], [neg_log_params_prior.item()]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print('Testing Functions ... ')\n",
    "    bnn = BNN(in_dim=10, out_dim=10, hidden_layers=1, hidden_dim=10, hidden_activation=torch.nn.ReLU(inplace=True))\n",
    "    train_random_input_tensor = torch.randn((10, 10))\n",
    "    train_random_output_tensor = torch.randn((10, 10))\n",
    "\n",
    "    test_random_input_tensor = torch.randn((5, 10))\n",
    "    test_random_output_tensor = torch.randn((5, 10))\n",
    "\n",
    "    print('Output from NN :')\n",
    "    print(bnn(train_random_input_tensor))\n",
    "    print('\\n')\n",
    "    \n",
    "    # print(next(iter(dataloader)))\n",
    "    # print('\\n')\n",
    "    # print(dataloader.dataset)\n",
    "\n",
    "    # # Prior mu and rho are not defined\n",
    "    # kl_loss_fn(bnn)\n",
    "    # for param in bnn.parameters():\n",
    "    #     print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bayesian Neural Networks - Hamiltonian Monte Carlo\n",
    "* Code mainly adopted from here : https://github.com/automl/pybnn/tree/master\n",
    "* Original paper here : http://arxiv.org/abs/1502.05700\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "class NegativeLogJoint(torch.nn.Module):\n",
    "    reduction:str\n",
    "    def __init__(self, model_type, params_var=10.0, noise_var=0.1, reduction:str='mean', dtype=torch.float32, device='cpu') -> None:\n",
    "        super(NegativeLogJoint, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.params_var = params_var\n",
    "        self.noise_var = noise_var\n",
    "        self.reduction = reduction\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        if not (self.model_type == 2 or self.model_type == 3):\n",
    "            raise ValueError(f'Provided likelihood_type is {self.model_type}. Should be 2 or 3.')\n",
    "\n",
    "    def gaussian_negative_log_likelihood(self,\n",
    "                                         pred: torch.tensor,\n",
    "                                         target: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "            This compute the log of the liklihood distribution.\n",
    "            Here we use the gaussian distribution for the likelihood function\n",
    "        \"\"\"\n",
    "        # Noise var is learned from data\n",
    "        if self.model_type == 2: \n",
    "            pred_mean = pred[:,0].view_as(target)\n",
    "            pred_log_var = pred[:,1].view_as(target)\n",
    "            log_likelihood = 0.5*pred_log_var + 0.5*math.log(2*math.pi) + ((target - pred_mean)**2)/(2*(torch.exp(pred_log_var) + 1e-16))\n",
    "        # Fixed noise var\n",
    "        else:\n",
    "            pred_mean = pred.view_as(target)\n",
    "            pred_log_var = torch.log(torch.tensor(self.noise_var))\n",
    "            log_likelihood = 0.5*pred_log_var + 0.5*torch.log(torch.tensor(2*math.pi)) + ((target - pred_mean)**2)/(2*(torch.exp(pred_log_var) + 1e-16))\n",
    "            # print(f'MSE = {(target - pred_mean)**2}')\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(log_likelihood)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(log_likelihood)\n",
    "        else:\n",
    "            return log_likelihood\n",
    "\n",
    "    def negative_log_variance_prior_fn(self,\n",
    "                                log_variance: torch.tensor, \n",
    "                                mean: float = 1e-6, \n",
    "                                variance: float = 0.01) -> torch.tensor:\n",
    "        \"\"\"\n",
    "            This calculates the log of the prior distribution for the variance parameter in N(f(X,theta_mu), theta_var). \n",
    "            Here we use the lognormal distribution for the variance prior.\n",
    "        \"\"\"\n",
    "        mean = torch.tensor(mean, dtype=self.dtype)\n",
    "        variance = torch.tensor(variance, dtype=self.dtype)\n",
    "        log_variance_prior = -0.5*torch.log(variance) - torch.exp(log_variance).sum() - (((log_variance - mean)**2)/(2*variance)).sum()\n",
    "        neg_log_variance_prior = -log_variance_prior\n",
    "        return neg_log_variance_prior\n",
    "    \n",
    "    def negative_log_params_prior_fn(self, params: Iterable[torch.tensor], fn='gaussian') -> torch.tensor:\n",
    "        \"\"\"\n",
    "            This calculates log of the prior distribution for the weight parameters (theta_mu) that calculate the mean (f(X,theta_mu)) in N(f(X,theta_mu), theta_var).\n",
    "            Here we use the normal distribution for the weight priors. This translates to the L2 norm on the weights.\n",
    "        \"\"\"\n",
    "        if fn == 'gaussian':\n",
    "            log_params_prior = torch.tensor(0.0, dtype=self.dtype, device=self.device)\n",
    "            for param in params:\n",
    "                log_params_prior += -0.5*(1/self.params_var)*torch.sum((param**2)) \n",
    "                # Including the constant part\n",
    "                # log_params_prior += 0.5*torch.log(torch.tensor(1/(2*math.pi*self.params_var)))*param.numel() - 0.5*(1/self.params_var)*torch.sum((param**2))\n",
    "            return -log_params_prior\n",
    "        elif fn == 'laplace':\n",
    "            log_params_prior = torch.tensor(0.0, dtype=self.dtype, device=self.device)\n",
    "            for param in params:\n",
    "                log_params_prior += (-1/self.params_var)*torch.sum(torch.abs(param))\n",
    "            return -log_params_prior\n",
    "        else:\n",
    "            raise ValueError(f'Provided fn {fn} is not valid. Should be gaussian or laplace.')\n",
    "\n",
    "    def forward(self, pred, target, params):\n",
    "        if self.model_type == 2:\n",
    "            gnll = self.gaussian_negative_log_likelihood(pred, target)\n",
    "            neg_log_var_prior = self.negative_log_variance_prior_fn(pred[:,1].view_as(target))\n",
    "            neg_log_params_prior = self.negative_log_params_prior_fn(params)\n",
    "            return gnll, neg_log_var_prior, neg_log_params_prior\n",
    "        else:\n",
    "            gnll = self.gaussian_negative_log_likelihood(pred, target)\n",
    "            neg_log_params_prior = self.negative_log_params_prior_fn(params)\n",
    "            return gnll, neg_log_params_prior\n",
    "\n",
    "\n",
    "def get_params(model:torch.nn.Module) -> torch.tensor:\n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        params.append(param.flatten())\n",
    "    return torch.concatenate(params)\n",
    "\n",
    "def get_params_grads(model:torch.nn.Module) -> torch.tensor:\n",
    "    params_grads = []\n",
    "    for param in model.parameters():\n",
    "        params_grads.append(param.grad.flatten())\n",
    "    return torch.concatenate(params_grads)\n",
    "\n",
    "def set_params(model:torch.nn.Module, params) -> None:\n",
    "    # stack_name, layer_num, param_name = name.split('.')\n",
    "    # new_param = torch.nn.parameter.Parameter(data=params[i:i+num_params_per_layer[j]].view_as(old_param), \n",
    "    #                                         requires_grad=True)\n",
    "    # Method 2 : Sets the params for the layer\n",
    "    # setattr(model._modules[stack_name][int(layer_num)], param_name, new_param)\n",
    "    # old_param.copy_(new_param.view_as(old_param)) # Does not work\n",
    "    pointer=0\n",
    "    for old_param in model.parameters():\n",
    "        # Method 1 : From PyTorch\n",
    "        num_param = old_param.numel()\n",
    "        old_param.data = params[pointer:pointer+num_param].view_as(old_param).data\n",
    "        pointer += num_param\n",
    "\n",
    "def calc_params_grads(model, train_X, train_y, negative_log_prob_fn, step_num, print_every=100) -> None:\n",
    "    # Forward Propagation\n",
    "    pred = model(train_X)\n",
    "    if negative_log_prob_fn.model_type == 2:\n",
    "        # Compute Loss\n",
    "        gnll, neg_log_var_prior, neg_log_params_prior = negative_log_prob_fn(pred, train_y, model.parameters())\n",
    "        negative_log_prob = gnll + neg_log_var_prior + neg_log_params_prior\n",
    "        if print_every is not None:\n",
    "            if step_num%print_every == 0:\n",
    "                print(f'Step {step_num} : gnll is {round(gnll.item(), 4)}, \\\n",
    "                    neg_log_var_prior is {round(neg_log_var_prior.item(), 4)}, \\\n",
    "                    neg_log_params_prior is {round(neg_log_params_prior.item(), 4)}, \\\n",
    "                    total loss is {round(negative_log_prob.item(), 4)}.')\n",
    "    else:\n",
    "        # Compute Loss\n",
    "        gnll, neg_log_params_prior = negative_log_prob_fn(pred, train_y, model.parameters())\n",
    "        print\n",
    "        negative_log_prob = gnll + neg_log_params_prior\n",
    "        if print_every is not None:\n",
    "            if step_num%print_every == 0:\n",
    "                print(f'Step {step_num} : gnll is {round(gnll.item(), 4)}, \\\n",
    "                    neg_log_params_prior is {round(neg_log_params_prior.item(), 4)}, \\\n",
    "                    total loss is {round(negative_log_prob.item(), 4)}.')\n",
    "    # Backpropagate to accumulate gradients in the parameters\n",
    "    negative_log_prob.backward()\n",
    "\n",
    "# Ref : https://arxiv.org/pdf/1206.1901 (MCMC Using Hamiltonian Dynamics)\n",
    "def run_hmc(model, train_X, train_y, potential_energy_fn, epsilon, L):\n",
    "    \"\"\"\n",
    "        model : BNN to do the forward pass.\n",
    "        current_q : This is the current position of the fictious particle. \n",
    "                    These are the weights of the BNN\n",
    "        U : This is the potential energy function.\n",
    "            This is the negative log of the probability distribution we want to sample from.\n",
    "        grad_U : This is the gradient of the potential energy function. \n",
    "                 This is the gradient of the negative log of the probability distribution we want to sample from.\n",
    "        epsilon : This is the step size of the leap frog integrator.\n",
    "        L : This is the number of leap frog steps to take.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Current set of weights as a 1D tensor\n",
    "    q = get_params(model)\n",
    "    current_q = q\n",
    "\n",
    "    # Randomly sample momentum variables from the proposal distribution\n",
    "    # In this case proposal dist is N(0,1).\n",
    "    p = torch.randn_like(q)\n",
    "    # Set this as current momentum\n",
    "    current_p = p\n",
    "\n",
    "    # Calculate the current Hamiltonian \n",
    "    if potential_energy_fn.model_type == 2: # Use this if liklihood is N(f(X,theta_mu), theta_var). In this case liklihood is homosckedastic model\n",
    "        current_gnll, current_neg_log_var_prior, current_neg_log_wt_prior = potential_energy_fn(model(train_X), train_y, current_q)\n",
    "        current_potential_energy = current_gnll + current_neg_log_var_prior + current_neg_log_wt_prior\n",
    "    else: # Use this if liklihood is N(f(X,theta_mu), 1). In this case the variance is not a learnable parameter.\n",
    "        current_gnll, current_neg_log_wt_prior = potential_energy_fn(model(train_X), train_y, current_q)\n",
    "        current_potential_energy = current_gnll + current_neg_log_wt_prior\n",
    "    current_kinetic_energy = 0.5*torch.sum(current_p**2) # Here is I. Can be scalar multiple of I.\n",
    "\n",
    "    # Foward + Backward Propagation\n",
    "    calc_params_grads(model, train_X, train_y, potential_energy_fn, 0)\n",
    "    # Accumulate gradients as a 1D tensor\n",
    "    params_grads = get_params_grads(model)\n",
    "\n",
    "    # Make half step for momentum at beginning\n",
    "    # p = p - epsilon/2*grad_U(q)\n",
    "    p.add_(params_grads, alpha=-epsilon/2)\n",
    "    # Zero out param grad\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Run 'L' leap frog steps\n",
    "    for i in range(1, L+1):\n",
    "\n",
    "        # Make full step for position\n",
    "        q.add_(p, alpha=epsilon)\n",
    "\n",
    "        # Set params to model\n",
    "        set_params(model, q)\n",
    "        # Foward + Backward Propagation\n",
    "        calc_params_grads(model, train_X, train_y, potential_energy_fn, i)\n",
    "        # Accumulate gradients as a 1D tensor\n",
    "        params_grads = get_params_grads(model)\n",
    "\n",
    "        # Make full step for momentum except at end\n",
    "        if i != L:\n",
    "            # p = p - epsilon*grad_U(q)\n",
    "            p.add_(params_grads, alpha=-epsilon)\n",
    "            # Zero out param grad\n",
    "            model.zero_grad()\n",
    "\n",
    "    # Make half step for momentum at end\n",
    "    # p = p - epsilon/2*grad_U(q)\n",
    "    p.add_(params_grads, alpha=-epsilon/2)\n",
    "    # Zero out param grad\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Negating momentum at end of trajectory to make proposal symmetric\n",
    "    # We actually dont need this step since we are anyways squaring p.\n",
    "    p = -p\n",
    "\n",
    "    # Calculate the Hamiltonian at the end of trajectory\n",
    "    if potential_energy_fn.model_type == 2:\n",
    "        proposed_gnll, proposed_neg_log_var_prior, proposed_neg_log_wt_prior = potential_energy_fn(model(train_X), train_y, model.parameters())\n",
    "        proposed_potential_energy = proposed_gnll + proposed_neg_log_var_prior + proposed_neg_log_wt_prior\n",
    "    else:\n",
    "        proposed_gnll, proposed_neg_log_wt_prior = potential_energy_fn(model(train_X), train_y, model.parameters())\n",
    "        proposed_potential_energy = proposed_gnll + proposed_neg_log_wt_prior\n",
    "    proposed_kinetic_energy = 0.5*torch.sum(p**2)\n",
    "\n",
    "    # Acceptance Criteria - Metropolis Hastings update\n",
    "    # Decide whether to accept sample or not\n",
    "    runif = torch.rand(1)\n",
    "    accep_prob = min(1, torch.exp(current_potential_energy - proposed_potential_energy + current_kinetic_energy - proposed_kinetic_energy))\n",
    "    if runif < accep_prob:\n",
    "        print(f'runif {runif}, accep prob {accep_prob}')\n",
    "        return q, 'accepted', accep_prob\n",
    "    else:\n",
    "        print(f'runif {runif}, accep prob {accep_prob}')\n",
    "        # Set the current_q back as model parameters\n",
    "        set_params(model, current_q)\n",
    "        return None, 'rejected', accep_prob\n",
    "\n",
    "# Ref : https://dl.acm.org/doi/10.5555/3157382.3157560 (Bayesian Optimization with Robust Bayesian Neural Networks.)\n",
    "class SGHMC(Optimizer):\n",
    "    def __init__(self, params, lr: float,\n",
    "                 num_burn_in_steps: int,\n",
    "                 scale_grad : float = 1.0,\n",
    "                 mdecay: float = 0.05,\n",
    "                 wd: float = 0.00002,\n",
    "                 epsilon: float = 1e-16,\n",
    "                 ) -> None:\n",
    "        defaults = {'lr':lr, \n",
    "                    'scale_grad':scale_grad,\n",
    "                    'num_burn_in_steps':num_burn_in_steps,\n",
    "                    'mdecay':mdecay,\n",
    "                    'wd':wd,\n",
    "                    'epsilon':epsilon}\n",
    "        super(SGHMC, self).__init__(params, defaults)\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # A group of tensors can be optimized separately\n",
    "        for group in self.param_groups:\n",
    "            for parameter in group['params']:\n",
    "\n",
    "                if parameter.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[parameter]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['iteration'] = 0\n",
    "                    state['momentum'] = torch.randn(parameter.size(), dtype=parameter.dtype, device=parameter.device)\n",
    "\n",
    "                state['iteration'] += 1\n",
    "\n",
    "                mdecay, lr, wd = group['mdecay'], group['lr'], group['wd']\n",
    "                scale_grad = group['scale_grad']\n",
    "\n",
    "                momentum = state['momentum']\n",
    "                grad = parameter.grad.data*scale_grad\n",
    "\n",
    "                sigma = torch.sqrt(torch.from_numpy(np.array(2*lr*mdecay, dtype=type(lr))))\n",
    "                sample_t = torch.normal(mean=torch.zeros_like(grad), std=torch.ones_like(grad)*sigma)\n",
    "                \n",
    "                # This is where the update steps take place (Ref : https://github.com/automl/pybnn/blob/master/pybnn/sampler/sghmc.py)\n",
    "                parameter.data.add_(lr*mdecay*momentum)\n",
    "                momentum.add_(-lr*grad - lr*mdecay*momentum + sample_t)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class SGLD(Optimizer):\n",
    "    def __init__(self, params, \n",
    "                 lr: float=1e-2,\n",
    "                 scale_grad : float = 1.0) -> None:\n",
    "        \n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            scale_grad=scale_grad\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for parameter in group['parameters']:\n",
    "\n",
    "                if parameter.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[parameter]\n",
    "\n",
    "                lr, scale_grad = group['lr'], group['scale_grad']\n",
    "\n",
    "                grad = parameter.grad.data*scale_grad\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['iteration'] = 0\n",
    "\n",
    "                sigma = torch.sqrt(torch.from_numpy(np.array(lr, dtype=type(lr))))\n",
    "                \n",
    "                # This is where the update steps take place (Ref : https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=56f89ce43d7e386bface3cba63e674fe748703fc)\n",
    "                parameter.data.add_(0.5*lr*grad + sigma+torch.normal(mean=torch.zeros_like(grad), std=torch.ones_like(grad)))\n",
    "\n",
    "                state['iteration'] += 1\n",
    "                state['sigma'] = sigma\n",
    "\n",
    "        return loss\n",
    "        \n",
    "# Test the piece of code\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    # net = NeuralNet(1, 1, 10, 3, device=device, dtype=dtype)\n",
    "    # print(net)\n",
    "    # # Sample input\n",
    "    # # INFO : torch.Tensor() : Always copies data. To avoid copy use torch.as_tensor()\n",
    "    # test_tensor = torch.as_tensor([[10], [20]], device=device, dtype=dtype)\n",
    "    # print(net(test_tensor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BNN by HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### USER INPUT ######\n",
    "# -----------------------------------------\n",
    "# BNN parameters\n",
    "# -----------------------------------------\n",
    "bnn_type = 3\n",
    "perform_hyperparam_search = False\n",
    "# -----------------------------------------\n",
    "# Encoder parameters\n",
    "# -----------------------------------------\n",
    "enc_hidden_layers = None\n",
    "enc_hidden_activations = None\n",
    "enc_out_dim = 2\n",
    "enc_out_activation = torch.nn.Tanh()\n",
    "# -----------------------------------------\n",
    "# Decoder parameters\n",
    "# -----------------------------------------\n",
    "dec_hidden_layers = None\n",
    "dec_hidden_activations = None\n",
    "dec_out_dim = train_X_data.shape[1]\n",
    "dec_out_activation = None\n",
    "# -----------------------------------------\n",
    "# Predictor parameters\n",
    "# -----------------------------------------\n",
    "pred_hidden_layers = None\n",
    "pred_hidden_activations = None\n",
    "pred_out_dim = train_y_data.shape[1]\n",
    "pred_out_activation = torch.nn.Sigmoid()\n",
    "# -----------------------------------------\n",
    "# Pretraining BNN Parameters\n",
    "# -----------------------------------------\n",
    "selected_init_weights = 'normal'\n",
    "# Dictionary of defaults to use to ihtialize wts\n",
    "init_wts_dist_params ={\n",
    "    'uniform': {'a':-1, 'b':1},\n",
    "    'normal': {'mean':0.0, 'std':10.0},\n",
    "    'xavier_normal': {'gain':1.0}\n",
    "}\n",
    "perform_pretrain = True\n",
    "pretrain_lr = 0.1\n",
    "use_lr_scheduler = False\n",
    "pretrain_num_epochs = 500\n",
    "# Full batch gradient descent\n",
    "pretrain_batch_size = train_dataset.shape[0]\n",
    "# -----------------------------------------\n",
    "# -log(p(w,D)) = -log(p(D|w)) - log(p(w))\n",
    "# -----------------------------------------\n",
    "# p(w) = N(0, params_var)\n",
    "# Larger values indicate less certainity in the parameters.\n",
    "# Small values indicate more certainity in the parameters.\n",
    "params_var = init_wts_dist_params['normal']['std']**2\n",
    "# p(D|w) = f(X; w) + N(0, noise_var) = N(f(X; w), noise_var) \n",
    "# homoscedastic model (same variance for all data points)\n",
    "# noise_var = 1.0 => -log(p(D|w)) = 0.5*sum((y - f(X; w))**2)\n",
    "noise_var = 0.001\n",
    "# -----------------------------------------\n",
    "# HMC Parameters\n",
    "# -----------------------------------------\n",
    "perform_hmc = False\n",
    "num_samples = 500\n",
    "# Number of samples to burn before storing \n",
    "num_samples_to_burn = 500\n",
    "path_length = 0.2\n",
    "# Step Size\n",
    "epsilon = 0.01\n",
    "# number_of_leapfrog_steps = L/epsilon \n",
    "adapt_epsilon = False\n",
    "# -----------------------------------------\n",
    "\n",
    "# TODO : Run hyperparameter search on model\n",
    "if perform_hyperparam_search:\n",
    "    pass\n",
    "\n",
    "# Intialize the model architecture\n",
    "encoder = BNN(in_dim=train_X_data.shape[1], out_dim=train_y_data.shape[1], \n",
    "                hidden_layers=enc_hidden_layers, hidden_activation=enc_hidden_activations,\n",
    "                out_activation=enc_out_activation,\n",
    "                bias=True,\n",
    "                model_type=bnn_type,\n",
    "                device=device,\n",
    "                dtype=dtype)\n",
    "\n",
    "predictor = BNN(in_dim=enc_out_dim, out_dim=pred_out_dim,\n",
    "                hidden_layers=pred_hidden_layers, hidden_activation=pred_hidden_activations,\n",
    "                out_activation=pred_out_activation,\n",
    "                bias=True,\n",
    "                model_type=bnn_type,\n",
    "                device=device,\n",
    "                dtype=dtype)\n",
    "\n",
    "decoder = BNN(in_dim=enc_out_dim, out_dim=dec_out_dim,\n",
    "                hidden_layers=dec_hidden_layers, hidden_activation=dec_hidden_activations,\n",
    "                out_activation=dec_out_activation,\n",
    "                bias=True,\n",
    "                model_type=bnn_type,\n",
    "                device=device,\n",
    "                dtype=dtype)\n",
    "\n",
    "def init_weights_scheme(scheme:str) -> None:\n",
    "    if scheme == 'uniform':\n",
    "        def init_weights(m:torch.nn.Module):\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                # print(f' Weight value before : {m.weight}')\n",
    "                torch.nn.init.uniform_(m.weight, a=init_wts_dist_params['uniform']['a'], b=init_wts_dist_params['uniform']['b'], generator=None)\n",
    "                # print(f' Weight value after : {m.weight}')\n",
    "        return init_weights\n",
    "    elif scheme == 'xavier_uniform':\n",
    "        def init_weights(m:torch.nn.Module):\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                # print(f' Weight value before : {m.weight}')\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=10.0)\n",
    "                # print(f' Weight value after : {m.weight}')\n",
    "        return init_weights\n",
    "    elif scheme == 'xavier_normal':\n",
    "        def init_weights(m:torch.nn.Module):\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                # print(f' Weight value before : {m.weight}')\n",
    "                # Gain is an optional scaling factor\n",
    "                torch.nn.init.xavier_normal_(m.weight, gain=init_wts_dist_params['xavier_normal']['gain'])\n",
    "                # print(f' Weight value after : {m.weight}')\n",
    "        return init_weights\n",
    "    elif scheme == 'normal':\n",
    "        def init_weights(m:torch.nn.Module):\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                # print(f' Weight value before : {m.weight}')\n",
    "                torch.nn.init.normal_(m.weight, mean=init_wts_dist_params['normal']['mean'], std=init_wts_dist_params['normal']['std'], generator=None)\n",
    "                # print(f' Weight value after : {m.weight}')\n",
    "        return init_weights\n",
    "    elif scheme == 'constant':\n",
    "        def init_weights(m:torch.nn.Module):\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                # print(f' Weight value before : {m.weight}')\n",
    "                torch.nn.init.constant_(m.weight, val=10.0)\n",
    "                # print(f' Weight value after : {m.weight}')\n",
    "        return init_weights\n",
    "    else:\n",
    "        raise TypeError(f'Provided scheme {scheme} is not valid. Use uniform, normal or constant.')\n",
    "    \n",
    "def init_bias_scheme(scheme:str) -> None:\n",
    "    if scheme == 'uniform':\n",
    "        def init_bias(m:torch.nn.Module):\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                # print(f' Bias value before : {m.bias}')\n",
    "                torch.nn.init.uniform_(m.weight, a=init_wts_dist_params['uniform']['a'], b=init_wts_dist_params['uniform']['b'], generator=None)\n",
    "                # print(f' Bias value after : {m.bias}')\n",
    "        return init_bias\n",
    "    elif scheme == 'normal':\n",
    "        def init_bias(m:torch.nn.Module):\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                # print(f' Bias value before : {m.bias}')\n",
    "                torch.nn.init.normal_(m.bias, mean=init_wts_dist_params['normal']['mean'], std=init_wts_dist_params['normal']['std'], generator=None)\n",
    "                # print(f' Bias value after : {m.bias}')\n",
    "        return init_bias\n",
    "    elif scheme == 'xavier_normal':\n",
    "        def init_bias(m:torch.nn.Module):\n",
    "            if type(m) == torch.nn.Linear:\n",
    "                # print(f' Bias value before : {m.bias}')\n",
    "                torch.nn.init.xavier_normal_(m.bias, gain=init_wts_dist_params['xavier_normal']['gain'])\n",
    "                # print(f' Bias value after : {m.bias}')\n",
    "    else:\n",
    "        raise TypeError(f'Provided scheme {scheme} is not valid. Use uniform, normal or constant.')\n",
    "\n",
    "if selected_init_weights == 'default':\n",
    "    # Initialize torch.nn.Linear weights from U(-k**0.5, k**0.5) k = 1/in_feats\n",
    "    pass\n",
    "else:\n",
    "    # Initialize model weights\n",
    "    bnn.apply(init_weights_scheme(selected_init_weights))\n",
    "    bnn.apply(init_bias_scheme(selected_init_weights))\n",
    "\n",
    "print('\\n')\n",
    "print('Selected BNN Architecture')\n",
    "print(bnn)\n",
    "print('\\n')\n",
    "\n",
    "# Define the losses for the predictor and decoder\n",
    "pred_negative_log_joint_fn = NegativeLogJoint(model_type=bnn_type, noise_var=noise_var, params_var=params_var, reduction='mean', dtype=dtype, device=device)\n",
    "dec_negative_log_joint_fn = NegativeLogJoint(model_type=bnn_type, noise_var=noise_var, params_var=params_var, reduction='mean', dtype=dtype, device=device)\n",
    "\n",
    "# Model pretrain to find good set of weights to start off with\n",
    "if perform_pretrain:\n",
    "    # Initialize the optimizer\n",
    "    # Since computing the negative log prob, we are minimizing.\n",
    "    adam = Adam(bnn.parameters(), lr=pretrain_lr, maximize=False)\n",
    "\n",
    "    if use_lr_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(adam, gamma=0.9999, verbose=True)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Training Loop\n",
    "    train_dataloader = DataLoader(TensorDataset(train_X_data, train_y_data),\n",
    "                                batch_size=pretrain_batch_size,\n",
    "                                shuffle=True)\n",
    "\n",
    "    test_dataloader = DataLoader(TensorDataset(val_X_data, val_y_data),\n",
    "                                batch_size=pretrain_batch_size,\n",
    "                                shuffle=True)\n",
    "\n",
    "    loss_fn_list = [pred_negative_log_joint_fn, dec_negative_log_joint_fn]\n",
    "    loss_wts = [1, 1]\n",
    "    loss_names = ['neg_log_joint', 'neg_log_joint']\n",
    "    store_train_losses_list = []\n",
    "    store_test_losses_list = []\n",
    "    store_train_gnll = []\n",
    "    store_train_neg_log_params_prior = []\n",
    "    store_test_gnll = []\n",
    "    store_test_neg_log_params_prior = []\n",
    "\n",
    "    for epoch in range(pretrain_num_epochs):\n",
    "        \n",
    "        train_loss_per_epoch_list, gnll, neg_log_params_prior = train_loop(train_dataloader, encoder, predictor, decoder, loss_fn_list, loss_wts, loss_names, adam, scheduler, print_every=None)\n",
    "        store_train_losses_list.append(train_loss_per_epoch_list)\n",
    "        store_train_gnll.append(gnll)\n",
    "        store_train_neg_log_params_prior.append(neg_log_params_prior)\n",
    "\n",
    "        test_loss_per_epoch_list, gnll, neg_log_params_prior = test_loop(test_dataloader, encoder, predictor, decoder, loss_fn_list, loss_wts, loss_names)   \n",
    "        store_test_losses_list.append(test_loss_per_epoch_list) \n",
    "        store_test_gnll.append(gnll)\n",
    "        store_test_neg_log_params_prior.append(neg_log_params_prior)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'-------------------------- Epoch {epoch} --------------------------')\n",
    "            print('--------------------- Train Epoch Stats ---------------------')\n",
    "            for loss_name, train_loss in zip(loss_names, train_loss_per_epoch_list):\n",
    "                print(f'Train {loss_name} / epoch is ............................. {round(train_loss, 8)}')\n",
    "            print(f'Train Total Loss / epoch is ..................... {round(sum(train_loss_per_epoch_list), 8)}')\n",
    "\n",
    "            print(f'--------------------- Test Epoch Stats ----------------------')\n",
    "            for loss_name, test_loss in zip(loss_names, test_loss_per_epoch_list):\n",
    "                print(f'Test {loss_name} / epoch is ............................. {round(test_loss, 8)}')\n",
    "            print(f'Test Total Loss / epoch is ..................... {round(sum(test_loss_per_epoch_list), 8)}')\n",
    "            if use_lr_scheduler:\n",
    "                print(f'Learning rate before step : {scheduler.get_last_lr()}')\n",
    "                print('\\n')\n",
    "            else:\n",
    "                print('\\n')\n",
    "        \n",
    "    # Plotting the losses\n",
    "    epochs = np.arange(0, pretrain_num_epochs, 1)\n",
    "    store_train_losses_array = np.array(store_train_losses_list)\n",
    "    store_test_losses_array = np.array(store_test_losses_list)\n",
    "    store_tot_train_losses = np.sum(store_train_losses_array, axis=1)\n",
    "    store_tot_test_losses = np.sum(store_test_losses_array, axis=1)\n",
    "\n",
    "    store_train_gnll_array = np.array(store_train_gnll)\n",
    "    store_test_gnll_array = np.array(store_test_gnll)\n",
    "    store_train_neg_log_params_prior_array = np.array(store_train_neg_log_params_prior)\n",
    "    store_test_neg_log_params_prior_array = np.array(store_test_neg_log_params_prior)\n",
    "\n",
    "    fig, ax = plt.subplots(1,3,figsize=(12,4))\n",
    "    ax[0].plot(epochs, store_train_losses_array[:,0], label='Train Neg. log. prob.')\n",
    "    ax[0].plot(epochs, store_test_losses_array[:,0], label='Test Neg. log. prob.')\n",
    "    ax[0].set_title('Neg. log. prob. v epochs')\n",
    "    ax[1].plot(epochs, store_train_gnll_array[:,0], label='Train GNLL')\n",
    "    ax[1].plot(epochs, store_test_gnll_array[:,0], label='Test GNLL')\n",
    "    ax[1].set_title('GNLL v epochs')\n",
    "    ax[2].plot(epochs, store_train_neg_log_params_prior_array[:,0], label='Train Neg. log. params prior')\n",
    "    ax[2].plot(epochs, store_test_neg_log_params_prior_array[:,0], label='Test Neg. log. params prior')\n",
    "    ax[2].set_title('Neg. log. params prior v epochs')\n",
    "    # plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Zero grad the model\n",
    "    bnn.zero_grad()\n",
    "    # Delete the optimizer\n",
    "    del adam\n",
    "\n",
    "    # Saving pytorch model\n",
    "    torch.save(copy.deepcopy(bnn.state_dict()), 'bnn_pretrained.pth')\n",
    "    torch.save(bnn.state_dict(), 'bnn_pretrained.pth')\n",
    "\n",
    "    # Accuracy on test set\n",
    "    print('-------------------------- Test Set Accuracy --------------------------')\n",
    "    preds = bnn(test_X_data)\n",
    "    print(f'Predictions : {preds}')\n",
    "    print(f'True values : {test_y_data}')\n",
    "\n",
    "##########################\n",
    "# HMC Starts from here \n",
    "##########################\n",
    "\n",
    "if perform_hmc:\n",
    "\n",
    "    # Load the pretrained model\n",
    "    # bnn.load_state_dict(torch.load('bnn_pretrained.pth'))\n",
    "\n",
    "    num_accepted = 0\n",
    "    num_rejected = 0\n",
    "    accepted_weight_samples = []\n",
    "\n",
    "    # For adaptive epsilon\n",
    "    gamma = 0.05\n",
    "    t = 10.0\n",
    "    kappa = 0.75\n",
    "    mu = np.log(10*epsilon)\n",
    "    log_best_epsilon = 0.0\n",
    "    closeness = 0.0 \n",
    "\n",
    "    # Set model into training mode\n",
    "    bnn.train()\n",
    "    print(f'---------------------- Starting HMC ----------------------')\n",
    "    for i in range(1, num_samples+num_samples_to_burn+1):\n",
    "        # Adapted from https://github.com/yucenli/bnn-bo/blob/main/models/hmc_utils.py\n",
    "        num_leapfrog_steps_per_sample = min(200, int(np.ceil(path_length/epsilon)))\n",
    "        print(f'-------------------- Running sample {i} --------------------')\n",
    "        print(f'Epsilon : {epsilon}, Path length : {path_length}, Num leapfrog steps /sample : {num_leapfrog_steps_per_sample}')\n",
    "        q, result, accep_prob = run_hmc(bnn, train_X_data, train_y_data, negative_log_joint_fn, epsilon, num_leapfrog_steps_per_sample)\n",
    "\n",
    "        if i <= num_samples_to_burn:\n",
    "            # Code sourced from : https://github.com/yucenli/bnn-bo/blob/main/models/hmc_utils.py\n",
    "            if adapt_epsilon:\n",
    "\n",
    "                iter = float(i + 1)\n",
    "                closeness_frac = 1.0/(i + t)\n",
    "                closeness = (1.0 - closeness_frac)*closeness + closeness_frac*(0.75 - accep_prob)\n",
    "                # The above equation is from  :  https://arxiv.org/pdf/1206.1901.pdf\n",
    "                log_epsilon = mu - (math.sqrt(iter)/gamma)*closeness\n",
    "                epsilon = math.exp(log_epsilon)\n",
    "\n",
    "                step_frac = math.pow(i, -kappa)\n",
    "                log_best_epsilon = (step_frac*log_epsilon) + (1 - step_frac)*log_best_epsilon\n",
    "\n",
    "                if (path_length / epsilon) > 200:\n",
    "                    path_length = path_length / 2\n",
    "                    print(\"new path length\", path_length, \"epsilon\", epsilon)\n",
    "\n",
    "                if i == num_samples_to_burn:\n",
    "                    epsilon = math.exp(log_best_epsilon)\n",
    "                    print(f'Final Epsilon : {epsilon}')\n",
    "            print(f'New epsilon : {epsilon}')\n",
    "            print(f'-------------------- Burning sample {i} --------------------\\n')\n",
    "        else:\n",
    "            # Print Accept or Reject\n",
    "            if result == 'accepted':\n",
    "                num_accepted += 1\n",
    "                # Add to list of proposed weight samples ..\n",
    "                accepted_weight_samples.append(q)\n",
    "            else:\n",
    "                num_rejected += 1\n",
    "            print(f'Final Epsilon : {epsilon}')\n",
    "            acceptance_ratio = num_accepted/(num_samples)\n",
    "            print(f'--- Sample {i} {result}; Accepted {num_accepted}; Accep ratio {acceptance_ratio} ---\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = get_params(bnn)\n",
    "print(q)\n",
    "for q, feat in zip(q, features):\n",
    "    print(f'Feature {feat} : {q}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_x, train_y, '.r')\n",
    "plt.plot(test_x, test_y, '--k', label='True', alpha=0.5)\n",
    "plt.plot(test_x, 9.9215*test_x, '-k', label='loss = 1.5963',  alpha=1.0)\n",
    "# plt.plot(test_x, accepted_weight_samples[0].detach().numpy()[0]*test_x, '--b', label='loss = 3.0149', alpha=0.5)\n",
    "# plt.plot(test_x, accepted_weight_samples[1].detach().numpy()[0]*test_x, '--r', label='loss = 1.5462', alpha=0.5)\n",
    "# plt.plot(test_x, accepted_weight_samples[2].detach().numpy()[0]*test_x, '--g', label='loss = 1.4276', alpha=0.5)\n",
    "# plt.plot(test_x, accepted_weight_samples[3].detach().numpy()[0]*test_x, '--y', label='loss = 1.3571', alpha=0.5)\n",
    "# plt.plot(test_x, accepted_weight_samples[4].detach().numpy()[0]*test_x, '--c', label='loss = 1.2948', alpha=0.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Distribution of weights p(w|D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Distribution of accepted weight samples\n",
    "accepted_weight_samples_list = list(torch.stack(accepted_weight_samples).detach().numpy().squeeze())    \n",
    "print(accepted_weight_samples_list)\n",
    "# Plotting the distribution of the weights\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,4))\n",
    "sns.kdeplot(accepted_weight_samples_list, ax=ax, label='p(w|D)')\n",
    "plt.title('Distribution of weights')\n",
    "# Label the mean of the distribution and its value in legend\n",
    "mean = round(float(np.mean(accepted_weight_samples_list)), 4)\n",
    "plt.axvline(mean, color='red', linestyle='--', label=f'Mean : {mean}')\n",
    "\n",
    "def normal_dist(x):\n",
    "    return (1/(2*math.pi*100)**0.5)*math.exp(-(x**2)/200)\n",
    "x = np.linspace(-30, 30, 1000)\n",
    "y = [normal_dist(i) for i in x]\n",
    "plt.plot(x, y, label='p(w) = N(0, 100)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Distribution of weights and hyperparameters p(w,alpha|D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint distribution of weights and data using seaborn\n",
    "\n",
    "# Plotting the data\n",
    "# randomly_selected_wts = np.random.choice(accepted_weight_samples_list, test_y.size(0))\n",
    "test_x_1000 = torch.reshape(torch.linspace(0, 1, 989).float().unsqueeze(1).to(device), (-1, 1))\n",
    "print(g(test_x_1000).squeeze().shape)\n",
    "print(len(accepted_weight_samples_list))\n",
    "sns.jointplot(x=g(test_x_1000).squeeze(), y=accepted_weight_samples_list, kind='kde', ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid for the density plot\n",
    "x_edges = np.linspace(-5, 20, 100)\n",
    "y_edges = np.linspace(8.5, 10, 100)\n",
    "x_grid, y_grid = np.meshgrid(x_edges, y_edges)\n",
    "\n",
    "# Calculate the density using a Gaussian kernel density estimate\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Create a Gaussian kernel density estimate of the data points\n",
    "kde = gaussian_kde([test_y.squeeze().cpu(), \n",
    "                    randomly_selected_wts])\n",
    "z_density = kde(np.vstack([x_grid.ravel(), y_grid.ravel()]))\n",
    "z_density = z_density.reshape(x_grid.shape)\n",
    "\n",
    "# # Plotting the surface\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.plot_surface(x_grid, y_grid, z_density, cmap='viridis')\n",
    "# ax.set_xlabel('X-axis')\n",
    "# ax.set_ylabel('Y-axis')\n",
    "# ax.set_zlabel('Density')\n",
    "# plt.show()\n",
    "\n",
    "# Plotting the surface\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=[go.Surface(z=z_density, x=x_grid, y=y_grid, colorscale='Viridis')])\n",
    "fig.update_layout(scene=dict(\n",
    "                    xaxis_title='X-axis',\n",
    "                    yaxis_title='Y-axis',\n",
    "                    zaxis_title='Density'),\n",
    "                  title='3D Density Plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = get_params(bnn)\n",
    "# accepted_weight_samples = [q]\n",
    "\n",
    "# Run through and get f preds for each weight sample\n",
    "f_draws = []\n",
    "with torch.no_grad():\n",
    "    for weights in accepted_weight_samples:\n",
    "        set_params(bnn, weights)\n",
    "        f_draws.append(bnn(test_X_data)[:,0].detach().numpy())\n",
    "\n",
    "f_draws_array = np.array(f_draws)\n",
    "print(f_draws_array.shape)\n",
    "\n",
    "f_mean = np.round(np.mean(f_draws_array, axis=0), 4)\n",
    "f_var = np.round(np.var(f_draws_array, axis=0), 4)\n",
    "f_std = np.round(np.sqrt(f_var), 4)\n",
    "\n",
    "print(test_y)\n",
    "print(f_mean)\n",
    "print(f_var)\n",
    "print(f_std)\n",
    "\n",
    "# # f_covar = f_preds.covariance_matrix\n",
    "# # f_samples = f_preds.sample(sample_shape=torch.Size((10,)))\n",
    "\n",
    "# plt.plot(test_x.squeeze().cpu(), test_y.squeeze().cpu(), '--k', label='True')\n",
    "# plt.plot(train_x.cpu(), train_y.cpu(), '.r')\n",
    "# # plt.plot(test_x.squeeze().cpu(), y_preds.mean.detach().numpy(), color='green', label='mean', linewidth=2)\n",
    "# plt.plot(test_x.squeeze().cpu(), f_mean, color='orange', label='mean', linewidth=2)\n",
    "# plt.gca().fill_between(test_x.squeeze().cpu(), f_mean - 2*f_std, f_mean + 2*f_std, label=r'2$\\sigma$', alpha = 0.2, color=\"orange\")\n",
    "\n",
    "# # Randomly choose 8 index to plot\n",
    "# idx = np.random.choice(f_draws_array.shape[0], 10)\n",
    "# for i in idx:\n",
    "#     plt.plot(test_x.squeeze().cpu(), f_draws_array[i,:], color='C0', alpha=0.1)\n",
    "# plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nestedae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
