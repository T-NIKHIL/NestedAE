{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a549ba2d",
   "metadata": {},
   "source": [
    "* This notebook experiments with new NestedAE architectures.\n",
    "* Features that are useful are then pushed to the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.svm import SVR, NuSVR, LinearSVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import kstest, ks_2samp\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList, ModuleDict, Linear, L1Loss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import umap\n",
    "\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, MaternKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from botorch.acquisition import UpperConfidenceBound, ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf, optimize_acqf_discrete\n",
    "\n",
    "from shap.explainers import Exact\n",
    "import shap\n",
    "\n",
    "from NestedAE.nn_utils import check_dict_key_exists, set_layer_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e523149a",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85437c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_module = 'ae1'\n",
    "class AE(Module):\n",
    "    def __init__(self, module_params):\n",
    "        super(AE, self).__init__()\n",
    "        ae_modules = {}\n",
    "        # Outer loop iterates over the ae_modules\n",
    "        for module_name, module_dict in module_params['modules'].items():\n",
    "            layer_list = ModuleList()\n",
    "            # Check for existence of keys or take defualts if not present\n",
    "            if check_dict_key_exists('hidden_layers', module_dict):\n",
    "                hidden_layers = module_dict['hidden_layers']\n",
    "            else:\n",
    "                hidden_layers = 0\n",
    "            if check_dict_key_exists('hidden_dim', module_dict):\n",
    "                hidden_dim = module_dict['hidden_dim']\n",
    "            else:\n",
    "                hidden_dim = None\n",
    "            if check_dict_key_exists('hidden_activation', module_dict):\n",
    "                hidden_activation = module_dict['hidden_activation']\n",
    "            else:\n",
    "                hidden_activation = None\n",
    "            if check_dict_key_exists('output_activation', module_dict):\n",
    "                output_activation = module_dict['output_activation']\n",
    "            else:\n",
    "                output_activation = None\n",
    "            if check_dict_key_exists('layer_dropout', module_dict):\n",
    "                layer_dropout = module_dict['layer_dropout']\n",
    "            else:\n",
    "                layer_dropout = None\n",
    "            if check_dict_key_exists('layer_kernel_init', module_dict):\n",
    "                layer_kernel_init = module_dict['layer_kernel_init']\n",
    "            else:\n",
    "                layer_kernel_init = None\n",
    "            if check_dict_key_exists('layer_bias_init', module_dict):\n",
    "                layer_bias_init = module_dict['layer_bias_init']\n",
    "            else:\n",
    "                layer_bias_init = None\n",
    "            if check_dict_key_exists('load_params', module_dict):\n",
    "                load_params = module_dict['load_params']\n",
    "            else:\n",
    "                load_params = False\n",
    "\n",
    "            num_layers = hidden_layers + 1\n",
    "            for layer_num in range(num_layers):\n",
    "                if layer_num == 0:\n",
    "                    # Calculate the input dimensions to first layer\n",
    "                    input_dim = module_dict['input_dim']\n",
    "\n",
    "                    if hidden_dim is not None:\n",
    "                        layer_list.append(Linear(in_features=input_dim,\n",
    "                                                out_features=module_dict['hidden_dim'],\n",
    "                                                bias=True))\n",
    "                    else:\n",
    "                        layer_list.append(Linear(in_features=input_dim,\n",
    "                                                out_features=module_dict['output_dim'],\n",
    "                                                bias=True))\n",
    "                        if output_activation:\n",
    "                            layer_list.append(output_activation)\n",
    "                        break # Only output layer\n",
    "                elif layer_num == num_layers - 1:\n",
    "                    layer_list.append(Linear(in_features=module_dict['hidden_dim'],\n",
    "                                                out_features=module_dict['output_dim'],\n",
    "                                                bias=True))\n",
    "                    if output_activation:\n",
    "                        layer_list.append(output_activation)\n",
    "                    break # Dont add hidden activations\n",
    "                else:\n",
    "                    layer_list.append(Linear(in_features=module_dict['hidden_dim'],\n",
    "                                                out_features=module_dict['hidden_dim'],\n",
    "                                                bias=True))\n",
    "                # Add hidden activations if specified\n",
    "                if hidden_activation:\n",
    "                    layer_list.append(hidden_activation)\n",
    "                if layer_dropout:\n",
    "                    layer_list.append(layer_dropout)\n",
    "            # Initialize weights for all layers\n",
    "            if layer_kernel_init:\n",
    "                layer_list = set_layer_init(layer_list, module_dict, init='kernel')\n",
    "            if layer_bias_init:\n",
    "                layer_list = set_layer_init(layer_list, module_dict, init='bias')\n",
    "\n",
    "            # Finally add to ae_module list\n",
    "            ae_modules[module_name] = layer_list\n",
    "        self.ae_modules = ModuleDict(ae_modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        # Stores all module outputs\n",
    "        ae_module_outputs = {}\n",
    "\n",
    "        # Pass through encoder\n",
    "        for j, layer in enumerate(self.ae_modules['encoder']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['z'] = layer(x)\n",
    "            else:\n",
    "                ae_module_outputs['z'] = layer(ae_module_outputs['z'])\n",
    "\n",
    "        # # Pass through predictor\n",
    "        # for j, layer in enumerate(self.ae_modules['bandgaps_predictor']):\n",
    "        #     if j == 0:\n",
    "        #         ae_module_outputs['y1_pred'] = layer(ae_module_outputs['z'])\n",
    "        #     else:\n",
    "        #         ae_module_outputs['y1_pred'] = layer(ae_module_outputs['y1_pred'])\n",
    "\n",
    "        # Pass through predictor\n",
    "        for j, layer in enumerate(self.ae_modules['BE_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['y1_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['y1_pred'] = layer(ae_module_outputs['y1_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['latents_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['latents_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['latents_pred'] = layer(ae_module_outputs['latents_pred'])\n",
    "\n",
    "        # for j, layer in enumerate(self.ae_modules['A_predictor']):\n",
    "        #     if j == 0:\n",
    "        #         ae_module_outputs['design1_pred'] = layer(ae_module_outputs['z'])\n",
    "        #     else:\n",
    "        #         ae_module_outputs['design1_pred'] = layer(ae_module_outputs['design1_pred'])\n",
    "\n",
    "        # for j, layer in enumerate(self.ae_modules['B_predictor']):\n",
    "        #     if j == 0:\n",
    "        #         ae_module_outputs['design2_pred'] = layer(ae_module_outputs['z'])\n",
    "        #     else:\n",
    "        #         ae_module_outputs['design2_pred'] = layer(ae_module_outputs['design2_pred'])\n",
    "\n",
    "        # for j, layer in enumerate(self.ae_modules['X_predictor']):\n",
    "        #     if j == 0:\n",
    "        #         ae_module_outputs['design3_pred'] = layer(ae_module_outputs['z'])\n",
    "        #     else:\n",
    "        #         ae_module_outputs['design3_pred'] = layer(ae_module_outputs['design3_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['solvent_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['design1_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['design1_pred'] = layer(ae_module_outputs['design1_pred'])\n",
    "\n",
    "        return ae_module_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f196eb",
   "metadata": {},
   "source": [
    "#### Datasets Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from BorutaShap import BorutaShap\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# For bandgaps and binding  energy combined dataset\n",
    "# dataset_loc = 'datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedae_dataset/perov_bandgaps_PBE_arun_reduced.csv'\n",
    "# For this dataset : \n",
    "dataset_loc = 'datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedae_dataset/perov_solv_BE_for_nestedhd.csv'\n",
    "# sheet_name = ''\n",
    "\n",
    "#########################################\n",
    "# FOR ABX3 BANDGAP DATASET \n",
    "#########################################\n",
    "\n",
    "# latent_col_names = []\n",
    "latent_col_names = ['l0', 'l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9']\n",
    "\n",
    "# Descriptors when just using the binding energy dataset\n",
    "# descriptors = ['A_IONRAD',\n",
    "#                 'A_MASS',\n",
    "#                 'A_DPM',\n",
    "#                 # 'B_IONRAD',\n",
    "#                 # 'B_MASS',\n",
    "#                 # 'B_EA',\n",
    "#                 # 'B_IE',\n",
    "#                 # 'B_En',\n",
    "#                 # 'B_AN',\n",
    "#                 'X_IONRAD',\n",
    "#                 'X_MASS',\n",
    "#                 'X_EA',\n",
    "#                 'X_IE',\n",
    "#                 'X_En',\n",
    "#                 'X_AN',\n",
    "#                 'SOLV_DENSITY',\n",
    "#                 'SOLV_DIELECTRIC',\n",
    "#                 'SOLV_GDN',\n",
    "#                 'SOLV_DPM',\n",
    "#                 'SOLV_MV',\n",
    "#                 'SOLV_UMBO'\n",
    "#             ]\n",
    "\n",
    "# descriptors = ['A_IONRAD',\n",
    "#                'A_MASS',\n",
    "#                'A_DPM',\n",
    "#                'B_IONRAD',\n",
    "#                'B_MASS',\n",
    "#                'B_EA',\n",
    "#                'B_IE',\n",
    "#                'B_En',\n",
    "#                'B_AN',\n",
    "#                'X_IONRAD',\n",
    "#                'X_MASS',\n",
    "#                'X_EA',\n",
    "#                'X_IE',\n",
    "#                'X_En',\n",
    "#                'X_AN'\n",
    "# ]\n",
    "descriptors = ['SOLV_DENSITY',\n",
    "               'SOLV_DIELECTRIC',\n",
    "               'SOLV_GDN',\n",
    "               'SOLV_DPM',\n",
    "               'SOLV_MV',\n",
    "               'SOLV_UMBO']\n",
    "\n",
    "# target = ['Gap']\n",
    "target = ['Target']\n",
    "\n",
    "# target_A_ion = ['K', 'Rb', 'Cs', 'MA', 'FA']\n",
    "# target_B_ion = ['Ca', 'Sr', 'Ba', 'Ge', 'Sn', 'Pb']\n",
    "# target_X_ion = ['Cl', 'Br', 'I']\n",
    "target_solvent = ['DMSO', 'THTO', 'DMF', 'NMP', 'ACETONE', 'METHA', 'GBL', 'NITRO']\n",
    "\n",
    "standardize_descs = True\n",
    "\n",
    "split_strategy = 'strat_kfold'\n",
    "train_split = 0.9\n",
    "# For dataset 1\n",
    "# defined_qs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# For dataset 2\n",
    "defined_qs = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "plot_train_test_dist = False\n",
    "\n",
    "plot_pcc_matrix = False\n",
    "\n",
    "model_save_dir = 'hd2_bandgaps_THEN_perov_solv_BE'\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "X_dataframe = pd.read_csv(dataset_loc)[descriptors + latent_col_names]\n",
    "# Y_dataframe = pd.read_csv(dataset_loc)[target + target_A_ion + target_B_ion + target_X_ion]\n",
    "Y_dataframe = pd.read_csv(dataset_loc)[target + target_solvent]\n",
    "\n",
    "if standardize_descs:\n",
    "    desc_means = []\n",
    "    desc_std_devs = []\n",
    "    for desc in X_dataframe.columns.tolist():\n",
    "        mean = X_dataframe[desc].mean()\n",
    "        desc_means.append(mean)\n",
    "        std_dev = X_dataframe[desc].std()\n",
    "        desc_std_devs.append(std_dev)\n",
    "        X_dataframe[desc] = (X_dataframe[desc] - mean) / std_dev\n",
    "    print('Descriptors standardized.')\n",
    "else:\n",
    "    print('Descriptors not standardized.')\n",
    "\n",
    "print(f'Dataframe Statistics : {X_dataframe.describe()}')\n",
    "\n",
    "print(f'Dataset columns : \\n')\n",
    "print(X_dataframe.columns)\n",
    "# dataset = np.concatenate((X_dataframe.to_numpy(dtype=np.float32), \n",
    "#                           Y_dataframe[target].to_numpy(dtype=np.float32),\n",
    "#                           Y_dataframe[target_A_ion].to_numpy(dtype=np.float32),\n",
    "#                           Y_dataframe[target_B_ion].to_numpy(dtype=np.float32),\n",
    "#                           Y_dataframe[target_X_ion].to_numpy(dtype=np.float32)),\n",
    "#                           axis=1)\n",
    "dataset = np.concatenate((X_dataframe.to_numpy(dtype=np.float32), \n",
    "                          Y_dataframe[target].to_numpy(dtype=np.float32),\n",
    "                          Y_dataframe[target_solvent].to_numpy(dtype=np.float32)), axis=1)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5618877",
   "metadata": {},
   "source": [
    "#### Data Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade12cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs = []\n",
    "val_idxs = []\n",
    "if split_strategy == 'kfold':\n",
    "    kf = KFold(n_splits=int(1/(1 - train_split)), shuffle=True, random_state=random_state)\n",
    "    for (train_idx, val_idx) in kf.split(dataset):\n",
    "        train_idxs.append(train_idx)\n",
    "        val_idxs.append(val_idx)\n",
    "elif split_strategy == 'strat_kfold':\n",
    "    print('Using a stratified k fold split strategy.')\n",
    "    y = Y_dataframe[target].to_numpy(dtype=np.float32)\n",
    "    skf = StratifiedKFold(n_splits=int(1/(1 - train_split)), shuffle=True, random_state=random_state)\n",
    "    y_binned = np.digitize(y, np.quantile(y, defined_qs))\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(dataset, y_binned)):\n",
    "        train_idxs.append(train_idx)\n",
    "        val_idxs.append(val_idx)\n",
    "        ks_stat, p_val = ks_2samp(y[train_idx], y[val_idx])\n",
    "        print(f'Fold {fold} ks-stat for target: {np.round(ks_stat, 3)}, p-value: {np.round(p_val, 3)}')\n",
    "elif split_strategy == 'random':    \n",
    "    # Train - Test split of indices\n",
    "    idxs = list(range(X_dataframe.shape[0]))\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.shuffle(idxs)\n",
    "        train_idxs.append(idxs[:int(train_split*len(idxs))])\n",
    "        val_idxs.append(idxs[int(train_split*len(idxs)):])\n",
    "else:\n",
    "    raise ValueError(f'Unknown split strategy: {split_strategy}')\n",
    "\n",
    "# for_fold = 5\n",
    "# # Check how do the histograms of train and test distribution match up\n",
    "# if plot_train_test_dist:\n",
    "#     X_dataframe_train = X_dataframe.iloc[train_idxs[for_fold]]\n",
    "#     Y_dataframe_train = Y_dataframe.iloc[train_idxs[for_fold]]\n",
    "#     X_dataframe_test = X_dataframe.iloc[val_idxs[for_fold]]\n",
    "#     Y_dataframe_test = Y_dataframe.iloc[val_idxs[for_fold]]\n",
    "#     # for col in X_dataframe.columns:\n",
    "#     #     # ks_1samp takes distribution of sample we want to compare (F(x)) against a continuous dist. (G(x))\n",
    "#     #     ks_stat, p_value = ks_2samp(X_dataframe_train[col], X_dataframe_test[col])\n",
    "#     #     plt.figure(figsize=(5, 3))\n",
    "#     #     sns.histplot(X_dataframe_train[col], kde=True, label='Train', color='blue', stat='probability', binwidth=0.1)\n",
    "#     #     sns.histplot(X_dataframe_test[col], kde=True, label='Test', color='red', stat='probability', binwidth=0.1)\n",
    "#     #     plt.title(f'Distribution of {col}')\n",
    "#     #     plt.xlabel(col)\n",
    "#     #     plt.ylabel('Density')\n",
    "#     #     plt.figtext(0.15, 0.8, f'KS-stat: {ks_stat:.3f}\\nP-value: {p_value:.3f}', bbox=dict(facecolor='white', alpha=0.5))\n",
    "#     #     plt.legend()\n",
    "#     #     plt.show()\n",
    "\n",
    "#     # Combine all histograms into a single plot\n",
    "#     fig, ax = plt.subplots(3, 5, figsize=(20, 10))\n",
    "#     ax = ax.flatten()\n",
    "#     for col in X_dataframe.columns:\n",
    "#         ks_stat, p_value = ks_2samp(X_dataframe_train[col], X_dataframe_test[col])\n",
    "#         # plt.figure(figsize=(5, 3))\n",
    "#         ax = fig.add_subplot(3, 5, X_dataframe.columns.get_loc(col)+1)\n",
    "#         ax.set_title(f'Distribution of {col}')\n",
    "#         ax.set_xlabel(col)\n",
    "#         ax.set_ylabel('Density')\n",
    "#         # ax.set_figtext(0.15, 0.8, f'KS-stat: {ks_stat:.3f}\\nP-value: {p_value:.3f}', bbox=dict(facecolor='white', alpha=0.5))\n",
    "#         sns.histplot(X_dataframe_train[col], kde=False, label='Train', color='blue', stat='probability', binwidth=0.1)\n",
    "#         sns.histplot(X_dataframe_test[col], kde=False, label='Test', color='red', stat='probability', binwidth=0.1)\n",
    "#         ax.text(0.5, 0.8, f'KS-stat: {ks_stat:.3f}\\nP-value: {p_value:.3f}', transform=ax.transAxes,\n",
    "#                 bbox=dict(facecolor='white', alpha=0.5), horizontalalignment='center')\n",
    "#         # Remove axis label\n",
    "#         ax.set_xlabel('')\n",
    "#         ax.set_ylabel('')\n",
    "#         ax.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d4d59",
   "metadata": {},
   "source": [
    "### Pearson Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the pearson correlation coefficients (ALWAYS USE THE TRAINING DATASET TO AVOID DATA LEAKAGE)\n",
    "# choose_descriptors = latent_col_names + descriptors + target + target_A_ion + target_B_ion + target_X_ion\n",
    "choose_descriptors = latent_col_names + descriptors + target_solvent\n",
    "choose_train_idx = 0\n",
    "if plot_pcc_matrix:\n",
    "    pcc = np.round(np.corrcoef(x=dataset[train_idxs[choose_train_idx]][:,:len(choose_descriptors)], rowvar=False), 2)\n",
    "    # Ref : Gryffin, https://online.ucpress.edu/collabra/article/9/1/87615/197169/A-Brief-Note-on-the-Standard-Error-of-the-Pearson\n",
    "    std_err_pcc = 1/((dataset[train_idxs[0]].shape[0] - 3)**0.5)\n",
    "    print(f'Standard error in PCC : {std_err_pcc}')\n",
    "    adj_pcc = (np.abs(pcc) - std_err_pcc)/(1 - std_err_pcc)\n",
    "    adj_pcc[adj_pcc < 0] = 0  # Ensure no negative values in adjusted PCC\n",
    "\n",
    "    # Add correlatin coeff values to the plot\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    pcc_plot = ax.matshow(adj_pcc, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    for i in range(adj_pcc.shape[0]):\n",
    "        for j in range(adj_pcc.shape[1]):\n",
    "            plt.text(j, i, f'{adj_pcc[i, j]:.2f}', ha='center', va='center', color='black')\n",
    "    ax.set_yticks(ticks=np.arange(len(choose_descriptors)), labels=choose_descriptors, rotation=0)\n",
    "    ax.set_xticks(ticks=np.arange(len(choose_descriptors)), labels=choose_descriptors, rotation=90)\n",
    "    ax.set_yticks(ticks=np.arange(len(choose_descriptors)))\n",
    "    ax.set_xticks(ticks=np.arange(len(choose_descriptors)))\n",
    "    # Add the colorbar\n",
    "    cbar = plt.colorbar(pcc_plot, ax=ax, fraction=0.0455)\n",
    "    # cbar.set_label('Adjusted Pearson Correlation Coefficient', rotation=270, labelpad=20)\n",
    "    # plt.title('Adjusted Pearson Correlation Coefficient Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ff19d",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Training params\n",
    "num_epochs = 2000\n",
    "lr = 0.01\n",
    "# Optimization loss params\n",
    "l2_coeff = 0\n",
    "l1_coeff = 0.001\n",
    "prop_l1_coeff = 0.001\n",
    "# Model params\n",
    "num_y1_latents = 1\n",
    "# Printing params\n",
    "print_every_n_batches = 100\n",
    "print_losses = False\n",
    "debug = False\n",
    "pred_lam = 1\n",
    "design_lam = 1\n",
    "latent_lam = 10\n",
    "##########################\n",
    "\n",
    "class Arctanh(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.atanh(torch.clamp(x, -0.999999, 0.999999))\n",
    "    \n",
    "atanh_act_fn = Arctanh()\n",
    "\n",
    "latent_dim = 6\n",
    "\n",
    "module_params = {'name':'AE1', \n",
    "                    'modules':{\n",
    "\n",
    "                        'encoder':{\n",
    "                            'input_dim':16,\n",
    "                            'output_dim':latent_dim, \n",
    "                            'hidden_dim':25, \n",
    "                            'hidden_layers':1, \n",
    "                            'hidden_activation':None, \n",
    "                            'output_activation':torch.nn.Tanh(), \n",
    "                            'layer_kernel_init':'xavier_normal', \n",
    "                            'layer_bias_init':'zeros', \n",
    "                            },\n",
    "\n",
    "                        # 'bandgaps_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':1,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':torch.nn.ReLU(),\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'},\n",
    "\n",
    "                        'BE_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':1,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':torch.nn.ReLU(),\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        },\n",
    "\n",
    "                        'latents_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':10,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':None,\n",
    "                            'output_activation':torch.nn.Tanh(),\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'},\n",
    "\n",
    "                        # 'A_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':5,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        # 'B_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':6,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        # 'X_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':3,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        'solvent_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':8,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':None,\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        },\n",
    "\n",
    "                    }}\n",
    "\n",
    "train_total_pred_loss_per_epoch_per_seed = []\n",
    "train_y_pred_loss_per_epoch_per_seed = []\n",
    "train_design_pred_loss_per_epoch_per_seed = []\n",
    "train_latent_pred_loss_per_epoch_per_seed = []\n",
    "train_x_pred_loss_per_epoch_per_seed = []\n",
    "\n",
    "val_y_pred_loss_per_epoch_per_seed = []\n",
    "val_design_pred_loss_per_epoch_per_seed = []\n",
    "val_latent_pred_loss_per_epoch_per_seed = []\n",
    "val_x_pred_loss_per_epoch_per_seed = []\n",
    "\n",
    "ind_losses_dict_train = {}\n",
    "ind_losses_dict_val = {}\n",
    "\n",
    "# for i, seed in enumerate(seeds):\n",
    "for i in range(len(train_idxs)):\n",
    "    print('\\n')\n",
    "    print(f'Fold {i}')\n",
    "    print('\\n' )\n",
    "    train_dataset = dataset[train_idxs[i]]\n",
    "    val_dataset = dataset[val_idxs[i]]\n",
    "\n",
    "    print(f'Train numpy dataset shape : {train_dataset.shape}, Val. numpy dataset shape : {val_dataset.shape}')\n",
    "\n",
    "    torch_train_dataset = torch.from_numpy(train_dataset).to(dtype=torch.float32)\n",
    "    train_data_loader = DataLoader(torch_train_dataset, batch_size=train_dataset.shape[0], shuffle=True)\n",
    "\n",
    "    torch_val_dataset = torch.from_numpy(val_dataset).to(dtype=torch.float32)\n",
    "    val_data_loader = DataLoader(torch_val_dataset, batch_size=val_dataset.shape[0], shuffle=False)\n",
    "\n",
    "    # RANDOM USED HERE - Param init\n",
    "    # Delete previos model\n",
    "    if i > 0:\n",
    "        print(\"Deleting previous model\")\n",
    "        del ae\n",
    "        ae = None\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    ae = AE(module_params)\n",
    "    print(ae)\n",
    "\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    adam = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=l2_coeff)\n",
    "    x_l1_loss = L1Loss(reduction='mean')\n",
    "\n",
    "    y1_l1_loss = L1Loss(reduction='mean')\n",
    "\n",
    "    design1_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    design2_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    design3_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    latent_l1_loss = L1Loss(reduction='mean')\n",
    "    def latent_cosine_loss(latent_pred, latent_true):\n",
    "        cosine_loss = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        return torch.mean(cosine_loss(latent_pred, latent_true))\n",
    "\n",
    "    train_total_pred_loss_per_epoch = []\n",
    "\n",
    "    train_y_pred_loss_per_epoch = []\n",
    "    train_design_pred_loss_per_epoch = []\n",
    "    train_latent_pred_loss_per_epoch = []\n",
    "    train_x_pred_loss_per_epoch = []\n",
    "\n",
    "    val_y_pred_loss_per_epoch = []\n",
    "    val_design_pred_loss_per_epoch = []\n",
    "    val_latent_pred_loss_per_epoch = []\n",
    "    val_x_pred_loss_per_epoch = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_total_pred_loss_per_batch = 0\n",
    "        train_y_pred_loss_per_batch = 0\n",
    "        train_design_pred_loss_per_batch = 0\n",
    "        train_latent_pred_loss_per_batch = 0\n",
    "        train_x_pred_loss_per_batch = 0\n",
    "\n",
    "        # Train Loop\n",
    "        print(f' --------- Epoch Stats {epoch+1}/{num_epochs} --------- ')\n",
    "        for batch, data in enumerate(train_data_loader):\n",
    "            ae.train()\n",
    "\n",
    "            #\n",
    "            # Select the X and y data\n",
    "            #\n",
    "\n",
    "            x = data[:, 0:(len(descriptors) + len(latent_col_names))]\n",
    "            \n",
    "            if len(latent_col_names) > 0:\n",
    "                latents = data[:, len(descriptors):(len(descriptors) + len(latent_col_names))]\n",
    "            \n",
    "            # Add additional property predictions here ..\n",
    "            y1 = data[:, len(descriptors) + len(latent_col_names)]\n",
    "\n",
    "            # For dataset 1\n",
    "            # design1 = data[:, (len(descriptors) + len(latent_col_names) + 1):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion))]\n",
    "            # design2 = data[:, (len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion)):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion))]\n",
    "            # design3 = data[:, (len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion)):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion) + len(target_X_ion))]\n",
    "            # # For dataset 2\n",
    "            design1 = data[:, (len(descriptors) + len(latent_col_names) + 1):(len(descriptors) + len(latent_col_names) + 1 + len(target_solvent))]\n",
    "\n",
    "            #\n",
    "            # Extract the AE prediuctions\n",
    "            #\n",
    "\n",
    "            # sgd.zero_grad()\n",
    "            adam.zero_grad()\n",
    "            ae_out = ae(x)\n",
    "            y1_pred, z = ae_out['y1_pred'], ae_out['z']\n",
    "            # x_pred = ae_out['x_pred']\n",
    "\n",
    "            # For dataset 1\n",
    "            # design1_pred, design2_pred, design3_pred = ae_out['design1_pred'], ae_out['design2_pred'], ae_out['design3_pred']\n",
    "            # For dataset 2\n",
    "            design1_pred = ae_out['design1_pred']\n",
    "\n",
    "            if len(latent_col_names) > 0:\n",
    "                latents_pred = ae_out['latents_pred']\n",
    "            \n",
    "            #\n",
    "            #  Calculate the prediction losses\n",
    "            #\n",
    "\n",
    "            train_y1_pred_loss = y1_l1_loss(y1_pred, y1.reshape(-1, 1))\n",
    "            if print_losses:print('train_y1_pred_loss:', train_y1_pred_loss)\n",
    "            train_y_pred_loss = train_y1_pred_loss\n",
    "\n",
    "            #\n",
    "            # Calculate the design predictions losses\n",
    "            #\n",
    "\n",
    "            train_design1_pred_loss = design1_loss(design1_pred, design1)\n",
    "            if print_losses: print('train_design1_pred_loss:', train_design1_pred_loss)\n",
    "\n",
    "            # train_design2_pred_loss = design2_loss(design2_pred, design2)\n",
    "            # if print_losses: print('train_design2_pred_loss:', train_design2_pred_loss)\n",
    "\n",
    "            # train_design3_pred_loss = design3_loss(design3_pred, design3)\n",
    "            # if print_losses: print('train_design3_pred_loss:', train_design3_pred_loss)\n",
    "\n",
    "            # train_design_pred_loss = train_design1_pred_loss + train_design2_pred_loss + train_design3_pred_loss \n",
    "            train_design_pred_loss = train_design1_pred_loss\n",
    "\n",
    "            #\n",
    "            # Calculate the latent losses\n",
    "            #\n",
    "\n",
    "            if len(latent_col_names) > 0:\n",
    "                # train_latent_pred_loss = latent_l1_loss(latents_pred, latents)\n",
    "                train_latent_pred_loss = latent_cosine_loss(latents_pred, latents)\n",
    "                print(f'latent l1 loss : {latent_l1_loss(latents_pred, latents)}')\n",
    "            else:\n",
    "                train_latent_pred_loss = torch.tensor(0)\n",
    "            if print_losses: print('train_latent_pred_loss:', train_latent_pred_loss)\n",
    "\n",
    "            #\n",
    "            # Calculate X prediction losses\n",
    "            #\n",
    "                    \n",
    "            # train_x_pred_loss = x_l1_loss(x_pred, x)\n",
    "            # if print_losses: print('train_x_pred_loss:', train_x_pred_loss)\n",
    "\n",
    "            train_total_pred_loss = pred_lam*train_y_pred_loss + design_lam*train_design_pred_loss + latent_lam*train_latent_pred_loss\n",
    "            # train_total_pred_loss = pred_lam*train_y_pred_loss + design_lam*train_design_pred_loss + latent_lam*train_latent_pred_loss + train_x_pred_loss\n",
    "\n",
    "            # Get index of max value fro each row of design_pred_idxs\n",
    "            design1_pred_idxs_train = torch.argmax(torch.softmax(design1_pred, dim=1), dim=1)\n",
    "            design1_true_idxs_train = torch.argmax(design1, dim=1)\n",
    "            if debug:\n",
    "                print(f'First 20 predicted indices train : {design1_pred_idxs_train[:20]}')\n",
    "                print(f'First 20 true indices train : {design1_true_idxs_train[:20]}')\n",
    "            # Calculate train accuracy\n",
    "            design1_train_accuracy = (design1_pred_idxs_train == design1_true_idxs_train).float().mean().item() * 100\n",
    "            if print_losses:\n",
    "                print(f'Train Accuracy : {design1_train_accuracy}')\n",
    "\n",
    "            # design2_pred_idxs_train = torch.argmax(torch.softmax(design2_pred, dim=1), dim=1)\n",
    "            # design2_true_idxs_train = torch.argmax(design2, dim=1)\n",
    "            # if debug:\n",
    "            #     print(f'First 20 predicted indices train : {design2_pred_idxs_train[:20]}')\n",
    "            #     print(f'First 20 true indices train : {design2_true_idxs_train[:20]}')\n",
    "            # # Calculate train accuracy\n",
    "            # design2_train_accuracy = (design2_pred_idxs_train == design2_true_idxs_train).float().mean().item() * 100\n",
    "            # if print_losses:\n",
    "            #     print(f'Train Accuracy : {design2_train_accuracy}')\n",
    "\n",
    "            # design3_pred_idxs_train = torch.argmax(torch.softmax(design3_pred, dim=1), dim=1)\n",
    "            # design3_true_idxs_train = torch.argmax(design3, dim=1)\n",
    "            # if debug:\n",
    "            #     print(f'First 20 predicted indices train : {design3_pred_idxs_train[:20]}')\n",
    "            #     print(f'First 20 true indices train : {design3_true_idxs_train[:20]}')\n",
    "            # # Calculate train accuracy\n",
    "            # design3_train_accuracy = (design3_pred_idxs_train == design3_true_idxs_train).float().mean().item() * 100\n",
    "            # if print_losses:\n",
    "            #     print(f'Train Accuracy : {design3_train_accuracy}')\n",
    "\n",
    "            if l1_coeff > 0:\n",
    "                for name, param in ae.named_parameters():\n",
    "                    # Exclude 'BE_preedictor' from L1\n",
    "                    if 'BE_predictor' in name:\n",
    "                        continue\n",
    "                    else:\n",
    "                        train_total_pred_loss += l1_coeff * param.abs().sum()\n",
    "\n",
    "            if prop_l1_coeff > 0:\n",
    "                for name, param in ae.named_parameters():\n",
    "                    # Only include 'BE_preedictor' from L1\n",
    "                    if 'BE_predictor' in name:\n",
    "                        train_total_pred_loss += prop_l1_coeff * param.abs().sum()\n",
    "                    else:\n",
    "                        continue\n",
    "            \n",
    "            # # ---  STEP 1  --- : Compute pearson correlation coefficients\n",
    "            # # --- STEP 1.1 --- : with respect to target\n",
    "            rho_abs_w_target1 = torch.empty(num_y1_latents)\n",
    "            y1_mean, y1_std = y1.mean(), y1.std()\n",
    "            for j, l in enumerate(range(num_y1_latents)):\n",
    "                z_mean, z_std = z[:, l].mean(), z[:, l].std()\n",
    "                Czy = ((z[:, l] - z_mean) * (y1 - y1_mean)).mean()\n",
    "                rho_abs_w_target1[j] = abs(Czy / (z_std * y1_std))\n",
    "                # # Verify correctness of correflation coefficient agsinst torch corrcoef function\n",
    "                # concat_tensor = cat((z[:, i].reshape(-1, 1), y.reshape(-1, 1)), dim=1)\n",
    "                # print('Computed using inbuilt function')\n",
    "                # print(abs(corrcoef(concat_tensor.T)[0, 1]))\n",
    "            \n",
    "            # --- STEP 1.2 --- : with respect to other latent variables\n",
    "            rho_abs_w_latents = []\n",
    "            for j in range(z.shape[1]):\n",
    "                z_j_mean, z_j_std = z[:, j].mean(), z[:, j].std()\n",
    "                for k in range(j+1, z.shape[1]):\n",
    "                    z_k_mean, z_k_std = z[:, k].mean(), z[:, k].std()\n",
    "                    Czz = ((z[:, j] - z_j_mean) * (z[:, k] - z_k_mean)).mean()\n",
    "                    rho_abs_w_latents.append(abs(Czz / (z_j_std * z_k_std)))\n",
    "            rho_abs_w_latents = torch.stack(rho_abs_w_latents)\n",
    "\n",
    "            # ---  STEP 2  --- : Compute std err in PCCs and adjust rho_abs_w_target & rho_abs_w_latents\n",
    "            std_err = 1/math.sqrt(len(y1_pred) - 3) # (Ref : Gryffin)\n",
    "            if debug : print('std_err:', std_err)\n",
    "\n",
    "            rho_abs_w_target1_adj = (rho_abs_w_target1 - std_err)/(1 - std_err)\n",
    "            rho_abs_w_target1_adj[rho_abs_w_target1_adj < 0] = 0\n",
    "            if debug : print('rho_abs_w_target1_adj :', rho_abs_w_target1_adj)\n",
    "\n",
    "            rho_abs_w_latents_adj = (rho_abs_w_latents - std_err)/(1 - std_err)\n",
    "            rho_abs_w_latents_adj[rho_abs_w_latents_adj < 0] = 0\n",
    "            if debug : print('rho_abs_w_latents_adj :', rho_abs_w_latents_adj)\n",
    "\n",
    "            # ---  STEP 3  --- : Atleast one of the PCCs wrt target should be maximized\n",
    "            lambda_0_1 = torch.mean(1 - rho_abs_w_target1_adj)\n",
    "            if debug : print('lambda_0_1:', lambda_0_1)\n",
    "            \n",
    "            # --- Step 4 --- : Favor PCCs between latents that are close to 0\n",
    "            lambda_1 = torch.mean(torch.pow(torch.sin((math.pi/2)*rho_abs_w_latents_adj), 2))\n",
    "            if debug : print('lambda_1:', lambda_1)\n",
    "            \n",
    "            # --- Step 5 --- : Add all the losses\n",
    "            train_total_pred_loss += lambda_0_1 + lambda_1\n",
    "            \n",
    "            train_total_pred_loss.backward()\n",
    "            # sgd.step()\n",
    "            adam.step()\n",
    "            # Store the losses for each batch\n",
    "            train_total_pred_loss_per_batch += train_total_pred_loss.item()\n",
    "            train_y_pred_loss_per_batch += train_y_pred_loss.item()\n",
    "            train_design_pred_loss_per_batch += train_design_pred_loss.item()\n",
    "            train_latent_pred_loss_per_batch += train_latent_pred_loss.item()\n",
    "            # train_x_pred_loss_per_batch += train_x_pred_loss.item()\n",
    "        \n",
    "        ind_losses_dict_train[f'seed_{i}'] = [train_y1_pred_loss.item(),\n",
    "                                              train_design1_pred_loss.item(),\n",
    "                                              train_latent_pred_loss.item()]\n",
    "                                            #   train_design2_pred_loss.item(), \n",
    "                                            #   train_design3_pred_loss.item()]\n",
    "\n",
    "        val_y_pred_loss_per_batch = 0\n",
    "        val_design_pred_loss_per_batch = 0\n",
    "        val_latent_pred_loss_per_batch = 0\n",
    "        val_x_pred_loss_per_batch = 0\n",
    "\n",
    "        # Validation Loop\n",
    "        for batch, data in enumerate(val_data_loader):\n",
    "            ae.eval()\n",
    "\n",
    "            x = data[:, 0:(len(descriptors) + len(latent_col_names))]\n",
    "\n",
    "            if len(latent_col_names) > 0:\n",
    "                latents = data[:, len(descriptors):(len(descriptors) + len(latent_col_names))]\n",
    "\n",
    "            # Add additional property predictions here ..\n",
    "            y1 = data[:, len(descriptors) + len(latent_col_names)]\n",
    "\n",
    "            # For dataset 1\n",
    "            # design1 = data[:, (len(descriptors) + len(latent_col_names) + 1):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion))]\n",
    "            # design2 = data[:, (len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion)):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion))]\n",
    "            # design3 = data[:, (len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion)):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion) + len(target_X_ion))]\n",
    "            # For dataset 2\n",
    "            design1 = data[:, (len(descriptors) + len(latent_col_names) + 1):(len(descriptors) + len(latent_col_names) + 1 + len(target_solvent))]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ae_out = ae(x)\n",
    "\n",
    "                y1_pred, z = ae_out['y1_pred'], ae_out['z']\n",
    "                # x_pred = ae_out['x_pred']\n",
    "\n",
    "                # design1_pred, design2_pred, design3_pred = ae_out['design1_pred'], ae_out['design2_pred'], ae_out['design3_pred']\n",
    "                design1_pred = ae_out['design1_pred']\n",
    "                if len(latent_col_names) > 0:\n",
    "                    latents_pred = ae_out['latents_pred']\n",
    "\n",
    "                val_y1_pred_loss = y1_l1_loss(y1_pred, y1.reshape(-1, 1))\n",
    "                if print_losses:\n",
    "                    print('val_y1_pred_loss:', val_y1_pred_loss)\n",
    "                val_y_pred_loss = val_y1_pred_loss\n",
    "\n",
    "                val_design1_pred_loss = design1_loss(design1_pred, design1)\n",
    "                # val_design2_pred_loss = design2_loss(design2_pred, design2)\n",
    "                # val_design3_pred_loss = design3_loss(design3_pred, design3)\n",
    "                # val_design_pred_loss = val_design1_pred_loss + val_design2_pred_loss + val_design3_pred_loss\n",
    "                val_design_pred_loss = val_design1_pred_loss\n",
    "\n",
    "                if len(latent_col_names) > 0:\n",
    "                    # val_latent_pred_loss = latent_l1_loss(latents_pred, latents)\n",
    "                    val_latent_pred_loss = latent_cosine_loss(latents_pred, latents)\n",
    "                    print(f'latent l1 loss : {latent_l1_loss(latents_pred, latents)}')\n",
    "                else:\n",
    "                    val_latent_pred_loss = torch.tensor(0)\n",
    "\n",
    "            # Get index of max value fro each row of design_pred_idxs\n",
    "            design1_pred_idxs_val = torch.argmax(torch.softmax(design1_pred, dim=1), dim=1)\n",
    "            design1_true_idxs_val = torch.argmax(design1, dim=1)\n",
    "            val_accuracy = (design1_pred_idxs_val == design1_true_idxs_val).float().mean().item() * 100\n",
    "            if print_losses:\n",
    "                print(f'Val Accuracy : {val_accuracy}')\n",
    "            \n",
    "            # design2_pred_idxs_val = torch.argmax(torch.softmax(design2_pred, dim=1), dim=1)\n",
    "            # design2_true_idxs_val = torch.argmax(design2, dim=1)\n",
    "            # val_accuracy = (design2_pred_idxs_val == design2_true_idxs_val).float().mean().item() * 100\n",
    "            # if print_losses:\n",
    "            #     print(f'Val Accuracy : {val_accuracy}')\n",
    "\n",
    "            # design3_pred_idxs_val = torch.argmax(torch.softmax(design3_pred, dim=1), dim=1)\n",
    "            # design3_true_idxs_val = torch.argmax(design3, dim=1)\n",
    "            # val_accuracy = (design3_pred_idxs_val == design3_true_idxs_val).float().mean().item() * 100\n",
    "            # if print_losses:\n",
    "            #     print(f'Val Accuracy : {val_accuracy}')\n",
    "\n",
    "            val_y_pred_loss_per_batch += val_y_pred_loss.item()\n",
    "            val_design_pred_loss_per_batch += val_design_pred_loss.item()\n",
    "            val_latent_pred_loss_per_batch += val_latent_pred_loss.item()\n",
    "            # val_x_pred_loss_per_batch += val_x_pred_loss.item()\n",
    "\n",
    "            # Printing purposes\n",
    "            if print_losses:\n",
    "                if batch % print_every_n_batches == 0:\n",
    "                    print(f'Batch {batch}/{len(val_data_loader)}, Y Pred Loss: {val_y_pred_loss.item():.4f}, Design Pred Loss: {val_design_pred_loss.item():.4f}')\n",
    "\n",
    "        # Store train loss curves\n",
    "        train_total_pred_loss_per_epoch.append(train_total_pred_loss_per_batch / len(train_data_loader))\n",
    "\n",
    "        train_y_pred_loss_per_epoch.append(train_y_pred_loss_per_batch / len(train_data_loader))\n",
    "        train_design_pred_loss_per_epoch.append(train_design_pred_loss_per_batch / len(train_data_loader))\n",
    "        train_latent_pred_loss_per_epoch.append(train_latent_pred_loss_per_batch / len(train_data_loader))\n",
    "        # train_x_pred_loss_per_epoch.append(train_x_pred_loss_per_batch / len(train_data_loader))\n",
    "\n",
    "        val_y_pred_loss_per_epoch.append(val_y_pred_loss_per_batch / len(val_data_loader))\n",
    "        val_design_pred_loss_per_epoch.append(val_design_pred_loss_per_batch / len(val_data_loader))\n",
    "        val_latent_pred_loss_per_epoch.append(val_latent_pred_loss_per_batch / len(val_data_loader))\n",
    "        # val_x_pred_loss_per_epoch.append(val_x_pred_loss_per_batch / len(val_data_loader))\n",
    "\n",
    "        if print_losses:\n",
    "            print(f' --------- Epoch Stats {epoch+1}/{num_epochs} --------- ')\n",
    "            print(f' -- Train -- Total Loss: {train_total_pred_loss_per_epoch[-1]:.4f},\\\n",
    "                                 Y Pred Loss: {train_y_pred_loss_per_epoch[-1]:.4f},\\\n",
    "                                 Design Pred Loss: {train_design_pred_loss_per_epoch[-1]:.4f},\\\n",
    "                                 Latent Pred Loss: {train_latent_pred_loss_per_epoch[-1]:.4f}')\n",
    "            print(f' --  Val  -- Y Pred Loss: {val_y_pred_loss_per_epoch[-1]:.4f},\\\n",
    "                                 Design Pred Loss: {val_design_pred_loss_per_epoch[-1]:.4f},\\\n",
    "                                 Latent Pred Loss: {val_latent_pred_loss_per_epoch[-1]:.4f}')\n",
    "            print(f' ------------------------------------------')\n",
    "\n",
    "    ind_losses_dict_val[f'seed_{i}'] = [val_y1_pred_loss.item(), \n",
    "                                        val_design1_pred_loss.item(), \n",
    "                                        val_latent_pred_loss.item()]\n",
    "                                        # val_design2_pred_loss.item(), \n",
    "                                        # val_design3_pred_loss.item()]\n",
    "                                        # val_x_pred_loss.item()]\n",
    "\n",
    "    train_total_pred_loss_per_epoch_per_seed.append(train_total_pred_loss_per_epoch)\n",
    "    train_y_pred_loss_per_epoch_per_seed.append(train_y_pred_loss_per_epoch)\n",
    "    train_design_pred_loss_per_epoch_per_seed.append(train_design_pred_loss_per_epoch)\n",
    "    train_latent_pred_loss_per_epoch_per_seed.append(train_latent_pred_loss_per_epoch)\n",
    "    # train_x_pred_loss_per_epoch_per_seed.append(train_x_pred_loss_per_epoch)\n",
    "\n",
    "    val_y_pred_loss_per_epoch_per_seed.append(val_y_pred_loss_per_epoch)\n",
    "    val_design_pred_loss_per_epoch_per_seed.append(val_design_pred_loss_per_epoch)\n",
    "    val_latent_pred_loss_per_epoch_per_seed.append(val_latent_pred_loss_per_epoch)\n",
    "    # val_x_pred_loss_per_epoch_per_seed.append(val_x_pred_loss_per_epoch)\n",
    "\n",
    "    # Save the model to the runs directory\n",
    "    model_save_path = f'runs/{model_save_dir}/fold{i}_soft_constraints'\n",
    "    torch.save(ae.state_dict(), model_save_path)\n",
    "    print(f'Model saved to {model_save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201693e1",
   "metadata": {},
   "source": [
    "### Choosing one of the models trained on the k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d33975",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "folds = np.arange(0, 10, 1)\n",
    "all_train_pred_losses = np.array([ind_losses_dict_train[f'seed_{fold}'] for fold in folds])\n",
    "all_val_pred_losses = np.array([ind_losses_dict_val[f'seed_{fold}'] for fold in folds])\n",
    "# ax[0].plot(folds, all_train_pred_losses[:, 0].squeeze(), label='Train Y1 Loss', marker='o', linestyle='-')\n",
    "# ax[0].plot(folds, all_train_pred_losses[:, 1], label='Train Design1 Loss', marker='o', linestyle='-')\n",
    "ax[0].plot(folds, all_train_pred_losses[:, 2], label='Train Latent Loss', marker='o', linestyle='-')\n",
    "# ax[0].plot(folds, all_train_pred_losses[:, 2], label='Train Design2 Loss', marker='o', linestyle='-')\n",
    "# ax[0].plot(folds, all_train_pred_losses[:, 3], label='Train Design3 Loss', marker='o', linestyle='-')\n",
    "ax[0].set_xlabel('Fold')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Train Losses per Fold')\n",
    "ax[0].legend()\n",
    "ax[0].set_xticks(folds)\n",
    "# ax[1].plot(folds, all_val_pred_losses[:, 0], label='Val Y1 Loss', marker='o', linestyle='-')\n",
    "# ax[1].plot(folds, all_val_pred_losses[:, 1], label='Val Design1 Loss', marker='o', linestyle='-')\n",
    "ax[1].plot(folds, all_val_pred_losses[:, 2], label='Val Latent Loss', marker='o', linestyle='-')\n",
    "# ax[1].plot(folds, all_val_pred_losses[:, 2], label='Val Design2 Loss', marker='o', linestyle='-')\n",
    "# ax[1].plot(folds, all_val_pred_losses[:, 3], label='Val Design3 Loss', marker='o', linestyle='-')\n",
    "ax[1].set_xlabel('Fold')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Val Losses per Fold')\n",
    "ax[1].legend()\n",
    "ax[1].set_xticks(folds)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(all_train_pred_losses)\n",
    "print(all_val_pred_losses)\n",
    "\n",
    "print(np.sum(all_val_pred_losses, axis=1))\n",
    "print(np.argmin(np.sum(all_val_pred_losses, axis=1)))\n",
    "# ax[0].plot(folds, ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c9dea",
   "metadata": {},
   "source": [
    "### Load the selected trained AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba080c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arctanh(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.atanh(torch.clamp(x, -0.999999, 0.999999))\n",
    "    \n",
    "atanh_act_fn = Arctanh()\n",
    "\n",
    "saved_module_name = 'hd2'\n",
    "latent_dim = 6\n",
    "fold_num = 7\n",
    "module_params = {'name':'AE1', \n",
    "                    'modules':{\n",
    "                        \n",
    "                        # AE1 encoder = AE2 encoder archiecture\n",
    "                        'encoder':{\n",
    "                            'input_dim':16,\n",
    "                            'output_dim':latent_dim, #8 \n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1, \n",
    "                            'hidden_activation':None, \n",
    "                            'output_activation':torch.nn.Tanh(), \n",
    "                            'layer_kernel_init':'xavier_normal', \n",
    "                            'layer_bias_init':'zeros', \n",
    "                            },\n",
    "\n",
    "                        # 'bandgaps_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':1,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':torch.nn.ReLU(),\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'},\n",
    "\n",
    "                        'BE_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':1,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':torch.nn.ReLU(),\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'},\n",
    "                        \n",
    "                        'latents_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':10,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':2,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':torch.nn.Tanh(),\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'},\n",
    "\n",
    "                        # 'A_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':5,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        # 'B_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':6,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        # 'X_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':3,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "                        \n",
    "                        'solvent_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':8,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':None,\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        },\n",
    "\n",
    "                    }}\n",
    "\n",
    "loaded_state_dict = f'runs/{saved_module_name}_bandgaps_THEN_perov_solv_BE/fold{fold_num}_soft_constraints_{latent_dim}D'\n",
    "loaded_ae = AE(module_params)\n",
    "loaded_ae.load_state_dict(torch.load(loaded_state_dict))\n",
    "loaded_ae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0770a16",
   "metadata": {},
   "source": [
    "### Property predictions for all samples in current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[train_idxs[fold_num]]\n",
    "val_dataset = dataset[val_idxs[fold_num]]\n",
    "print(f'Train numpy dataset shape : {train_dataset.shape}, Val. numpy dataset shape : {val_dataset.shape}')\n",
    "train_dataset_tensor = torch.from_numpy(train_dataset[:, 0:(len(descriptors) + len(latent_col_names))]).to(dtype=torch.float32)\n",
    "val_dataset_tensor = torch.from_numpy(val_dataset[:, 0:(len(descriptors) + len(latent_col_names))]).to(dtype=torch.float32)\n",
    "\n",
    "# bandgaps_true_train = train_dataset[:, 15]\n",
    "# bandgaps_true_val = val_dataset[:, 15]\n",
    "# bandgaps_true = np.concatenate((bandgaps_true_train, bandgaps_true_val), axis=0)\n",
    "\n",
    "be_true_train = train_dataset[:, 16]\n",
    "be_true_val = val_dataset[:, 16]\n",
    "be_true = np.concatenate((be_true_train, be_true_val), axis=0)\n",
    "\n",
    "loaded_ae.eval()\n",
    "with torch.no_grad():\n",
    "    ae_out_train = loaded_ae(train_dataset_tensor)\n",
    "    ae_out_val = loaded_ae(val_dataset_tensor)\n",
    "\n",
    "# bandgaps_pred_train = ae_out_train['y1_pred']\n",
    "# bandgaps_pred_val = ae_out_val['y1_pred']\n",
    "# bandgaps_pred = torch.cat((bandgaps_pred_train, bandgaps_pred_val), dim=0)\n",
    "\n",
    "be_pred_train = ae_out_train['y1_pred']\n",
    "be_pred_val = ae_out_val['y1_pred']\n",
    "be_pred = torch.cat((be_pred_train, be_pred_val), dim=0)\n",
    "\n",
    "# mhp_bgs_true_vs_pred_train = pd.DataFrame({'True Bandgaps': bandgaps_true_train, 'Predicted Bandgaps': bandgaps_pred_train.detach().numpy().squeeze()})\n",
    "# mhp_bgs_true_vs_pred_train.to_csv('bandgaps_data_train.csv', index=False)\n",
    "\n",
    "# mhp_bgs_true_vs_pred_val = pd.DataFrame({'True Bandgaps': bandgaps_true_val, 'Predicted Bandgaps': bandgaps_pred_val.numpy().squeeze()})\n",
    "# mhp_bgs_true_vs_pred_val.to_csv('bandgaps_data_val.csv', index=False)\n",
    "\n",
    "mhp_be_true_vs_pred_train = pd.DataFrame({'True Binding Energy': be_true_train, 'Predicted Binding Energy': be_pred_train.numpy().squeeze()})\n",
    "mhp_be_true_vs_pred_train.to_csv('be_data_train.csv', index=False)\n",
    "\n",
    "mhp_be_true_vs_pred_val = pd.DataFrame({'True Binding Energy': be_true_val, 'Predicted Binding Energy': be_pred_val.numpy().squeeze()})\n",
    "mhp_be_true_vs_pred_val.to_csv('be_data_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef686ea",
   "metadata": {},
   "source": [
    "### Property predictions for samples binding energy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(desc_means).reshape(-1, 1), np.array(desc_std_devs).reshape(-1, 1))\n",
    "\n",
    "# x_df = pd.read_csv('datasets/H2_prod_rate/props_from_sa_h2_rate.csv')[descriptors]\n",
    "x_df = pd.read_csv('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedae_dataset/perov_solv_BE_for_nestedhd.csv')[descriptors]\n",
    "\n",
    "for i, desc in enumerate(x_df.columns.tolist()):\n",
    "    mean = x_df[desc].mean()\n",
    "    std = x_df[desc].std()\n",
    "    x_df[desc] = (x_df[desc] - desc_means[i]) / desc_std_devs[i]\n",
    "\n",
    "x_torch = torch.from_numpy(x_df.to_numpy(dtype=np.float32)).to(dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    ae_out = loaded_ae(x_torch)\n",
    "latents = ae_out['z']\n",
    "bandgaps = ae_out['y1_pred']\n",
    "a_design_pred = torch.argmax(torch.softmax(ae_out['design1_pred'], dim=1), dim=1)\n",
    "b_design_pred = torch.softmax(ae_out['design2_pred'], dim=1)\n",
    "x_design_pred = torch.softmax(ae_out['design3_pred'], dim=1)\n",
    "np.savetxt('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedhd_HD1_preds_perov_solv_BE/a_design_pred.csv', a_design_pred.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedhd_HD1_preds_perov_solv_BE/b_design_pred.csv', b_design_pred.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedhd_HD1_preds_perov_solv_BE/x_design_pred.csv', x_design_pred.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedhd_HD1_preds_perov_solv_BE/bandgaps_pred.csv', bandgaps.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedhd_HD1_preds_perov_solv_BE/latents_pred.csv', latents.detach().numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb494698",
   "metadata": {},
   "source": [
    "#### Plot model training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latents = len(latent_col_names)\n",
    "epochs = np.arange(1, num_epochs + 1)\n",
    "# Plot mean and standard deviation of losses\n",
    "train_mean_total_pred_loss = np.mean(np.log(np.array(train_total_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "train_std_total_pred_loss = np.std(np.log(np.array(train_total_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "train_mean_y_pred_loss = np.mean(np.log(np.array(train_y_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "train_std_y_pred_loss = np.std(np.log(np.array(train_y_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "train_mean_design_pred_loss = np.mean(np.log(np.array(train_design_pred_loss_per_epoch_per_seed)), axis=0) \n",
    "train_std_design_pred_loss = np.std(np.log(np.array(train_design_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "if num_latents > 0:\n",
    "    train_mean_latent_pred_loss = np.mean(np.log(np.array(train_latent_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "    train_std_latent_pred_loss = np.std(np.log(np.array(train_latent_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "else:\n",
    "    train_mean_latent_pred_loss = np.mean(train_latent_pred_loss_per_epoch_per_seed, axis=0)\n",
    "    train_std_latent_pred_loss = np.std(train_latent_pred_loss_per_epoch_per_seed, axis=0)\n",
    "\n",
    "train_mean_x_pred_loss = np.mean(np.log(np.array(train_x_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "train_std_x_pred_loss = np.std(np.log(np.array(train_x_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "val_mean_y_pred_loss = np.mean(np.log(np.array(val_y_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "val_std_y_pred_loss = np.std(np.log(np.array(val_y_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "val_mean_design_pred_loss = np.mean(np.log(np.array(val_design_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "val_std_design_pred_loss = np.std(np.log(np.array(val_design_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "if num_latents > 0:\n",
    "    val_mean_latent_pred_loss = np.mean(np.log(np.array(val_latent_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "    val_std_latent_pred_loss = np.std(np.log(np.array(val_latent_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "else:\n",
    "    val_mean_latent_pred_loss = np.mean(val_latent_pred_loss_per_epoch_per_seed, axis=0)\n",
    "    val_std_latent_pred_loss = np.std(val_latent_pred_loss_per_epoch_per_seed, axis=0)\n",
    "\n",
    "val_mean_x_pred_loss = np.mean(np.log(np.array(val_x_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "val_std_x_pred_loss = np.std(np.log(np.array(val_x_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 12))\n",
    "plt.subplot(7, 1, 1)\n",
    "plt.plot(epochs, train_mean_total_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(train_total_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(train_total_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', \n",
    "        color='black')\n",
    "plt.fill_between(epochs, train_mean_total_pred_loss - train_std_total_pred_loss, \n",
    "                 train_mean_total_pred_loss + train_std_total_pred_loss, color='black', alpha=0.2)\n",
    "plt.title('Train Total Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(7, 1, 2)\n",
    "plt.plot(epochs, train_mean_y_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(train_y_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(train_y_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', \n",
    "         color='blue')\n",
    "plt.fill_between(epochs, train_mean_y_pred_loss - train_std_y_pred_loss,\n",
    "                train_mean_y_pred_loss + train_std_y_pred_loss, color='blue', alpha=0.2)\n",
    "plt.title('Train Y Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(7, 1, 3)\n",
    "plt.plot(epochs, train_mean_design_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(train_design_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(train_design_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', \n",
    "         color='green')\n",
    "plt.fill_between(epochs, train_mean_design_pred_loss - train_std_design_pred_loss,\n",
    "                 train_mean_design_pred_loss + train_std_design_pred_loss, color='green', alpha=0.2)\n",
    "plt.title('Train Design Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(7, 1, 4)\n",
    "plt.plot(epochs, train_mean_latent_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(train_latent_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(train_latent_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', \n",
    "         color='darkorange')\n",
    "plt.fill_between(epochs, train_mean_latent_pred_loss - train_std_latent_pred_loss,\n",
    "                  train_mean_latent_pred_loss + train_std_latent_pred_loss, color='darkorange', alpha=0.2)\n",
    "plt.title('Train Latent Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "# plt.subplot(7, 1, 4)\n",
    "# plt.plot(epochs, train_mean_x_pred_loss, \n",
    "#          label=f'{round(np.mean(np.array(train_x_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "#                f'{round(np.std(np.array(train_x_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', \n",
    "#          color='darkorange')\n",
    "# plt.fill_between(epochs, train_mean_x_pred_loss - train_std_x_pred_loss,\n",
    "#                   train_mean_x_pred_loss + train_std_x_pred_loss, color='darkorange', alpha=0.2)\n",
    "# plt.title('Train X Pred Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('log(loss)')\n",
    "# plt.legend()\n",
    "\n",
    "plt.subplot(7, 1, 5)\n",
    "plt.plot(epochs, val_mean_y_pred_loss,\n",
    "         label=f'{round(np.mean(np.array(val_y_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(val_y_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', color='lightskyblue')\n",
    "plt.fill_between(epochs, val_mean_y_pred_loss - val_std_y_pred_loss,\n",
    "                  val_mean_y_pred_loss + val_std_y_pred_loss, color='lightskyblue', alpha=0.2)\n",
    "plt.title('Val Y Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(7, 1, 6)\n",
    "plt.plot(epochs, val_mean_design_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(val_design_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(val_design_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', color='yellowgreen')\n",
    "plt.fill_between(epochs, val_mean_design_pred_loss - val_std_design_pred_loss, val_mean_design_pred_loss + val_std_design_pred_loss, color='yellowgreen', alpha=0.2)\n",
    "plt.title('Val Design Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(7, 1, 7)\n",
    "plt.plot(epochs, val_mean_latent_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(val_latent_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-'+\n",
    "               f'{round(np.std(np.array(val_latent_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', color='orange')\n",
    "plt.fill_between(epochs, val_mean_latent_pred_loss - val_std_latent_pred_loss, val_mean_latent_pred_loss + val_std_latent_pred_loss, color='orange', alpha=0.2)\n",
    "plt.title('Val Latent Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "# plt.subplot(7, 1, 7)\n",
    "# plt.plot(epochs, val_mean_x_pred_loss, \n",
    "#          label=f'{round(np.mean(np.array(val_x_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-'+\n",
    "#                f'{round(np.std(np.array(val_x_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', color='orange')\n",
    "# plt.fill_between(epochs, val_mean_x_pred_loss - val_std_x_pred_loss, val_mean_x_pred_loss + val_std_x_pred_loss, color='orange', alpha=0.2)\n",
    "# plt.title('Val X Pred Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('log(loss)')\n",
    "# plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NestedAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
