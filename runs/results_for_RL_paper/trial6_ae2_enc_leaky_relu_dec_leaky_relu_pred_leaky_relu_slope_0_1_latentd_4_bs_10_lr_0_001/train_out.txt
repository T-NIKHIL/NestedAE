 --> User provided command line run_dir argument : ../runs/results_for_RL_paper
 --> User provided command line ae argument : 2
 --> Setting global random seed 0.
 --> Running on cpu.
 --> Number of threads : 10
 --> Number of interop threads : 10
 --> PyTorch configurations


 --> Submodule encoder layers :
ModuleList(
  (0): Linear(in_features=5, out_features=4, bias=True)
  (1): LeakyReLU(negative_slope=0.1)
)


 --> Submodule predictor layers :
ModuleList(
  (0): Linear(in_features=4, out_features=1, bias=True)
  (1): LeakyReLU(negative_slope=0.1)
)


 --> Submodule decoder layers :
ModuleList(
  (0): Linear(in_features=4, out_features=5, bias=True)
  (1): LeakyReLU(negative_slope=0.1)
)
 --> Model Compilation step complete.
┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name       ┃ Type       ┃ Params ┃
┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ submodules │ ModuleDict │     54 │
└───┴────────────┴────────────┴────────┘
Trainable params: 54                                                            
Non-trainable params: 0                                                         
Total params: 54                                                                
Total estimated model params size (MB): 0                                       
 --> Example Input : 
{'f1tof4_w_ae1_latent': tensor([-1.1743, -2.9995,  0.8002,  0.2705, -1.7234]), 'f5': tensor([-0.7479])}


--> Model Trace : 


 ---------------------------------- 
module_name:encoder
input id:['f1tof4_w_ae1_latent']
input to submodule :
tensor([-1.1743, -2.9995,  0.8002,  0.2705, -1.7234])
output id:encoder
output from submodule :
tensor([ 0.6837, -0.1561,  0.2142, -0.2610], grad_fn=<LeakyReluBackward0>)
Submodule output dictionary :
{'encoder': tensor([ 0.6837, -0.1561,  0.2142, -0.2610], grad_fn=<LeakyReluBackward0>)}
 ---------------------------------- 




 ---------------------------------- 
module_name:predictor
input id:['encoder']
input to submodule :
tensor([ 0.6837, -0.1561,  0.2142, -0.2610], grad_fn=<CatBackward0>)
output id:predictor
output from submodule :
tensor([-0.0733], grad_fn=<LeakyReluBackward0>)
Submodule output dictionary :
{'encoder': tensor([ 0.6837, -0.1561,  0.2142, -0.2610], grad_fn=<LeakyReluBackward0>),
 'predictor': tensor([-0.0733], grad_fn=<LeakyReluBackward0>)}
 ---------------------------------- 




 ---------------------------------- 
module_name:decoder
input id:['encoder']
input to submodule :
tensor([ 0.6837, -0.1561,  0.2142, -0.2610], grad_fn=<CatBackward0>)
output id:decoder
output from submodule :
tensor([ 0.0176, -0.0491, -0.0009,  0.2532,  0.2333],
       grad_fn=<LeakyReluBackward0>)
Submodule output dictionary :
{'decoder': tensor([ 0.0176, -0.0491, -0.0009,  0.2532,  0.2333],
       grad_fn=<LeakyReluBackward0>),
 'encoder': tensor([ 0.6837, -0.1561,  0.2142, -0.2610], grad_fn=<LeakyReluBackward0>),
 'predictor': tensor([-0.0733], grad_fn=<LeakyReluBackward0>)}
 ---------------------------------- 


