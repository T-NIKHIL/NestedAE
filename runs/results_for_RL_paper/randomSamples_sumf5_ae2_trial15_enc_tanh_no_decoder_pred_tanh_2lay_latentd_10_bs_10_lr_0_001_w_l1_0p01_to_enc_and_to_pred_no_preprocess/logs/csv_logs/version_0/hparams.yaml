nn_datasets_dict:
  train:
    X2:
      header: null
      path: ../datasets/synthetic_dataset/synthetic_data_randomSamples_200_sumf5_with_ae1_latents_concat.csv
      skiprows: 1
      variables:
        f1tof4_w_ae1_latent:
          cols:
          - 8
          - 9
          - 10
          - 11
          - 13
          - 14
          - 15
          - 16
          - 17
          - 18
          - 19
          - 20
          preprocess: std
        f5:
          cols:
          - 12
          preprocess: null
nn_params_dict:
  model_type: randomSamples_sumf5_ae2_trial15_enc_tanh_no_decoder_pred_tanh_2lay_latentd_10_bs_10_lr_0_001_w_l1_0p01_to_enc_and_to_pred_no_preprocess
  submodules:
    encoder:
      connect_to:
      - f1tof4_w_ae1_latent
      layer_activation:
      - tanh
      layer_bias_init:
      - zeros
      layer_kernel_init:
      - xavier_normal
      layer_kernel_init_gain:
      - 1
      layer_type:
      - linear
      layer_weight_reg:
        l1: 0.01
        l2: 0
      num_nodes_per_layer:
      - 10
      save_output_on_fit_end: true
      save_params: true
    predictor:
      connect_to:
      - encoder
      layer_activation:
      - tanh
      layer_bias_init:
      - zeros
      layer_kernel_init:
      - xavier_normal
      layer_kernel_init_gain:
      - 1
      layer_type:
      - linear
      layer_weight_reg:
        l1: 0
        l2: 0
      num_nodes_per_layer:
      - 10
      save_output_on_fit_end: false
      save_params: false
    predictor_ext:
      connect_to:
      - predictor
      layer_activation:
      - tanh
      layer_bias_init:
      - zeros
      layer_kernel_init:
      - xavier_normal
      layer_kernel_init_gain:
      - 1
      layer_type:
      - linear
      layer_weight_reg:
        l1: 0.01
        l2: 0
      loss:
        target: f5
        type: mae
        wt: 1
      num_nodes_per_layer:
      - 1
      save_output_on_fit_end: true
      save_params: true
nn_save_dir: ../runs/results_for_RL_paper/randomSamples_sumf5_ae2_trial15_enc_tanh_no_decoder_pred_tanh_2lay_latentd_10_bs_10_lr_0_001_w_l1_0p01_to_enc_and_to_pred_no_preprocess
nn_train_params_dict:
  batch_size: 10
  callbacks:
    early_stopping:
      min_delta: 1.0e-05
      mode: min
      monitor: total_val_loss
      patience: 200
    model_checkpoint:
      mode: min
      monitor: total_val_loss
      save_top_k: 1
  epochs: 2000
  global_seed: 0
  optimizer:
    lr: 0.001
    type: adam
  shuffle_data_between_epochs: true
  test_split: 0.2
