{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67099ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "from sklearn.linear_model import Lasso\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d369d7d",
   "metadata": {},
   "source": [
    "#### Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstructChebyshev():\n",
    "    def __init__(self, n_poly, n_levels, n_samples):\n",
    "        self.n_poly = n_poly\n",
    "        self.polys = None\n",
    "        self.n_levels = n_levels\n",
    "        self.n_samples = n_samples\n",
    "        self.dataset  = None\n",
    "\n",
    "    def check_othogonality(self):\n",
    "        if self.polys is None:\n",
    "            raise ValueError(\"Create polys first by calling get_polys() method.\")\n",
    "        else:\n",
    "            # Ignore the first and last points as they cause inf when calculating weight\n",
    "            x = np.linspace(0, 1, self.n_levels)[1:-1]\n",
    "            dx = x[1] - x[0]\n",
    "            w = (1-x**2)*(-0.5)\n",
    "            for i in range(self.n_poly):\n",
    "                for j in range(i+1, self.n_poly):\n",
    "                    # Each factor is a function. Unlike vectors where we calculate dot products, for functions we compute inner products\n",
    "                    # <f,g> = \\int_{-1}Ë†{1} f(x)*g(x)dx\n",
    "                    # Chebyshev polynomials are orthogonal wrt the weight function w(x) = 1/sqrt(1-x^2). So compute weighted inner product.\n",
    "                    result = np.sum(self.polys[:, i] * self.polys[:, j] * w * dx, axis=0)\n",
    "                    print(f\"Inner product of polys {i} and {j}: {round(result, 2)}\")\n",
    "        \n",
    "    def get_polys(self, plot=False):\n",
    "        # Create linearly independent factors\n",
    "        x = np.linspace(0, 1, self.n_levels + 2)[1:-1]\n",
    "        # # Method 1 (Does not normalize the factors)\n",
    "        # for i in range(self.n_factors):\n",
    "        #     factor = x**i\n",
    "        #     if i == 0:\n",
    "        #         factors = factor.reshape(-1, 1)\n",
    "        #     else:\n",
    "        #         for j in range(i-1, -1, -1):\n",
    "        #             factor -= quad(lambda x: (x**i)*(x**j)*(1-x**2)**(-0.5), -1, 1)[0]/quad(lambda x: (x**j)*(x**j)*(1-x**2)**(-0.5), -1, 1)[0] * factors[:, j]\n",
    "        #         factors = np.hstack((factors, factor.reshape(-1, 1)))\n",
    "        # Method 2 (Recurrence relation)\n",
    "        for i in range(self.n_poly + 1):\n",
    "            if i == 0:\n",
    "                polys = np.ones(self.n_levels, dtype=np.float32).reshape(-1, 1)\n",
    "            elif i == 1:\n",
    "                polys = np.hstack((polys, x.reshape(-1, 1)), dtype=np.float32)\n",
    "            else:\n",
    "                poly = 2*x*polys[:, i-1] - polys[:, i-2]\n",
    "                polys = np.hstack((polys, poly.reshape(-1, 1)), dtype=np.float32)\n",
    "        # Drop first factor since it is just an array of ones\n",
    "        self.polys = polys[:, 1:]\n",
    "        if plot:\n",
    "            # 0, 1, 2, 3, 4, 5, 6, 7, 8\n",
    "            descriptor_set_1 = [7, 2, 3]\n",
    "            descriptor_set_2 = [6, 0, 4]\n",
    "            descriptor_set_3 = [8, 1, 5]\n",
    "            for desc in descriptor_set_1:\n",
    "                plt.plot(x, self.polys[:, desc], '.-', label=f'Poly {desc}')\n",
    "            plt.title('Descriptor Set 1')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('t(x)')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            for desc in descriptor_set_2:\n",
    "                plt.plot(x, self.polys[:, desc], '.-', label=f'Poly {desc}')\n",
    "            plt.title('Descriptor Set 2')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('t(x)')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            for desc in descriptor_set_3:\n",
    "                plt.plot(x, self.polys[:, desc], '.-', label=f'Poly {desc}')\n",
    "            plt.title('Descriptor Set 3')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('t(x)')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        return self.polys\n",
    "    \n",
    "    def meshgrid(self):\n",
    "        if self.polys is None:\n",
    "            raise ValueError(\"Run get_polys() method first\")\n",
    "        else:\n",
    "            for i in range(self.n_poly):\n",
    "                samples = np.array([[level]*(self.n_levels**i) \\\n",
    "                                        for level in self.polys[:, i]]*(self.n_levels**(self.n_poly - i - 1))).reshape(-1, 1)\n",
    "                if i == 0:\n",
    "                    dataset = samples\n",
    "                else:\n",
    "                    dataset = np.hstack((dataset, samples))\n",
    "        self.dataset = dataset\n",
    "        return self.dataset\n",
    "            \n",
    "    def gaussian_pmf(self, x , mu, sigma):\n",
    "        return (1/(sigma * math.sqrt(2 * math.pi))) * math.exp(-((x - mu)**2)/(2 * sigma**2)) \n",
    "    \n",
    "    def sample(self, dist='uniform'):\n",
    "        if self.polys is None:\n",
    "            raise ValueError(\"Run get_polys() method first\")\n",
    "        else:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if dist == 'uniform':\n",
    "                p = [1/self.n_levels]*self.n_levels\n",
    "            elif dist == 'normal':\n",
    "                 # Note : Here the gaussiana mixture probabilities kept same for each polynomials\n",
    "                 p = []\n",
    "                 means = [0.2, 0.6]\n",
    "                 sigmas = [2, 1.5]\n",
    "                 wts = [0.8, 0.2]\n",
    "                 for x in self.polys[:, 0]:\n",
    "                     total_prob = 0\n",
    "                     for mean, sigma, wt in zip(means, sigmas, wts):\n",
    "                        prob = self.gaussian_pmf(x, mean, sigma)\n",
    "                        total_prob += wt * prob\n",
    "                     p.append(total_prob)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid distribution type. Use 'uniform' or 'normal'.\")\n",
    "            for i in range(self.n_poly):\n",
    "                samples = np.random.choice(self.polys[:, i], size=self.n_samples, replace=True, p=p).reshape(-1, 1)\n",
    "                if i == 0:\n",
    "                    dataset = samples\n",
    "                else:\n",
    "                    dataset = np.hstack((dataset, samples))\n",
    "            # Remove duplicates\n",
    "            dataset = np.unique(dataset, axis=0)\n",
    "            self.dataset = dataset\n",
    "        return dataset\n",
    "\n",
    "# # num_samples = [int((0.00000001/100)*(8**20)), int((0.0000001/100)**(8**20)), int((0.000001/100)**(8**20))]\n",
    "# num_samples = [1000, 10000, 100000]\n",
    "# create_orthogonal_polynomials = ConstructChebyshev(9, 9, num_samples[0])\n",
    "# create_orthogonal_polynomials.get_polys(plot=True)\n",
    "# # create_orthogonal_polynomials.check_othogonality()\n",
    "# # # create_orthogonal_polynomials.meshgrid().shape\n",
    "# create_orthogonal_polynomials.sample().shape\n",
    "# dataset = create_orthogonal_polynomials.dataset\n",
    "\n",
    "num_desc_scale_1 = 8\n",
    "num_targets_scale_1 = 1\n",
    "num_desc_scale_2 = 5\n",
    "num_targets_scale_2 = 1\n",
    "num_desc_scale_3 = 2\n",
    "num_targets_scale_3 = 1\n",
    "num_samples = 1000 \n",
    "noise_scale = 1.0\n",
    "noise_label = '1'\n",
    "seed_for_noise_variables = 42\n",
    "seed_for_features = 100\n",
    "round_off = 3\n",
    "\n",
    "np.random.seed(seed_for_noise_variables)\n",
    "random.seed(seed_for_noise_variables)\n",
    "torch.manual_seed(seed_for_noise_variables)\n",
    "zetas = np.round(np.random.normal(loc=0, scale=noise_scale, size=(num_samples, num_desc_scale_1 + num_targets_scale_1 + num_desc_scale_2 + num_targets_scale_2 + num_desc_scale_3 + num_targets_scale_3)), round_off)\n",
    "print(f'Zetas shape : {zetas.shape}')\n",
    "print(f'Zetas head : \\n')\n",
    "print(zetas[0:10, :])\n",
    "zetas_scale_1 = zetas[:, :num_desc_scale_1 + num_targets_scale_1]\n",
    "zetas_scale_2 = zetas[:, num_desc_scale_1 + num_targets_scale_1:num_desc_scale_1 + num_targets_scale_1 + num_desc_scale_2 + num_targets_scale_2]\n",
    "zetas_scale_3 = zetas[:, num_desc_scale_1 + num_targets_scale_1 + num_desc_scale_2 + num_targets_scale_2:]\n",
    "print(zetas_scale_1.shape, zetas_scale_2.shape, zetas_scale_3.shape)\n",
    "print('\\n')\n",
    "\n",
    "np.random.seed(seed_for_features)\n",
    "random.seed(seed_for_features)\n",
    "torch.manual_seed(seed_for_features)\n",
    "dataset_scale_1 = np.round(np.random.uniform(low=-1, high=1, size=(num_samples, num_desc_scale_1)), round_off)\n",
    "print(f'dataset_scale_1 shape : {dataset_scale_1.shape}')\n",
    "print(f'dataset_scale_1 head : \\n')\n",
    "print(dataset_scale_1[0:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2204f",
   "metadata": {},
   "source": [
    "#### Add the dependent variables to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "plot_scale_2 = True\n",
    "plot_scale_3 = True\n",
    "\n",
    "# Create 3D surface plots\n",
    "def plot_surface(data, plot_title, axis_labels):\n",
    "    # Create a 3D surface plot using plotly\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=data[:, 0],\n",
    "        y=data[:, 1],\n",
    "        z=data[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            color=data[:, -1],  # Set color to the third column\n",
    "            colorscale='Viridis',  # Choose a colorscale\n",
    "            opacity=0.8,\n",
    "            line=dict(width=0)\n",
    "        )\n",
    "    )])\n",
    "    fig.update_layout(title=plot_title, autosize=False,\n",
    "                        scene=dict(\n",
    "                            xaxis_title=axis_labels[0],\n",
    "                            yaxis_title=axis_labels[1],\n",
    "                            zaxis_title=axis_labels[2],\n",
    "                            aspectmode='cube'\n",
    "                        ),\n",
    "                        margin=dict(l=0, r=0, b=0, t=0),\n",
    "                        width=400,\n",
    "                        height=400)\n",
    "    fig.show()\n",
    "    \n",
    "y0_1 = (dataset_scale_1[:, 0] + zetas_scale_1[:, 0])**2 + (dataset_scale_1[:, 7] + zetas_scale_1[:, 7])**2\n",
    "dataset_scale_1 = np.hstack((dataset_scale_1, y0_1.reshape(-1, 1)))\n",
    "print(f'dataset_scale_1 shape : {dataset_scale_1.shape}')\n",
    "if save:\n",
    "    np.save(f'dataset_scale_1_{num_samples}_{noise_label}.npy', dataset_scale_1)\n",
    "    np.save(f'zetas_scale_1_{num_samples}_{noise_label}.npy', zetas_scale_1)\n",
    "\n",
    "# Linear dependent variables with noise\n",
    "x0_2 = 2*(dataset_scale_1[:, 0] + zetas_scale_1[:, 0]) + 3*(dataset_scale_1[:, 1] + zetas_scale_1[:, 1])\n",
    "if plot_scale_2:\n",
    "    data = np.hstack((dataset_scale_1[:, 0].reshape(-1, 1), \n",
    "                        dataset_scale_1[:, 1].reshape(-1, 1), \n",
    "                        x0_2.reshape(-1, 1)))\n",
    "    plot_surface(data, 'x0_2', ['x0_1', 'x1_1', 'x0_2'])\n",
    "\n",
    "x1_2 = (dataset_scale_1[:, 0] + zetas_scale_1[:, 0]) + (dataset_scale_1[:, 2] + zetas_scale_1[:, 2])\n",
    "if plot_scale_2:\n",
    "    data = np.hstack((dataset_scale_1[:, 0].reshape(-1, 1), \n",
    "                        dataset_scale_1[:, 2].reshape(-1, 1), \n",
    "                        x1_2.reshape(-1, 1)))\n",
    "    plot_surface(data, 'x1_2', ['x0_1', 'x2_1', 'x1_2'])\n",
    "\n",
    "# Non Linear dependent variables with noise (upto degree 2)\n",
    "x2_2 = (dataset_scale_1[:, 0] + zetas_scale_1[:, 0])**2 \n",
    "if plot_scale_2:\n",
    "    plt.plot(dataset_scale_1[:, 0], x2_2, '.')\n",
    "    plt.xlabel('x0_1')\n",
    "    plt.ylabel('x2_2')\n",
    "    plt.show()\n",
    "\n",
    "x3_2 = (dataset_scale_1[:, 1] + zetas_scale_1[:, 1])**2\n",
    "if plot_scale_2:\n",
    "    plt.plot(dataset_scale_1[:, 1], x3_2, '.')\n",
    "    plt.xlabel('x1_1')\n",
    "    plt.ylabel('x3_2')\n",
    "    plt.show()\n",
    "\n",
    "x4_2 = (dataset_scale_1[:, 1] + zetas_scale_1[:, 1])*(dataset_scale_1[:, 2] + zetas_scale_1[:, 2])\n",
    "if plot_scale_2:\n",
    "    data = np.hstack((dataset_scale_1[:, 1].reshape(-1, 1), \n",
    "                        dataset_scale_1[:, 2].reshape(-1, 1), \n",
    "                        x4_2.reshape(-1, 1)))\n",
    "    plot_surface(data, 'x4_2', ['x1_1', 'x2_1', 'x4_2'])\n",
    "\n",
    "dataset_scale_2 = np.hstack((x0_2.reshape(-1, 1),\n",
    "                             x1_2.reshape(-1, 1),\n",
    "                             x2_2.reshape(-1, 1),\n",
    "                             x3_2.reshape(-1, 1),\n",
    "                             x4_2.reshape(-1, 1)))\n",
    "\n",
    "y0_2 = (dataset_scale_1[:, -1] + zetas_scale_1[:, -1])**2 + (dataset_scale_2[:, 4] + zetas_scale_2[:, 4])**4\n",
    "if plot_scale_2:\n",
    "    data = np.hstack((dataset_scale_1[:, -1].reshape(-1, 1),\n",
    "                      dataset_scale_2[:, 4].reshape(-1, 1),\n",
    "                      y0_2.reshape(-1, 1)))\n",
    "    plot_surface(data, 'y0_2', ['y0_1', 'x4_2', 'y0_2'])\n",
    "\n",
    "dataset_scale_2 = np.hstack((dataset_scale_2, y0_2.reshape(-1, 1)))\n",
    "print(f'dataset_scale_2 shape : {dataset_scale_2.shape}')\n",
    "\n",
    "if save:\n",
    "    np.save(f'dataset_scale_2_{num_samples}_{noise_label}.npy', dataset_scale_2)\n",
    "    np.save(f'zetas_scale_2_{num_samples}_{noise_label}.npy', zetas_scale_2)\n",
    "\n",
    "x0_3 = (dataset_scale_2[:, 0] + zetas_scale_2[:, 0]) - (dataset_scale_2[:, 4] + zetas_scale_2[:, 4])\n",
    "if plot_scale_3:\n",
    "    data = np.hstack((dataset_scale_2[:, 0].reshape(-1, 1), \n",
    "                      dataset_scale_2[:, 4].reshape(-1, 1), \n",
    "                      x0_3.reshape(-1, 1)))\n",
    "    plot_surface(data, 'x0_3', ['x0_2', 'x4_2', 'x0_3'])\n",
    "\n",
    "# Non Linear dependent variables with noise (upto degree 2)\n",
    "x1_3 = (dataset_scale_2[:, 0] + zetas_scale_2[:, 0])*(dataset_scale_2[:, 4] + zetas_scale_2[:, 4])\n",
    "if plot_scale_3:\n",
    "    data = np.hstack((dataset_scale_2[:, 0].reshape(-1, 1), \n",
    "                      dataset_scale_2[:, 4].reshape(-1, 1), \n",
    "                      x1_3.reshape(-1, 1)))\n",
    "    plot_surface(data, 'x1_3', ['x0_2', 'x4_2', 'x1_3'])\n",
    "\n",
    "dataset_scale_3 = np.hstack((x0_3.reshape(-1, 1),  \n",
    "                             x1_3.reshape(-1, 1)))\n",
    "\n",
    "y0_3 = (dataset_scale_2[:, -1] + zetas_scale_2[:, -1])**2 + (dataset_scale_3[:, 0] + zetas_scale_3[:, 0])**4\n",
    "if plot_scale_3:\n",
    "    data = np.hstack((dataset_scale_2[:, -1].reshape(-1, 1),\n",
    "                      dataset_scale_3[:, 0].reshape(-1, 1),\n",
    "                        y0_3.reshape(-1, 1)))\n",
    "    plot_surface(data, 'y0_3', ['y0_2', 'x0_3', 'y0_3'])\n",
    "\n",
    "dataset_scale_3 = np.hstack((dataset_scale_3, y0_3.reshape(-1, 1)))\n",
    "print(f'dataset_scale_3 shape : {dataset_scale_3.shape}')\n",
    "\n",
    "if save:\n",
    "    np.save(f'dataset_scale_3_{num_samples}_{noise_label}.npy', dataset_scale_3)\n",
    "    np.save(f'zetas_scale_3_{num_samples}_{noise_label}.npy', zetas_scale_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7f0d6",
   "metadata": {},
   "source": [
    "#### Analyze feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the samples\n",
    "for i in range(create_orthogonal_polynomials.n_poly + 2):\n",
    "    unique, counts = np.unique(linear_dataset[:, i], return_counts=True)\n",
    "    plt.bar(unique, counts, width=0.05)\n",
    "    if i != 8 or i != 9:\n",
    "        plt.xticks(unique, rotation=90)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency') \n",
    "    plt.title('Distribution of Samples for Polynomial ' + str(i))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fe788",
   "metadata": {},
   "source": [
    "#### Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99143d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Data params\n",
    "val_split = 0.2\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "# Training params\n",
    "num_epochs = 2000\n",
    "# Optimization loss params\n",
    "l1_coeff = 1e-3\n",
    "##########################\n",
    "\n",
    "model_coeffs = []\n",
    "model_intercepts = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for seed in seeds:\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # # RANDOM USED HERE - Shuffling to create train and val via cross-validation\n",
    "    numpy_dataset = np.load('linear_noise_0_1_size_100000.npy')\n",
    "    data_idxs = np.arange(0, len(numpy_dataset))\n",
    "    np.random.shuffle(data_idxs)\n",
    "    print(f'First 10 data idxs : {data_idxs[:10]}')\n",
    "    train_numpy_idxs = data_idxs[:int((1-val_split)*len(numpy_dataset))]\n",
    "    val_numpy_idxs = data_idxs[int((1-val_split)*len(numpy_dataset)):]\n",
    "    train_numpy_dataset = numpy_dataset[train_numpy_idxs, :]\n",
    "    val_numpy_dataset = numpy_dataset[val_numpy_idxs, :]\n",
    "\n",
    "    train_numpy_dataset_std = (train_numpy_dataset - np.mean(train_numpy_dataset, axis=0))/np.std(train_numpy_dataset, axis=0)\n",
    "    val_numpy_dataset_std = (val_numpy_dataset - np.mean(val_numpy_dataset, axis=0))/np.std(val_numpy_dataset, axis=0)\n",
    "\n",
    "    # Create Lasso model\n",
    "    lasso = Lasso(alpha=l1_coeff, fit_intercept=True, max_iter=num_epochs, random_state=seed)\n",
    "    # Fit the model\n",
    "    lasso.fit(train_numpy_dataset_std[:, :-3], train_numpy_dataset_std[:, -1])\n",
    "    # Train losses \n",
    "    train_preds = lasso.predict(train_numpy_dataset_std[:, :-3])\n",
    "    train_loss = np.mean((train_numpy_dataset_std[:, -1] - train_preds))\n",
    "    train_losses.append(train_loss)\n",
    "    # Get the coefficients\n",
    "    model_coeffs.append(lasso.coef_)\n",
    "    model_intercepts.append(lasso.intercept_)\n",
    "\n",
    "    # Test the model on validation set data\n",
    "    val_preds = lasso.predict(val_numpy_dataset_std[:, :-3])\n",
    "    val_loss = np.mean((val_numpy_dataset_std[:, -1] - val_preds))\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Train loss: {train_loss}, Val loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wts_arr = np.abs(np.round(np.array(model_coeffs), 2))\n",
    "\n",
    "print(model_wts_arr)\n",
    "\n",
    "feature_nums = np.arange(0, 12, 1) # feature\n",
    "feature_labels = ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10', 't11']\n",
    "seed_nums = np.arange(4, -1, -1) # seed\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "colors = ['r', 'g', 'b', 'y', 'orange']\n",
    "for c, seed_num in zip(colors, seed_nums):\n",
    "    cs = [c]*len(feature_nums)\n",
    "    ax.bar(feature_nums, model_wts_arr[seed_num, :], zs=seed_num, zdir='y', color=cs, alpha=0.8)\n",
    "    ax.set_xticks(feature_nums, feature_labels)\n",
    "    # Add weights to the top of the bars\n",
    "    for feature_num, height in zip(feature_nums, model_wts_arr[seed_num, :]):\n",
    "        ax.text(feature_num, seed_num, height, f'{height:.2f}', color='black', ha='center', va='bottom')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train loss mean : {np.mean(train_losses)} , std : {np.std(train_losses)}')\n",
    "print(f'Val loss mean : {np.mean(val_losses)} , std : {np.std(val_losses)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a786e36",
   "metadata": {},
   "source": [
    "#### Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f54fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NestedAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
