{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16e15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import kstest, ks_2samp\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList, ModuleDict, Linear, L1Loss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from NestedAE.nn_utils import check_dict_key_exists, set_layer_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e523149a",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85437c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAE(Module):\n",
    "    def __init__(self, module_params):\n",
    "        super(AE, self).__init__()\n",
    "        ae_modules = {}\n",
    "        # Outer loop iterates over the ae_modules\n",
    "        for module_name, module_dict in module_params['modules'].items():\n",
    "            layer_list = ModuleList()\n",
    "            # Check for existence of keys or take defualts if not present\n",
    "            if check_dict_key_exists('hidden_layers', module_dict):\n",
    "                hidden_layers = module_dict['hidden_layers']\n",
    "            else:\n",
    "                hidden_layers = 0\n",
    "            if check_dict_key_exists('hidden_dim', module_dict):\n",
    "                hidden_dim = module_dict['hidden_dim']\n",
    "            else:\n",
    "                hidden_dim = None\n",
    "            if check_dict_key_exists('hidden_activation', module_dict):\n",
    "                hidden_activation = module_dict['hidden_activation']\n",
    "            else:\n",
    "                hidden_activation = None\n",
    "            if check_dict_key_exists('output_activation', module_dict):\n",
    "                output_activation = module_dict['output_activation']\n",
    "            else:\n",
    "                output_activation = None\n",
    "            if check_dict_key_exists('layer_dropout', module_dict):\n",
    "                layer_dropout = module_dict['layer_dropout']\n",
    "            else:\n",
    "                layer_dropout = None\n",
    "            if check_dict_key_exists('layer_kernel_init', module_dict):\n",
    "                layer_kernel_init = module_dict['layer_kernel_init']\n",
    "            else:\n",
    "                layer_kernel_init = None\n",
    "            if check_dict_key_exists('layer_bias_init', module_dict):\n",
    "                layer_bias_init = module_dict['layer_bias_init']\n",
    "            else:\n",
    "                layer_bias_init = None\n",
    "            if check_dict_key_exists('load_params', module_dict):\n",
    "                load_params = module_dict['load_params']\n",
    "            else:\n",
    "                load_params = False\n",
    "\n",
    "            num_layers = hidden_layers + 1\n",
    "            for layer_num in range(num_layers):\n",
    "                if layer_num == 0:\n",
    "                    # Calculate the input dimensions to first layer\n",
    "                    input_dim = module_dict['input_dim']\n",
    "\n",
    "                    if hidden_dim is not None:\n",
    "                        layer_list.append(Linear(in_features=input_dim,\n",
    "                                                out_features=module_dict['hidden_dim'],\n",
    "                                                bias=True))\n",
    "                    else:\n",
    "                        layer_list.append(Linear(in_features=input_dim,\n",
    "                                                out_features=module_dict['output_dim'],\n",
    "                                                bias=True))\n",
    "                        if output_activation:\n",
    "                            layer_list.append(output_activation)\n",
    "                        break # Only output layer\n",
    "                elif layer_num == num_layers - 1:\n",
    "                    layer_list.append(Linear(in_features=module_dict['hidden_dim'],\n",
    "                                                out_features=module_dict['output_dim'],\n",
    "                                                bias=True))\n",
    "                    if output_activation:\n",
    "                        layer_list.append(output_activation)\n",
    "                    break # Dont add hidden activations\n",
    "                else:\n",
    "                    layer_list.append(Linear(in_features=module_dict['hidden_dim'],\n",
    "                                                out_features=module_dict['hidden_dim'],\n",
    "                                                bias=True))\n",
    "                # Add hidden activations if specified\n",
    "                if hidden_activation:\n",
    "                    layer_list.append(hidden_activation)\n",
    "                if layer_dropout:\n",
    "                    layer_list.append(layer_dropout)\n",
    "            # Initialize weights for all layers\n",
    "            if layer_kernel_init:\n",
    "                layer_list = set_layer_init(layer_list, module_dict, init='kernel')\n",
    "            if layer_bias_init:\n",
    "                layer_list = set_layer_init(layer_list, module_dict, init='bias')\n",
    "\n",
    "            # Finally add to ae_module list\n",
    "            ae_modules[module_name] = layer_list\n",
    "        self.ae_modules = ModuleDict(ae_modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        # Stores all module outputs\n",
    "        ae_module_outputs = {}\n",
    "\n",
    "        # Pass through encoder\n",
    "        for j, layer in enumerate(self.ae_modules['encoder']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['z'] = layer(x)\n",
    "            else:\n",
    "                ae_module_outputs['z'] = layer(ae_module_outputs['z'])\n",
    "\n",
    "        # Pass through predictor\n",
    "        for j, layer in enumerate(self.ae_modules['STHE_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['y1_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['y1_pred'] = layer(ae_module_outputs['y1_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['A_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['design1_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['design1_pred'] = layer(ae_module_outputs['design1_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['B_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['design2_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['design2_pred'] = layer(ae_module_outputs['design2_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['X_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['design3_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['design3_pred'] = layer(ae_module_outputs['design3_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['phase_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['design4_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['design4_pred'] = layer(ae_module_outputs['design4_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['decoder']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['x_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['x_pred'] = layer(ae_module_outputs['x_pred'])\n",
    "\n",
    "        return ae_module_outputs\n",
    "    \n",
    "class NestedAE(Module):\n",
    "    def __init__(self, module_params):\n",
    "        super(AE, self).__init__()\n",
    "        ae_modules = {}\n",
    "        # Outer loop iterates over the ae_modules\n",
    "        for module_name, module_dict in module_params['modules'].items():\n",
    "            layer_list = ModuleList()\n",
    "            # Check for existence of keys or take defualts if not present\n",
    "            if check_dict_key_exists('hidden_layers', module_dict):\n",
    "                hidden_layers = module_dict['hidden_layers']\n",
    "            else:\n",
    "                hidden_layers = 0\n",
    "            if check_dict_key_exists('hidden_dim', module_dict):\n",
    "                hidden_dim = module_dict['hidden_dim']\n",
    "            else:\n",
    "                hidden_dim = None\n",
    "            if check_dict_key_exists('hidden_activation', module_dict):\n",
    "                hidden_activation = module_dict['hidden_activation']\n",
    "            else:\n",
    "                hidden_activation = None\n",
    "            if check_dict_key_exists('output_activation', module_dict):\n",
    "                output_activation = module_dict['output_activation']\n",
    "            else:\n",
    "                output_activation = None\n",
    "            if check_dict_key_exists('layer_dropout', module_dict):\n",
    "                layer_dropout = module_dict['layer_dropout']\n",
    "            else:\n",
    "                layer_dropout = None\n",
    "            if check_dict_key_exists('layer_kernel_init', module_dict):\n",
    "                layer_kernel_init = module_dict['layer_kernel_init']\n",
    "            else:\n",
    "                layer_kernel_init = None\n",
    "            if check_dict_key_exists('layer_bias_init', module_dict):\n",
    "                layer_bias_init = module_dict['layer_bias_init']\n",
    "            else:\n",
    "                layer_bias_init = None\n",
    "            if check_dict_key_exists('load_params', module_dict):\n",
    "                load_params = module_dict['load_params']\n",
    "            else:\n",
    "                load_params = False\n",
    "\n",
    "            num_layers = hidden_layers + 1\n",
    "            for layer_num in range(num_layers):\n",
    "                if layer_num == 0:\n",
    "                    # Calculate the input dimensions to first layer\n",
    "                    input_dim = module_dict['input_dim']\n",
    "\n",
    "                    if hidden_dim is not None:\n",
    "                        layer_list.append(Linear(in_features=input_dim,\n",
    "                                                out_features=module_dict['hidden_dim'],\n",
    "                                                bias=True))\n",
    "                    else:\n",
    "                        layer_list.append(Linear(in_features=input_dim,\n",
    "                                                out_features=module_dict['output_dim'],\n",
    "                                                bias=True))\n",
    "                        if output_activation:\n",
    "                            layer_list.append(output_activation)\n",
    "                        break # Only output layer\n",
    "                elif layer_num == num_layers - 1:\n",
    "                    layer_list.append(Linear(in_features=module_dict['hidden_dim'],\n",
    "                                                out_features=module_dict['output_dim'],\n",
    "                                                bias=True))\n",
    "                    if output_activation:\n",
    "                        layer_list.append(output_activation)\n",
    "                    break # Dont add hidden activations\n",
    "                else:\n",
    "                    layer_list.append(Linear(in_features=module_dict['hidden_dim'],\n",
    "                                                out_features=module_dict['hidden_dim'],\n",
    "                                                bias=True))\n",
    "                # Add hidden activations if specified\n",
    "                if hidden_activation:\n",
    "                    layer_list.append(hidden_activation)\n",
    "                if layer_dropout:\n",
    "                    layer_list.append(layer_dropout)\n",
    "            # Initialize weights for all layers\n",
    "            if layer_kernel_init:\n",
    "                layer_list = set_layer_init(layer_list, module_dict, init='kernel')\n",
    "            if layer_bias_init:\n",
    "                layer_list = set_layer_init(layer_list, module_dict, init='bias')\n",
    "\n",
    "            # Finally add to ae_module list\n",
    "            ae_modules[module_name] = layer_list\n",
    "        self.ae_modules = ModuleDict(ae_modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        # Stores all module outputs\n",
    "        ae_module_outputs = {}\n",
    "\n",
    "        # Pass through encoder\n",
    "        for j, layer in enumerate(self.ae_modules['encoder']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['z'] = layer(x)\n",
    "            else:\n",
    "                ae_module_outputs['z'] = layer(ae_module_outputs['z'])\n",
    "\n",
    "        # Second AE modules\n",
    "        # # Pass through predictor\n",
    "        # for j, layer in enumerate(self.ae_modules['STHE_predictor']):\n",
    "        #     if j == 0:\n",
    "        #         ae_module_outputs['y1_pred'] = layer(ae_module_outputs['z'])\n",
    "        #     else:\n",
    "        #         ae_module_outputs['y1_pred'] = layer(ae_module_outputs['y1_pred'])\n",
    "\n",
    "        # for j, layer in enumerate(self.ae_modules['latents_predictor']):\n",
    "        #     if j == 0:\n",
    "        #         ae_module_outputs['latents_pred'] = layer(ae_module_outputs['z'])\n",
    "        #     else:\n",
    "        #         ae_module_outputs['latents_pred'] = layer(ae_module_outputs['latents_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['A_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['design1_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['design1_pred'] = layer(ae_module_outputs['design1_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['B_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['design2_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['design2_pred'] = layer(ae_module_outputs['design2_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['X_predictor']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['design3_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['design3_pred'] = layer(ae_module_outputs['design3_pred'])\n",
    "\n",
    "        # Second AE modules\n",
    "        # for j, layer in enumerate(self.ae_modules['phase_predictor']):\n",
    "        #     if j == 0:\n",
    "        #         ae_module_outputs['design4_pred'] = layer(ae_module_outputs['z'])\n",
    "        #     else:\n",
    "        #         ae_module_outputs['design4_pred'] = layer(ae_module_outputs['design4_pred'])\n",
    "\n",
    "        for j, layer in enumerate(self.ae_modules['decoder']):\n",
    "            if j == 0:\n",
    "                ae_module_outputs['x_pred'] = layer(ae_module_outputs['z'])\n",
    "            else:\n",
    "                ae_module_outputs['x_pred'] = layer(ae_module_outputs['x_pred'])\n",
    "\n",
    "        return ae_module_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f196eb",
   "metadata": {},
   "source": [
    "#### Datasets Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loc = 'datasets/MHP_for_water_splitting/dataset.csv'\n",
    "\n",
    "#########################################\n",
    "# FOR ABX3 BANDGAP DATASET \n",
    "#########################################\n",
    "\n",
    "# latent_col_names = []\n",
    "latent_col_names = ['l0', 'l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9', 'l10']\n",
    "\n",
    "# Descriptors when just using the binding energy dataset\n",
    "# descriptors = ['A_IONRAD',\n",
    "#                 'A_MASS',\n",
    "#                 'A_DPM',\n",
    "#                 # 'B_IONRAD',\n",
    "#                 # 'B_MASS',\n",
    "#                 # 'B_EA',\n",
    "#                 # 'B_IE',\n",
    "#                 # 'B_En',\n",
    "#                 # 'B_AN',\n",
    "#                 'X_IONRAD',\n",
    "#                 'X_MASS',\n",
    "#                 'X_EA',\n",
    "#                 'X_IE',\n",
    "#                 'X_En',\n",
    "#                 'X_AN',\n",
    "#                 'SOLV_DENSITY',\n",
    "#                 'SOLV_DIELECTRIC',\n",
    "#                 'SOLV_GDN',\n",
    "#                 'SOLV_DPM',\n",
    "#                 'SOLV_MV',\n",
    "#                 'SOLV_UMBO'\n",
    "#             ]\n",
    "\n",
    "# descriptors = ['A_IONRAD',\n",
    "#                'A_MASS',\n",
    "#                'A_DPM',\n",
    "#                'B_IONRAD',\n",
    "#                'B_MASS',\n",
    "#                'B_EA',\n",
    "#                'B_IE',\n",
    "#                'B_En',\n",
    "#                'B_AN',\n",
    "#                'X_IONRAD',\n",
    "#                'X_MASS',\n",
    "#                'X_EA',\n",
    "#                'X_IE',\n",
    "#                'X_En',\n",
    "#                'X_AN'\n",
    "# ]\n",
    "descriptors = ['SOLV_DENSITY',\n",
    "               'SOLV_DIELECTRIC',\n",
    "               'SOLV_GDN',\n",
    "               'SOLV_DPM',\n",
    "               'SOLV_MV',\n",
    "               'SOLV_UMBO']\n",
    "\n",
    "# target = ['Gap']\n",
    "target = ['Target']\n",
    "\n",
    "# target_A_ion = ['K', 'Rb', 'Cs', 'MA', 'FA']\n",
    "# target_B_ion = ['Ca', 'Sr', 'Ba', 'Ge', 'Sn', 'Pb']\n",
    "# target_X_ion = ['Cl', 'Br', 'I']\n",
    "target_solvent = ['DMSO', 'THTO', 'DMF', 'NMP', 'ACETONE', 'METHA', 'GBL', 'NITRO']\n",
    "\n",
    "standardize_descs = True\n",
    "\n",
    "split_strategy = 'strat_kfold'\n",
    "train_split = 0.9\n",
    "# For dataset 1\n",
    "# defined_qs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# For dataset 2\n",
    "defined_qs = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "plot_train_test_dist = False\n",
    "\n",
    "plot_pcc_matrix = False\n",
    "\n",
    "model_save_dir = 'ae2_bandgaps_THEN_perov_solv_BE'\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "X_dataframe = pd.read_csv(dataset_loc)[descriptors + latent_col_names]\n",
    "# Y_dataframe = pd.read_csv(dataset_loc)[target + target_A_ion + target_B_ion + target_X_ion]\n",
    "Y_dataframe = pd.read_csv(dataset_loc)[target + target_solvent]\n",
    "\n",
    "if standardize_descs:\n",
    "    desc_means = []\n",
    "    desc_std_devs = []\n",
    "    for desc in X_dataframe.columns.tolist():\n",
    "        mean = X_dataframe[desc].mean()\n",
    "        desc_means.append(mean)\n",
    "        std_dev = X_dataframe[desc].std()\n",
    "        desc_std_devs.append(std_dev)\n",
    "        X_dataframe[desc] = (X_dataframe[desc] - mean) / std_dev\n",
    "    print('Descriptors standardized.')\n",
    "else:\n",
    "    print('Descriptors not standardized.')\n",
    "\n",
    "print(f'Dataframe Statistics : {X_dataframe.describe()}')\n",
    "\n",
    "print(f'Dataset columns : \\n')\n",
    "print(X_dataframe.columns)\n",
    "# dataset = np.concatenate((X_dataframe.to_numpy(dtype=np.float32), \n",
    "#                           Y_dataframe[target].to_numpy(dtype=np.float32),\n",
    "#                           Y_dataframe[target_A_ion].to_numpy(dtype=np.float32),\n",
    "#                           Y_dataframe[target_B_ion].to_numpy(dtype=np.float32),\n",
    "#                           Y_dataframe[target_X_ion].to_numpy(dtype=np.float32)),\n",
    "#                           axis=1)\n",
    "dataset = np.concatenate((X_dataframe.to_numpy(dtype=np.float32), \n",
    "                          Y_dataframe[target].to_numpy(dtype=np.float32),\n",
    "                          Y_dataframe[target_solvent].to_numpy(dtype=np.float32)), axis=1)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069284fb",
   "metadata": {},
   "source": [
    "### Compare with other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25331a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_dataframe.to_numpy(dtype=np.float32)\n",
    "y = Y_dataframe[target].to_numpy(dtype=np.float32)\n",
    "\n",
    "print(f'The quantiles : {np.quantile(y, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])}')\n",
    "y_binned = np.digitize(y, np.quantile(y, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "# kf = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "split_idxs_all_folds = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(x, y_binned)):\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(x)):\n",
    "    split_idxs_all_folds.append((train_idx, val_idx))\n",
    "    ks_stat, p_value = ks_2samp(y[train_idx], y[val_idx])\n",
    "    print(f'Fold {fold} : KS statistic = {ks_stat}, p-value = {p_value}')\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param_grid_xgb = {\n",
    "        'n_estimators': [250, 500, 1000, 2000],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [2, 3, 4, 5],\n",
    "        'subsample': [0.4, 0.6, 0.8, 1.0],\n",
    "        # 'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        # 'alpha': [0, 0.01, 0.1],\n",
    "        # 'lambda':[0, 0.01, 0.1]\n",
    "    }\n",
    "param_grid_RF = {\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_depth': [2, 3, 4, 5, 10],\n",
    "    'ccp_alpha': [0.0, 0.01, 0.1]\n",
    "}\n",
    "param_grid_nusvr = {\n",
    "    'nu': [0.25, 0.5, 0.75],\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "    'degree': [2, 3, 4],  # Only relevant for 'poly' kernel\n",
    "    'coef0': [0.0, 0.1, 0.5],  # Only relevant for 'poly' and 'sigmoid' kernels\n",
    "    'C': [0.5, 1, 5, 10, 50, 100]\n",
    "}\n",
    "param_grid_lasso = {\n",
    "        'alpha':[0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "}\n",
    "param_grid_ridge = {\n",
    "          'alpha':[0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=XGBRegressor(random_state=random_state),\n",
    "                    param_grid=param_grid_xgb,\n",
    "                    scoring='neg_mean_absolute_error', \n",
    "                    cv=split_idxs_all_folds, # If using None defaults to 5 fold cross-validation\n",
    "                    n_jobs=-1, \n",
    "                    verbose=1,\n",
    "                    return_train_score=True)\n",
    "grid.fit(x, y.squeeze())\n",
    "print(f'Best parameters found: {grid.best_params_}') # Parameter setting that gave best performance on hold out data\n",
    "print(f'Best Cross validated score of the best estimator found: {-grid.best_score_}') # Mean cross-validated score of the best_estimator\n",
    "# Get the mean absolute error on the training set across all folds for best model\n",
    "print(f\"Mean test score of the best estimator across all folds : {grid.cv_results_['mean_test_score'][grid.best_index_]}\")\n",
    "print(f\"Std of test score of the best estimator across all folds : {grid.cv_results_['std_test_score'][grid.best_index_]}\")\n",
    "print(f\"Mean train score of the best estimator across all folds : {grid.cv_results_['mean_train_score'][grid.best_index_]}\")\n",
    "print(f\"Std of train score of the best estimator across all folds : {grid.cv_results_['std_train_score'][grid.best_index_]}\")\n",
    "\n",
    "# Find the indices of top 10 test scores\n",
    "top_10_indices = sorted(range(len(grid.cv_results_['mean_test_score'])), key=lambda i: grid.cv_results_['mean_test_score'][i], reverse=True)[:10]\n",
    "# Get the top 10 parameter sets\n",
    "print(\"Top 10 parameter sets:\")\n",
    "for idx in top_10_indices:\n",
    "    print(f\"Params: {grid.cv_results_['params'][idx]}, Mean Test Score: {grid.cv_results_['mean_test_score'][idx]}, Std Test Score: {grid.cv_results_['std_test_score'][idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69643cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Ekeany/Boruta-Shap -> Change to boruta_shap conda env\n",
    "\n",
    "# use boruta_shap env and import these libraries in the top cell \n",
    "# from BorutaShap import BorutaShap\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "feature_selector = BorutaShap(importance_measure='shap', classification=False)\n",
    "feature_selector.fit(X=pd.DataFrame(dataset[:, :(len(latent_col_names + descriptors))], columns=latent_col_names + descriptors),\n",
    "                     y=dataset[:, 15].reshape(-1, 1), \n",
    "                     n_trials=100,\n",
    "                     sample=False,\n",
    "                     train_or_test='train',\n",
    "                     random_state=random_state,\n",
    "                     normalize=True,\n",
    "                     verbose=True) # importance values normaized accdg to Z score\n",
    "feature_selector.plot(which_features='all', y_scale=None) # Convert to log by y_scale='log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = feature_selector.history_x.iloc[1:]\n",
    "data['index'] = data.index\n",
    "data = pd.melt(data, id_vars='index', var_name='Methods')\n",
    "\n",
    "decision_mapper = feature_selector.create_mapping_of_features_to_attribute(maps=['Tentative','Rejected','Accepted', 'Shadow'])\n",
    "data['Decision'] = data['Methods'].map(decision_mapper)\n",
    "data.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "options = { 'accepted' : feature_selector.filter_data(data,'Decision', 'Accepted'),\n",
    "            'tentative': feature_selector.filter_data(data,'Decision', 'Tentative'),\n",
    "            'rejected' : feature_selector.filter_data(data,'Decision', 'Rejected'),\n",
    "            'all' : data}\n",
    "print(data)\n",
    "\n",
    "# Save data to csv file\n",
    "data.to_csv('data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5618877",
   "metadata": {},
   "source": [
    "#### Data Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade12cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs = []\n",
    "val_idxs = []\n",
    "if split_strategy == 'kfold':\n",
    "    kf = KFold(n_splits=int(1/(1 - train_split)), shuffle=True, random_state=random_state)\n",
    "    for (train_idx, val_idx) in kf.split(dataset):\n",
    "        train_idxs.append(train_idx)\n",
    "        val_idxs.append(val_idx)\n",
    "elif split_strategy == 'strat_kfold':\n",
    "    print('Using a stratified k fold split strategy.')\n",
    "    y = Y_dataframe[target].to_numpy(dtype=np.float32)\n",
    "    skf = StratifiedKFold(n_splits=int(1/(1 - train_split)), shuffle=True, random_state=random_state)\n",
    "    y_binned = np.digitize(y, np.quantile(y, defined_qs))\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(dataset, y_binned)):\n",
    "        train_idxs.append(train_idx)\n",
    "        val_idxs.append(val_idx)\n",
    "        ks_stat, p_val = ks_2samp(y[train_idx], y[val_idx])\n",
    "        print(f'Fold {fold} ks-stat for target: {np.round(ks_stat, 3)}, p-value: {np.round(p_val, 3)}')\n",
    "elif split_strategy == 'random':    \n",
    "    # Train - Test split of indices\n",
    "    idxs = list(range(X_dataframe.shape[0]))\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.shuffle(idxs)\n",
    "        train_idxs.append(idxs[:int(train_split*len(idxs))])\n",
    "        val_idxs.append(idxs[int(train_split*len(idxs)):])\n",
    "else:\n",
    "    raise ValueError(f'Unknown split strategy: {split_strategy}')\n",
    "\n",
    "# for_fold = 5\n",
    "# # Check how do the histograms of train and test distribution match up\n",
    "# if plot_train_test_dist:\n",
    "#     X_dataframe_train = X_dataframe.iloc[train_idxs[for_fold]]\n",
    "#     Y_dataframe_train = Y_dataframe.iloc[train_idxs[for_fold]]\n",
    "#     X_dataframe_test = X_dataframe.iloc[val_idxs[for_fold]]\n",
    "#     Y_dataframe_test = Y_dataframe.iloc[val_idxs[for_fold]]\n",
    "#     # for col in X_dataframe.columns:\n",
    "#     #     # ks_1samp takes distribution of sample we want to compare (F(x)) against a continuous dist. (G(x))\n",
    "#     #     ks_stat, p_value = ks_2samp(X_dataframe_train[col], X_dataframe_test[col])\n",
    "#     #     plt.figure(figsize=(5, 3))\n",
    "#     #     sns.histplot(X_dataframe_train[col], kde=True, label='Train', color='blue', stat='probability', binwidth=0.1)\n",
    "#     #     sns.histplot(X_dataframe_test[col], kde=True, label='Test', color='red', stat='probability', binwidth=0.1)\n",
    "#     #     plt.title(f'Distribution of {col}')\n",
    "#     #     plt.xlabel(col)\n",
    "#     #     plt.ylabel('Density')\n",
    "#     #     plt.figtext(0.15, 0.8, f'KS-stat: {ks_stat:.3f}\\nP-value: {p_value:.3f}', bbox=dict(facecolor='white', alpha=0.5))\n",
    "#     #     plt.legend()\n",
    "#     #     plt.show()\n",
    "\n",
    "#     # Combine all histograms into a single plot\n",
    "#     fig, ax = plt.subplots(3, 5, figsize=(20, 10))\n",
    "#     ax = ax.flatten()\n",
    "#     for col in X_dataframe.columns:\n",
    "#         ks_stat, p_value = ks_2samp(X_dataframe_train[col], X_dataframe_test[col])\n",
    "#         # plt.figure(figsize=(5, 3))\n",
    "#         ax = fig.add_subplot(3, 5, X_dataframe.columns.get_loc(col)+1)\n",
    "#         ax.set_title(f'Distribution of {col}')\n",
    "#         ax.set_xlabel(col)\n",
    "#         ax.set_ylabel('Density')\n",
    "#         # ax.set_figtext(0.15, 0.8, f'KS-stat: {ks_stat:.3f}\\nP-value: {p_value:.3f}', bbox=dict(facecolor='white', alpha=0.5))\n",
    "#         sns.histplot(X_dataframe_train[col], kde=False, label='Train', color='blue', stat='probability', binwidth=0.1)\n",
    "#         sns.histplot(X_dataframe_test[col], kde=False, label='Test', color='red', stat='probability', binwidth=0.1)\n",
    "#         ax.text(0.5, 0.8, f'KS-stat: {ks_stat:.3f}\\nP-value: {p_value:.3f}', transform=ax.transAxes,\n",
    "#                 bbox=dict(facecolor='white', alpha=0.5), horizontalalignment='center')\n",
    "#         # Remove axis label\n",
    "#         ax.set_xlabel('')\n",
    "#         ax.set_ylabel('')\n",
    "#         ax.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d4d59",
   "metadata": {},
   "source": [
    "### Pearson Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the pearson correlation coefficients (ALWAYS USE THE TRAINING DATASET TO AVOID DATA LEAKAGE)\n",
    "# choose_descriptors = latent_col_names + descriptors + target + target_A_ion + target_B_ion + target_X_ion\n",
    "choose_descriptors = latent_col_names + descriptors + target_solvent\n",
    "choose_train_idx = 0\n",
    "if plot_pcc_matrix:\n",
    "    pcc = np.round(np.corrcoef(x=dataset[train_idxs[choose_train_idx]][:,:len(choose_descriptors)], rowvar=False), 2)\n",
    "    # Ref : Gryffin, https://online.ucpress.edu/collabra/article/9/1/87615/197169/A-Brief-Note-on-the-Standard-Error-of-the-Pearson\n",
    "    std_err_pcc = 1/((dataset[train_idxs[0]].shape[0] - 3)**0.5)\n",
    "    print(f'Standard error in PCC : {std_err_pcc}')\n",
    "    adj_pcc = (np.abs(pcc) - std_err_pcc)/(1 - std_err_pcc)\n",
    "    adj_pcc[adj_pcc < 0] = 0  # Ensure no negative values in adjusted PCC\n",
    "\n",
    "    # Add correlatin coeff values to the plot\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    pcc_plot = ax.matshow(adj_pcc, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    for i in range(adj_pcc.shape[0]):\n",
    "        for j in range(adj_pcc.shape[1]):\n",
    "            plt.text(j, i, f'{adj_pcc[i, j]:.2f}', ha='center', va='center', color='black')\n",
    "    ax.set_yticks(ticks=np.arange(len(choose_descriptors)), labels=choose_descriptors, rotation=0)\n",
    "    ax.set_xticks(ticks=np.arange(len(choose_descriptors)), labels=choose_descriptors, rotation=90)\n",
    "    ax.set_yticks(ticks=np.arange(len(choose_descriptors)))\n",
    "    ax.set_xticks(ticks=np.arange(len(choose_descriptors)))\n",
    "    # Add the colorbar\n",
    "    cbar = plt.colorbar(pcc_plot, ax=ax, fraction=0.0455)\n",
    "    # cbar.set_label('Adjusted Pearson Correlation Coefficient', rotation=270, labelpad=20)\n",
    "    # plt.title('Adjusted Pearson Correlation Coefficient Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ff19d",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Training params\n",
    "num_epochs = 1000\n",
    "lr = 0.01\n",
    "# Optimization loss params\n",
    "l2_coeff = 0\n",
    "l1_coeff = 0.001\n",
    "prop_l1_coeff = 0.001\n",
    "# Model params\n",
    "num_y1_latents = 1\n",
    "# Printing params\n",
    "print_every_n_batches = 100\n",
    "print_losses = False\n",
    "debug = False\n",
    "pred_lam = 1\n",
    "design_lam = 1\n",
    "latent_lam = 10\n",
    "##########################\n",
    "\n",
    "class Arctanh(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.atanh(torch.clamp(x, -0.999999, 0.999999))\n",
    "    \n",
    "atanh_act_fn = Arctanh()\n",
    "\n",
    "# For AE1 trained on bandgaps dataset\n",
    "# latent_dim = 11\n",
    "# For HD1 trained on bandgaps dataset\n",
    "# latent_dim = 10\n",
    "\n",
    "# For AE2 trained on mhp-solvent-be dataset\n",
    "latent_dim = 7\n",
    "# For HD2 trained on mhp-solvent-be dataset\n",
    "# latent_dim = 6\n",
    "\n",
    "module_params = {'name':'AE1', \n",
    "                    'modules':{\n",
    "\n",
    "                        'encoder':{\n",
    "                            'input_dim':17,\n",
    "                            'output_dim':latent_dim, \n",
    "                            'hidden_dim':25, \n",
    "                            'hidden_layers':1, \n",
    "                            'hidden_activation':None, \n",
    "                            'output_activation':torch.nn.Tanh(), \n",
    "                            'layer_kernel_init':'xavier_normal', \n",
    "                            'layer_bias_init':'zeros', \n",
    "                            },\n",
    "\n",
    "                        # 'bandgaps_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':1,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':torch.nn.ReLU(),\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'},\n",
    "\n",
    "                        'BE_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':1,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':torch.nn.ReLU(),\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        },\n",
    "\n",
    "                        'latents_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':11,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':None,\n",
    "                            'output_activation':torch.nn.Tanh(),\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'},\n",
    "\n",
    "                        # 'A_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':5,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        # 'B_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':6,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        # 'X_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':3,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        'decoder':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':6,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':None,\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        },\n",
    "\n",
    "                        'solvent_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':8,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':None,\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        },\n",
    "\n",
    "                    }}\n",
    "\n",
    "train_total_pred_loss_per_epoch_per_seed = []\n",
    "train_y_pred_loss_per_epoch_per_seed = []\n",
    "train_design_pred_loss_per_epoch_per_seed = []\n",
    "train_latent_pred_loss_per_epoch_per_seed = []\n",
    "train_x_pred_loss_per_epoch_per_seed = []\n",
    "\n",
    "val_y_pred_loss_per_epoch_per_seed = []\n",
    "val_design_pred_loss_per_epoch_per_seed = []\n",
    "val_latent_pred_loss_per_epoch_per_seed = []\n",
    "val_x_pred_loss_per_epoch_per_seed = []\n",
    "\n",
    "ind_losses_dict_train = {}\n",
    "ind_losses_dict_val = {}\n",
    "\n",
    "# for i, seed in enumerate(seeds):\n",
    "for i in range(len(train_idxs)):\n",
    "    print('\\n')\n",
    "    print(f'Fold {i}')\n",
    "    print('\\n' )\n",
    "    train_dataset = dataset[train_idxs[i]]\n",
    "    val_dataset = dataset[val_idxs[i]]\n",
    "\n",
    "    print(f'Train numpy dataset shape : {train_dataset.shape}, Val. numpy dataset shape : {val_dataset.shape}')\n",
    "\n",
    "    torch_train_dataset = torch.from_numpy(train_dataset).to(dtype=torch.float32)\n",
    "    train_data_loader = DataLoader(torch_train_dataset, batch_size=train_dataset.shape[0], shuffle=True)\n",
    "\n",
    "    torch_val_dataset = torch.from_numpy(val_dataset).to(dtype=torch.float32)\n",
    "    val_data_loader = DataLoader(torch_val_dataset, batch_size=val_dataset.shape[0], shuffle=False)\n",
    "\n",
    "    # RANDOM USED HERE - Param init\n",
    "    # Delete previos model\n",
    "    if i > 0:\n",
    "        print(\"Deleting previous model\")\n",
    "        del ae\n",
    "        ae = None\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    ae = AE(module_params)\n",
    "    print(ae)\n",
    "\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    adam = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=l2_coeff)\n",
    "    x_l1_loss = L1Loss(reduction='mean')\n",
    "\n",
    "    y1_l1_loss = L1Loss(reduction='mean')\n",
    "\n",
    "    design1_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    design2_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    design3_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    latent_l1_loss = L1Loss(reduction='mean')\n",
    "\n",
    "    train_total_pred_loss_per_epoch = []\n",
    "\n",
    "    train_y_pred_loss_per_epoch = []\n",
    "    train_design_pred_loss_per_epoch = []\n",
    "    train_latent_pred_loss_per_epoch = []\n",
    "    train_x_pred_loss_per_epoch = []\n",
    "\n",
    "    val_y_pred_loss_per_epoch = []\n",
    "    val_design_pred_loss_per_epoch = []\n",
    "    val_latent_pred_loss_per_epoch = []\n",
    "    val_x_pred_loss_per_epoch = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_total_pred_loss_per_batch = 0\n",
    "        train_y_pred_loss_per_batch = 0\n",
    "        train_design_pred_loss_per_batch = 0\n",
    "        train_latent_pred_loss_per_batch = 0\n",
    "        train_x_pred_loss_per_batch = 0\n",
    "\n",
    "        # Train Loop\n",
    "        print(f' --------- Epoch Stats {epoch+1}/{num_epochs} --------- ')\n",
    "        for batch, data in enumerate(train_data_loader):\n",
    "            ae.train()\n",
    "\n",
    "            #\n",
    "            # Select the X and y data\n",
    "            #\n",
    "\n",
    "            x = data[:, 0:(len(descriptors) + len(latent_col_names))]\n",
    "            descrip_true = data[:, 0:len(descriptors)]\n",
    "            \n",
    "            if len(latent_col_names) > 0:\n",
    "                latents = data[:, len(descriptors):(len(descriptors) + len(latent_col_names))]\n",
    "            \n",
    "            # Add additional property predictions here ..\n",
    "            y1 = data[:, len(descriptors) + len(latent_col_names)]\n",
    "\n",
    "            # For dataset 1\n",
    "            # design1 = data[:, (len(descriptors) + len(latent_col_names) + 1):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion))]\n",
    "            # design2 = data[:, (len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion)):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion))]\n",
    "            # design3 = data[:, (len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion)):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion) + len(target_X_ion))]\n",
    "            # # For dataset 2\n",
    "            design1 = data[:, (len(descriptors) + len(latent_col_names) + 1):(len(descriptors) + len(latent_col_names) + 1 + len(target_solvent))]\n",
    "\n",
    "            #\n",
    "            # Extract the AE prediuctions\n",
    "            #\n",
    "\n",
    "            # sgd.zero_grad()\n",
    "            adam.zero_grad()\n",
    "            ae_out = ae(x)\n",
    "            y1_pred, z = ae_out['y1_pred'], ae_out['z']\n",
    "            x_pred = ae_out['x_pred']\n",
    "\n",
    "            # For dataset 1\n",
    "            # design1_pred, design2_pred, design3_pred = ae_out['design1_pred'], ae_out['design2_pred'], ae_out['design3_pred']\n",
    "            # For dataset 2\n",
    "            design1_pred = ae_out['design1_pred']\n",
    "\n",
    "            if len(latent_col_names) > 0:\n",
    "                latents_pred = ae_out['latents_pred']\n",
    "            \n",
    "            #\n",
    "            #  Calculate the prediction losses\n",
    "            #\n",
    "\n",
    "            train_y1_pred_loss = y1_l1_loss(y1_pred, y1.reshape(-1, 1))\n",
    "            if print_losses:print('train_y1_pred_loss:', train_y1_pred_loss)\n",
    "            train_y_pred_loss = train_y1_pred_loss\n",
    "\n",
    "            #\n",
    "            # Calculate the design predictions losses\n",
    "            #\n",
    "\n",
    "            train_design1_pred_loss = design1_loss(design1_pred, design1)\n",
    "            if print_losses: print('train_design1_pred_loss:', train_design1_pred_loss)\n",
    "\n",
    "            # train_design2_pred_loss = design2_loss(design2_pred, design2)\n",
    "            # if print_losses: print('train_design2_pred_loss:', train_design2_pred_loss)\n",
    "\n",
    "            # train_design3_pred_loss = design3_loss(design3_pred, design3)\n",
    "            # if print_losses: print('train_design3_pred_loss:', train_design3_pred_loss)\n",
    "\n",
    "            # train_design_pred_loss = train_design1_pred_loss + train_design2_pred_loss + train_design3_pred_loss \n",
    "            train_design_pred_loss = train_design1_pred_loss\n",
    "\n",
    "            #\n",
    "            # Calculate the latent losses\n",
    "            #\n",
    "\n",
    "            if len(latent_col_names) > 0:\n",
    "                train_latent_pred_loss = latent_l1_loss(latents_pred, latents)\n",
    "            else:\n",
    "                train_latent_pred_loss = torch.tensor(0)\n",
    "            if print_losses: print('train_latent_pred_loss:', train_latent_pred_loss)\n",
    "\n",
    "            #\n",
    "            # Calculate X prediction losses\n",
    "            #\n",
    "                    \n",
    "            train_x_pred_loss = x_l1_loss(x_pred, descrip_true)\n",
    "            if print_losses: print('train_x_pred_loss:', train_x_pred_loss)\n",
    "\n",
    "            # train_total_pred_loss = pred_lam*train_y_pred_loss + design_lam*train_design_pred_loss + latent_lam*train_latent_pred_loss\n",
    "            train_total_pred_loss = pred_lam*train_y_pred_loss + design_lam*train_design_pred_loss + latent_lam*train_latent_pred_loss + train_x_pred_loss\n",
    "\n",
    "            # Get index of max value fro each row of design_pred_idxs\n",
    "            design1_pred_idxs_train = torch.argmax(torch.softmax(design1_pred, dim=1), dim=1)\n",
    "            design1_true_idxs_train = torch.argmax(design1, dim=1)\n",
    "            if debug:\n",
    "                print(f'First 20 predicted indices train : {design1_pred_idxs_train[:20]}')\n",
    "                print(f'First 20 true indices train : {design1_true_idxs_train[:20]}')\n",
    "            # Calculate train accuracy\n",
    "            design1_train_accuracy = (design1_pred_idxs_train == design1_true_idxs_train).float().mean().item() * 100\n",
    "            if print_losses:\n",
    "                print(f'Train Accuracy : {design1_train_accuracy}')\n",
    "\n",
    "            # design2_pred_idxs_train = torch.argmax(torch.softmax(design2_pred, dim=1), dim=1)\n",
    "            # design2_true_idxs_train = torch.argmax(design2, dim=1)\n",
    "            # if debug:\n",
    "            #     print(f'First 20 predicted indices train : {design2_pred_idxs_train[:20]}')\n",
    "            #     print(f'First 20 true indices train : {design2_true_idxs_train[:20]}')\n",
    "            # # Calculate train accuracy\n",
    "            # design2_train_accuracy = (design2_pred_idxs_train == design2_true_idxs_train).float().mean().item() * 100\n",
    "            # if print_losses:\n",
    "            #     print(f'Train Accuracy : {design2_train_accuracy}')\n",
    "\n",
    "            # design3_pred_idxs_train = torch.argmax(torch.softmax(design3_pred, dim=1), dim=1)\n",
    "            # design3_true_idxs_train = torch.argmax(design3, dim=1)\n",
    "            # if debug:\n",
    "            #     print(f'First 20 predicted indices train : {design3_pred_idxs_train[:20]}')\n",
    "            #     print(f'First 20 true indices train : {design3_true_idxs_train[:20]}')\n",
    "            # # Calculate train accuracy\n",
    "            # design3_train_accuracy = (design3_pred_idxs_train == design3_true_idxs_train).float().mean().item() * 100\n",
    "            # if print_losses:\n",
    "            #     print(f'Train Accuracy : {design3_train_accuracy}')\n",
    "\n",
    "            if l1_coeff > 0:\n",
    "                for name, param in ae.named_parameters():\n",
    "                    # Exclude 'BE_preedictor' from L1\n",
    "                    if 'BE_predictor' in name:\n",
    "                        continue\n",
    "                    else:\n",
    "                        train_total_pred_loss += l1_coeff * param.abs().sum()\n",
    "\n",
    "            if prop_l1_coeff > 0:\n",
    "                for name, param in ae.named_parameters():\n",
    "                    # Only include 'BE_preedictor' from L1\n",
    "                    if 'BE_predictor' in name:\n",
    "                        train_total_pred_loss += prop_l1_coeff * param.abs().sum()\n",
    "                    else:\n",
    "                        continue\n",
    "            \n",
    "            # # ---  STEP 1  --- : Compute pearson correlation coefficients\n",
    "            # # --- STEP 1.1 --- : with respect to target\n",
    "            rho_abs_w_target1 = torch.empty(num_y1_latents)\n",
    "            y1_mean, y1_std = y1.mean(), y1.std()\n",
    "            for j, l in enumerate(range(num_y1_latents)):\n",
    "                z_mean, z_std = z[:, l].mean(), z[:, l].std()\n",
    "                Czy = ((z[:, l] - z_mean) * (y1 - y1_mean)).mean()\n",
    "                rho_abs_w_target1[j] = abs(Czy / (z_std * y1_std))\n",
    "                # # Verify correctness of correflation coefficient agsinst torch corrcoef function\n",
    "                # concat_tensor = cat((z[:, i].reshape(-1, 1), y.reshape(-1, 1)), dim=1)\n",
    "                # print('Computed using inbuilt function')\n",
    "                # print(abs(corrcoef(concat_tensor.T)[0, 1]))\n",
    "            \n",
    "            # --- STEP 1.2 --- : with respect to other latent variables\n",
    "            rho_abs_w_latents = []\n",
    "            for j in range(z.shape[1]):\n",
    "                z_j_mean, z_j_std = z[:, j].mean(), z[:, j].std()\n",
    "                for k in range(j+1, z.shape[1]):\n",
    "                    z_k_mean, z_k_std = z[:, k].mean(), z[:, k].std()\n",
    "                    Czz = ((z[:, j] - z_j_mean) * (z[:, k] - z_k_mean)).mean()\n",
    "                    rho_abs_w_latents.append(abs(Czz / (z_j_std * z_k_std)))\n",
    "            rho_abs_w_latents = torch.stack(rho_abs_w_latents)\n",
    "\n",
    "            # ---  STEP 2  --- : Compute std err in PCCs and adjust rho_abs_w_target & rho_abs_w_latents\n",
    "            std_err = 1/math.sqrt(len(y1_pred) - 3) # (Ref : Gryffin)\n",
    "            if debug : print('std_err:', std_err)\n",
    "\n",
    "            rho_abs_w_target1_adj = (rho_abs_w_target1 - std_err)/(1 - std_err)\n",
    "            rho_abs_w_target1_adj[rho_abs_w_target1_adj < 0] = 0\n",
    "            if debug : print('rho_abs_w_target1_adj :', rho_abs_w_target1_adj)\n",
    "\n",
    "            rho_abs_w_latents_adj = (rho_abs_w_latents - std_err)/(1 - std_err)\n",
    "            rho_abs_w_latents_adj[rho_abs_w_latents_adj < 0] = 0\n",
    "            if debug : print('rho_abs_w_latents_adj :', rho_abs_w_latents_adj)\n",
    "\n",
    "            # ---  STEP 3  --- : Atleast one of the PCCs wrt target should be maximized\n",
    "            lambda_0_1 = torch.mean(1 - rho_abs_w_target1_adj)\n",
    "            if debug : print('lambda_0_1:', lambda_0_1)\n",
    "            \n",
    "            # --- Step 4 --- : Favor PCCs between latents that are close to 0\n",
    "            lambda_1 = torch.mean(torch.pow(torch.sin((math.pi/2)*rho_abs_w_latents_adj), 2))\n",
    "            if debug : print('lambda_1:', lambda_1)\n",
    "            \n",
    "            # --- Step 5 --- : Add all the losses\n",
    "            train_total_pred_loss += lambda_0_1 + lambda_1\n",
    "            \n",
    "            train_total_pred_loss.backward()\n",
    "            # sgd.step()\n",
    "            adam.step()\n",
    "            # Store the losses for each batch\n",
    "            train_total_pred_loss_per_batch += train_total_pred_loss.item()\n",
    "            train_y_pred_loss_per_batch += train_y_pred_loss.item()\n",
    "            train_design_pred_loss_per_batch += train_design_pred_loss.item()\n",
    "            train_latent_pred_loss_per_batch += train_latent_pred_loss.item()\n",
    "            train_x_pred_loss_per_batch += train_x_pred_loss.item()\n",
    "        \n",
    "        ind_losses_dict_train[f'seed_{i}'] = [train_y1_pred_loss.item(),\n",
    "                                              train_design1_pred_loss.item(),\n",
    "                                              train_latent_pred_loss.item(),\n",
    "                                            #   train_design2_pred_loss.item(), \n",
    "                                            #   train_design3_pred_loss.item(),\n",
    "                                              train_x_pred_loss.item()]\n",
    "\n",
    "        val_y_pred_loss_per_batch = 0\n",
    "        val_design_pred_loss_per_batch = 0\n",
    "        val_latent_pred_loss_per_batch = 0\n",
    "        val_x_pred_loss_per_batch = 0\n",
    "\n",
    "        # Validation Loop\n",
    "        for batch, data in enumerate(val_data_loader):\n",
    "            ae.eval()\n",
    "\n",
    "            x = data[:, 0:(len(descriptors) + len(latent_col_names))]\n",
    "            descrip_true = data[:, 0:len(descriptors)]\n",
    "\n",
    "            if len(latent_col_names) > 0:\n",
    "                latents = data[:, len(descriptors):(len(descriptors) + len(latent_col_names))]\n",
    "\n",
    "            # Add additional property predictions here ..\n",
    "            y1 = data[:, len(descriptors) + len(latent_col_names)]\n",
    "\n",
    "            # For dataset 1\n",
    "            # design1 = data[:, (len(descriptors) + len(latent_col_names) + 1):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion))]\n",
    "            # design2 = data[:, (len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion)):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion))]\n",
    "            # design3 = data[:, (len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion)):(len(descriptors) + len(latent_col_names) + 1 + len(target_A_ion) + len(target_B_ion) + len(target_X_ion))]\n",
    "            # For dataset 2\n",
    "            design1 = data[:, (len(descriptors) + len(latent_col_names) + 1):(len(descriptors) + len(latent_col_names) + 1 + len(target_solvent))]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ae_out = ae(x)\n",
    "\n",
    "                y1_pred, z = ae_out['y1_pred'], ae_out['z']\n",
    "                x_pred = ae_out['x_pred']\n",
    "\n",
    "                # design1_pred, design2_pred, design3_pred = ae_out['design1_pred'], ae_out['design2_pred'], ae_out['design3_pred']\n",
    "                design1_pred = ae_out['design1_pred']\n",
    "                if len(latent_col_names) > 0:\n",
    "                    latents_pred = ae_out['latents_pred']\n",
    "\n",
    "                val_y1_pred_loss = y1_l1_loss(y1_pred, y1.reshape(-1, 1))\n",
    "                if print_losses:\n",
    "                    print('val_y1_pred_loss:', val_y1_pred_loss)\n",
    "                val_y_pred_loss = val_y1_pred_loss\n",
    "\n",
    "                val_design1_pred_loss = design1_loss(design1_pred, design1)\n",
    "                # val_design2_pred_loss = design2_loss(design2_pred, design2)\n",
    "                # val_design3_pred_loss = design3_loss(design3_pred, design3)\n",
    "                # val_design_pred_loss = val_design1_pred_loss + val_design2_pred_loss + val_design3_pred_loss\n",
    "                val_design_pred_loss = val_design1_pred_loss\n",
    "\n",
    "                if len(latent_col_names) > 0:\n",
    "                    val_latent_pred_loss = latent_l1_loss(latents_pred, latents)\n",
    "                else:\n",
    "                    val_latent_pred_loss = torch.tensor(0)\n",
    "\n",
    "                val_x_pred_loss = x_l1_loss(x_pred, descrip_true)\n",
    "                if print_losses:\n",
    "                    print('val_x_pred_loss:', val_x_pred_loss)\n",
    "\n",
    "            # Get index of max value fro each row of design_pred_idxs\n",
    "            design1_pred_idxs_val = torch.argmax(torch.softmax(design1_pred, dim=1), dim=1)\n",
    "            design1_true_idxs_val = torch.argmax(design1, dim=1)\n",
    "            val_accuracy = (design1_pred_idxs_val == design1_true_idxs_val).float().mean().item() * 100\n",
    "            if print_losses:\n",
    "                print(f'Val Accuracy : {val_accuracy}')\n",
    "            \n",
    "            # design2_pred_idxs_val = torch.argmax(torch.softmax(design2_pred, dim=1), dim=1)\n",
    "            # design2_true_idxs_val = torch.argmax(design2, dim=1)\n",
    "            # val_accuracy = (design2_pred_idxs_val == design2_true_idxs_val).float().mean().item() * 100\n",
    "            # if print_losses:\n",
    "            #     print(f'Val Accuracy : {val_accuracy}')\n",
    "\n",
    "            # design3_pred_idxs_val = torch.argmax(torch.softmax(design3_pred, dim=1), dim=1)\n",
    "            # design3_true_idxs_val = torch.argmax(design3, dim=1)\n",
    "            # val_accuracy = (design3_pred_idxs_val == design3_true_idxs_val).float().mean().item() * 100\n",
    "            # if print_losses:\n",
    "            #     print(f'Val Accuracy : {val_accuracy}')\n",
    "\n",
    "            val_y_pred_loss_per_batch += val_y_pred_loss.item()\n",
    "            val_design_pred_loss_per_batch += val_design_pred_loss.item()\n",
    "            val_latent_pred_loss_per_batch += val_latent_pred_loss.item()\n",
    "            val_x_pred_loss_per_batch += val_x_pred_loss.item()\n",
    "\n",
    "            # Printing purposes\n",
    "            if print_losses:\n",
    "                if batch % print_every_n_batches == 0:\n",
    "                    print(f'Batch {batch}/{len(val_data_loader)}, Y Pred Loss: {val_y_pred_loss.item():.4f}, Design Pred Loss: {val_design_pred_loss.item():.4f}')\n",
    "\n",
    "        # Store train loss curves\n",
    "        train_total_pred_loss_per_epoch.append(train_total_pred_loss_per_batch / len(train_data_loader))\n",
    "\n",
    "        train_y_pred_loss_per_epoch.append(train_y_pred_loss_per_batch / len(train_data_loader))\n",
    "        train_design_pred_loss_per_epoch.append(train_design_pred_loss_per_batch / len(train_data_loader))\n",
    "        train_latent_pred_loss_per_epoch.append(train_latent_pred_loss_per_batch / len(train_data_loader))\n",
    "        train_x_pred_loss_per_epoch.append(train_x_pred_loss_per_batch / len(train_data_loader))\n",
    "\n",
    "        val_y_pred_loss_per_epoch.append(val_y_pred_loss_per_batch / len(val_data_loader))\n",
    "        val_design_pred_loss_per_epoch.append(val_design_pred_loss_per_batch / len(val_data_loader))\n",
    "        val_latent_pred_loss_per_epoch.append(val_latent_pred_loss_per_batch / len(val_data_loader))\n",
    "        val_x_pred_loss_per_epoch.append(val_x_pred_loss_per_batch / len(val_data_loader))\n",
    "\n",
    "        if print_losses:\n",
    "            print(f' --------- Epoch Stats {epoch+1}/{num_epochs} --------- ')\n",
    "            print(f' -- Train -- Total Loss: {train_total_pred_loss_per_epoch[-1]:.4f},\\\n",
    "                                 Y Pred Loss: {train_y_pred_loss_per_epoch[-1]:.4f},\\\n",
    "                                 Design Pred Loss: {train_design_pred_loss_per_epoch[-1]:.4f},\\\n",
    "                                 Latent Pred Loss: {train_latent_pred_loss_per_epoch[-1]:.4f}')\n",
    "            print(f' --  Val  -- Y Pred Loss: {val_y_pred_loss_per_epoch[-1]:.4f},\\\n",
    "                                 Design Pred Loss: {val_design_pred_loss_per_epoch[-1]:.4f},\\\n",
    "                                 Latent Pred Loss: {val_latent_pred_loss_per_epoch[-1]:.4f}')\n",
    "            print(f' ------------------------------------------')\n",
    "\n",
    "    ind_losses_dict_val[f'seed_{i}'] = [val_y1_pred_loss.item(), \n",
    "                                        val_design1_pred_loss.item(), \n",
    "                                        val_latent_pred_loss.item(),\n",
    "                                        # val_design2_pred_loss.item(), \n",
    "                                        # val_design3_pred_loss.item(),\n",
    "                                        val_x_pred_loss.item()]\n",
    "\n",
    "    train_total_pred_loss_per_epoch_per_seed.append(train_total_pred_loss_per_epoch)\n",
    "    train_y_pred_loss_per_epoch_per_seed.append(train_y_pred_loss_per_epoch)\n",
    "    train_design_pred_loss_per_epoch_per_seed.append(train_design_pred_loss_per_epoch)\n",
    "    train_latent_pred_loss_per_epoch_per_seed.append(train_latent_pred_loss_per_epoch)\n",
    "    train_x_pred_loss_per_epoch_per_seed.append(train_x_pred_loss_per_epoch)\n",
    "\n",
    "    val_y_pred_loss_per_epoch_per_seed.append(val_y_pred_loss_per_epoch)\n",
    "    val_design_pred_loss_per_epoch_per_seed.append(val_design_pred_loss_per_epoch)\n",
    "    val_latent_pred_loss_per_epoch_per_seed.append(val_latent_pred_loss_per_epoch)\n",
    "    val_x_pred_loss_per_epoch_per_seed.append(val_x_pred_loss_per_epoch)\n",
    "\n",
    "    # Save the model to the runs directory\n",
    "    model_save_path = f'runs/{model_save_dir}/fold{i}_soft_constraints'\n",
    "    torch.save(ae.state_dict(), model_save_path)\n",
    "    print(f'Model saved to {model_save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898df50",
   "metadata": {},
   "source": [
    "### Choosing one of the models trained on the k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc754de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "folds = np.arange(0, 10, 1)\n",
    "all_train_pred_losses = np.array([ind_losses_dict_train[f'seed_{fold}'] for fold in folds])\n",
    "all_val_pred_losses = np.array([ind_losses_dict_val[f'seed_{fold}'] for fold in folds])\n",
    "# ax[0].plot(folds, all_train_pred_losses[:, 0].squeeze(), label='Train Y1 Loss', marker='o', linestyle='-')\n",
    "ax[0].plot(folds, all_train_pred_losses[:, 1], label='Train Design1 Loss', marker='o', linestyle='-')\n",
    "ax[0].plot(folds, all_train_pred_losses[:, 2], label='Train Latent Loss', marker='o', linestyle='-')\n",
    "ax[0].plot(folds, all_train_pred_losses[:, 3], label='Train Desc Loss', marker='o', linestyle='-')\n",
    "# ax[0].plot(folds, all_train_pred_losses[:, 2], label='Train Design2 Loss', marker='o', linestyle='-')\n",
    "# ax[0].plot(folds, all_train_pred_losses[:, 3], label='Train Design3 Loss', marker='o', linestyle='-')\n",
    "# ax[0].plot(folds, all_train_pred_losses[:, 4], label='Train X Loss', marker='o', linestyle='-')\n",
    "ax[0].set_xlabel('Fold')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Train Losses per Fold')\n",
    "ax[0].legend()\n",
    "ax[0].set_xticks(folds)\n",
    "ax[1].plot(folds, all_val_pred_losses[:, 0], label='Val Y1 Loss', marker='o', linestyle='-')\n",
    "ax[1].plot(folds, all_val_pred_losses[:, 1], label='Val Design1 Loss', marker='o', linestyle='-')\n",
    "ax[1].plot(folds, all_val_pred_losses[:, 2], label='Val Latent Loss', marker='o', linestyle='-')\n",
    "ax[1].plot(folds, all_val_pred_losses[:, 3], label='Val Desc Loss', marker='o', linestyle='-')\n",
    "# ax[1].plot(folds, all_val_pred_losses[:, 2], label='Val Design2 Loss', marker='o', linestyle='-')\n",
    "# ax[1].plot(folds, all_val_pred_losses[:, 3], label='Val Design3 Loss', marker='o', linestyle='-')\n",
    "# ax[1].plot(folds, all_val_pred_losses[:, 4], label='Val X Loss', marker='o', linestyle='-')\n",
    "ax[1].set_xlabel('Fold')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Val Losses per Fold')\n",
    "ax[1].legend()\n",
    "ax[1].set_xticks(folds)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(all_train_pred_losses)\n",
    "print(all_val_pred_losses)\n",
    "\n",
    "print(np.sum(all_val_pred_losses, axis=1))\n",
    "print(np.argmin(np.sum(all_val_pred_losses, axis=1)))\n",
    "# ax[0].plot(folds, ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c9dea",
   "metadata": {},
   "source": [
    "### Load the selected trained AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba080c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arctanh(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.atanh(torch.clamp(x, -0.999999, 0.999999))\n",
    "    \n",
    "atanh_act_fn = Arctanh()\n",
    "\n",
    "saved_module_name = 'ae2'\n",
    "latent_dim = 7\n",
    "fold_num = 3\n",
    "module_params = {'name':'AE1', \n",
    "                    'modules':{\n",
    "                        \n",
    "                        # AE1 encoder = AE2 encoder archiecture\n",
    "                        'encoder':{\n",
    "                            'input_dim':17,\n",
    "                            'output_dim':latent_dim, #8 \n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1, \n",
    "                            'hidden_activation':None, \n",
    "                            'output_activation':torch.nn.Tanh(), \n",
    "                            'layer_kernel_init':'xavier_normal', \n",
    "                            'layer_bias_init':'zeros', \n",
    "                            },\n",
    "\n",
    "                        # 'bandgaps_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':1,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':torch.nn.ReLU(),\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'},\n",
    "\n",
    "                        'BE_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':1,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':torch.nn.ReLU(),\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'},\n",
    "                        \n",
    "                        'latents_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':11,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':atanh_act_fn,\n",
    "                            'output_activation':None,\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'},\n",
    "\n",
    "                        # 'A_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':5,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        # 'B_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':6,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        # 'X_predictor':{\n",
    "                        #     'input_dim':latent_dim,\n",
    "                        #     'output_dim':3,\n",
    "                        #     'hidden_dim':25,\n",
    "                        #     'hidden_layers':1,\n",
    "                        #     'hidden_activation':torch.nn.ReLU(),\n",
    "                        #     'output_activation':None,\n",
    "                        #     # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                        #     'layer_kernel_init':'xavier_normal',\n",
    "                        #     'layer_bias_init':'zeros'\n",
    "                        # },\n",
    "\n",
    "                        'decoder':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':6,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':None,\n",
    "                            # 'layer_dropout':torch.nn.Dropout(p=0.5),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        },\n",
    "                        \n",
    "                        'solvent_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':8,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':None,\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        },\n",
    "\n",
    "                    }}\n",
    "\n",
    "loaded_state_dict = f'runs/{saved_module_name}_bandgaps_THEN_perov_solv_BE/fold{fold_num}_soft_constraints_{latent_dim}D'\n",
    "loaded_ae = AE(module_params)\n",
    "loaded_ae.load_state_dict(torch.load(loaded_state_dict))\n",
    "loaded_ae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0770a16",
   "metadata": {},
   "source": [
    "### Property predictions for all samples in current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[train_idxs[fold_num]]\n",
    "val_dataset = dataset[val_idxs[fold_num]]\n",
    "print(f'Train numpy dataset shape : {train_dataset.shape}, Val. numpy dataset shape : {val_dataset.shape}')\n",
    "train_dataset_tensor = torch.from_numpy(train_dataset[:, 0:(len(descriptors) + len(latent_col_names))]).to(dtype=torch.float32)\n",
    "val_dataset_tensor = torch.from_numpy(val_dataset[:, 0:(len(descriptors) + len(latent_col_names))]).to(dtype=torch.float32)\n",
    "\n",
    "# bandgaps_true_train = train_dataset[:, 15]\n",
    "# bandgaps_true_val = val_dataset[:, 15]\n",
    "# bandgaps_true = np.concatenate((bandgaps_true_train, bandgaps_true_val), axis=0)\n",
    "\n",
    "be_true_train = train_dataset[:, 17]\n",
    "be_true_val = val_dataset[:, 17]\n",
    "be_true = np.concatenate((be_true_train, be_true_val), axis=0)\n",
    "\n",
    "loaded_ae.eval()\n",
    "with torch.no_grad():\n",
    "    ae_out_train = loaded_ae(train_dataset_tensor)\n",
    "    ae_out_val = loaded_ae(val_dataset_tensor)\n",
    "\n",
    "# bandgaps_pred_train = ae_out_train['y1_pred']\n",
    "# bandgaps_pred_val = ae_out_val['y1_pred']\n",
    "# bandgaps_pred = torch.cat((bandgaps_pred_train, bandgaps_pred_val), dim=0)\n",
    "\n",
    "be_pred_train = ae_out_train['y1_pred']\n",
    "be_pred_val = ae_out_val['y1_pred']\n",
    "be_pred = torch.cat((be_pred_train, be_pred_val), dim=0)\n",
    "\n",
    "# mhp_bgs_true_vs_pred_train = pd.DataFrame({'True Bandgaps': bandgaps_true_train, 'Predicted Bandgaps': bandgaps_pred_train.detach().numpy().squeeze()})\n",
    "# mhp_bgs_true_vs_pred_train.to_csv('bandgaps_data_train.csv', index=False)\n",
    "\n",
    "# mhp_bgs_true_vs_pred_val = pd.DataFrame({'True Bandgaps': bandgaps_true_val, 'Predicted Bandgaps': bandgaps_pred_val.numpy().squeeze()})\n",
    "# mhp_bgs_true_vs_pred_val.to_csv('bandgaps_data_val.csv', index=False)\n",
    "\n",
    "mhp_be_true_vs_pred_train = pd.DataFrame({'True Binding Energy': be_true_train, 'Predicted Binding Energy': be_pred_train.numpy().squeeze()})\n",
    "mhp_be_true_vs_pred_train.to_csv('be_data_train.csv', index=False)\n",
    "\n",
    "mhp_be_true_vs_pred_val = pd.DataFrame({'True Binding Energy': be_true_val, 'Predicted Binding Energy': be_pred_val.numpy().squeeze()})\n",
    "mhp_be_true_vs_pred_val.to_csv('be_data_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef686ea",
   "metadata": {},
   "source": [
    "### Property predictions for samples binding energy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(desc_means).reshape(-1, 1), np.array(desc_std_devs).reshape(-1, 1))\n",
    "\n",
    "# x_df = pd.read_csv('datasets/H2_prod_rate/props_from_sa_h2_rate.csv')[descriptors]\n",
    "x_df = pd.read_csv('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedae_dataset/perov_solv_BE_for_nestedae.csv')[descriptors]\n",
    "\n",
    "for i, desc in enumerate(x_df.columns.tolist()):\n",
    "    mean = x_df[desc].mean()\n",
    "    std = x_df[desc].std()\n",
    "    x_df[desc] = (x_df[desc] - desc_means[i]) / desc_std_devs[i]\n",
    "\n",
    "x_torch = torch.from_numpy(x_df.to_numpy(dtype=np.float32)).to(dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    ae_out = loaded_ae(x_torch)\n",
    "latents = ae_out['z']\n",
    "bandgaps = ae_out['y1_pred']\n",
    "a_design_pred = torch.argmax(torch.softmax(ae_out['design1_pred'], dim=1), dim=1)\n",
    "b_design_pred = torch.softmax(ae_out['design2_pred'], dim=1)\n",
    "x_design_pred = torch.softmax(ae_out['design3_pred'], dim=1)\n",
    "np.savetxt('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedae_AE1_preds_perov_solv_BE/a_design_pred.csv', a_design_pred.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedae_AE1_preds_perov_solv_BE/b_design_pred.csv', b_design_pred.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedae_AE1_preds_perov_solv_BE/x_design_pred.csv', x_design_pred.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedae_AE1_preds_perov_solv_BE/bandgaps_pred.csv', bandgaps.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt('datasets/MHP_bandgaps_AND_perov_solvent_BE/nestedae_AE1_preds_perov_solv_BE/latents_pred.csv', latents.detach().numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb494698",
   "metadata": {},
   "source": [
    "#### Plot model training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latents = len(latent_col_names)\n",
    "epochs = np.arange(1, num_epochs + 1)\n",
    "# Plot mean and standard deviation of losses\n",
    "train_mean_total_pred_loss = np.mean(np.log(np.array(train_total_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "train_std_total_pred_loss = np.std(np.log(np.array(train_total_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "train_mean_y_pred_loss = np.mean(np.log(np.array(train_y_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "train_std_y_pred_loss = np.std(np.log(np.array(train_y_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "train_mean_design_pred_loss = np.mean(np.log(np.array(train_design_pred_loss_per_epoch_per_seed)), axis=0) \n",
    "train_std_design_pred_loss = np.std(np.log(np.array(train_design_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "if num_latents > 0:\n",
    "    train_mean_latent_pred_loss = np.mean(np.log(np.array(train_latent_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "    train_std_latent_pred_loss = np.std(np.log(np.array(train_latent_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "else:\n",
    "    train_mean_latent_pred_loss = np.mean(train_latent_pred_loss_per_epoch_per_seed, axis=0)\n",
    "    train_std_latent_pred_loss = np.std(train_latent_pred_loss_per_epoch_per_seed, axis=0)\n",
    "\n",
    "train_mean_x_pred_loss = np.mean(np.log(np.array(train_x_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "train_std_x_pred_loss = np.std(np.log(np.array(train_x_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "val_mean_y_pred_loss = np.mean(np.log(np.array(val_y_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "val_std_y_pred_loss = np.std(np.log(np.array(val_y_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "val_mean_design_pred_loss = np.mean(np.log(np.array(val_design_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "val_std_design_pred_loss = np.std(np.log(np.array(val_design_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "if num_latents > 0:\n",
    "    val_mean_latent_pred_loss = np.mean(np.log(np.array(val_latent_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "    val_std_latent_pred_loss = np.std(np.log(np.array(val_latent_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "else:\n",
    "    val_mean_latent_pred_loss = np.mean(val_latent_pred_loss_per_epoch_per_seed, axis=0)\n",
    "    val_std_latent_pred_loss = np.std(val_latent_pred_loss_per_epoch_per_seed, axis=0)\n",
    "\n",
    "val_mean_x_pred_loss = np.mean(np.log(np.array(val_x_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "val_std_x_pred_loss = np.std(np.log(np.array(val_x_pred_loss_per_epoch_per_seed)), axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 15))\n",
    "plt.subplot(9, 1, 1)\n",
    "plt.plot(epochs, train_mean_total_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(train_total_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(train_total_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', \n",
    "        color='black')\n",
    "plt.fill_between(epochs, train_mean_total_pred_loss - train_std_total_pred_loss, \n",
    "                 train_mean_total_pred_loss + train_std_total_pred_loss, color='black', alpha=0.2)\n",
    "plt.title('Train Total Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(9, 1, 2)\n",
    "plt.plot(epochs, train_mean_y_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(train_y_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(train_y_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', \n",
    "         color='blue')\n",
    "plt.fill_between(epochs, train_mean_y_pred_loss - train_std_y_pred_loss,\n",
    "                train_mean_y_pred_loss + train_std_y_pred_loss, color='blue', alpha=0.2)\n",
    "plt.title('Train Y Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(9, 1, 3)\n",
    "plt.plot(epochs, train_mean_design_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(train_design_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(train_design_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', \n",
    "         color='green')\n",
    "plt.fill_between(epochs, train_mean_design_pred_loss - train_std_design_pred_loss,\n",
    "                 train_mean_design_pred_loss + train_std_design_pred_loss, color='green', alpha=0.2)\n",
    "plt.title('Train Design Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(9, 1, 4)\n",
    "plt.plot(epochs, train_mean_latent_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(train_latent_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(train_latent_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', \n",
    "         color='darkorange')\n",
    "plt.fill_between(epochs, train_mean_latent_pred_loss - train_std_latent_pred_loss,\n",
    "                  train_mean_latent_pred_loss + train_std_latent_pred_loss, color='darkorange', alpha=0.2)\n",
    "plt.title('Train Latent Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(9, 1, 5)\n",
    "plt.plot(epochs, train_mean_x_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(train_x_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(train_x_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', \n",
    "         color='darkorange')\n",
    "plt.fill_between(epochs, train_mean_x_pred_loss - train_std_x_pred_loss,\n",
    "                  train_mean_x_pred_loss + train_std_x_pred_loss, color='darkorange', alpha=0.2)\n",
    "plt.title('Train X Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(9, 1, 6)\n",
    "plt.plot(epochs, val_mean_y_pred_loss,\n",
    "         label=f'{round(np.mean(np.array(val_y_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(val_y_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', color='lightskyblue')\n",
    "plt.fill_between(epochs, val_mean_y_pred_loss - val_std_y_pred_loss,\n",
    "                  val_mean_y_pred_loss + val_std_y_pred_loss, color='lightskyblue', alpha=0.2)\n",
    "plt.title('Val Y Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(9, 1, 7)\n",
    "plt.plot(epochs, val_mean_design_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(val_design_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-' +\n",
    "               f'{round(np.std(np.array(val_design_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', color='yellowgreen')\n",
    "plt.fill_between(epochs, val_mean_design_pred_loss - val_std_design_pred_loss, val_mean_design_pred_loss + val_std_design_pred_loss, color='yellowgreen', alpha=0.2)\n",
    "plt.title('Val Design Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(9, 1, 8)\n",
    "plt.plot(epochs, val_mean_latent_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(val_latent_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-'+\n",
    "               f'{round(np.std(np.array(val_latent_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', color='orange')\n",
    "plt.fill_between(epochs, val_mean_latent_pred_loss - val_std_latent_pred_loss, val_mean_latent_pred_loss + val_std_latent_pred_loss, color='orange', alpha=0.2)\n",
    "plt.title('Val Latent Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(9, 1, 9)\n",
    "plt.plot(epochs, val_mean_x_pred_loss, \n",
    "         label=f'{round(np.mean(np.array(val_x_pred_loss_per_epoch_per_seed)[:, -1]), 2)} +/-'+\n",
    "               f'{round(np.std(np.array(val_x_pred_loss_per_epoch_per_seed)[:, -1]), 2)}', color='orange')\n",
    "plt.fill_between(epochs, val_mean_x_pred_loss - val_std_x_pred_loss, val_mean_x_pred_loss + val_std_x_pred_loss, color='orange', alpha=0.2)\n",
    "plt.title('Val X Pred Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('log(loss)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cac0b",
   "metadata": {},
   "source": [
    "### Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(ExactGP, GPyTorchModel):\n",
    "    _num_outputs = 1\n",
    "    MIN_INFERRED_NOISE_LEVEL = 1e-5\n",
    "\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        if kernel == 'RBF':\n",
    "            self.covar_module = ScaleKernel(RBFKernel())\n",
    "        elif kernel == 'Matern':\n",
    "            self.covar_module = ScaleKernel(MaternKernel(nu=2.5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "def train_model(train_x, train_y, epochs, lr):\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood, 'Matern')\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model, likelihood\n",
    "\n",
    "def optimize_acq_fn_and_get_observation(acq_fn, x_test, y_test):\n",
    "    candidates, _ = optimize_acqf_discrete(\n",
    "        acq_function=acq_fn,\n",
    "        choices = x_test, # IMP : Restricts the space from which to select the next candidate \n",
    "        q=1, # Number of candidates to select\n",
    "        max_batch_size=2048,  \n",
    "        num_restarts=10, # Number of random restarts for optimizer\n",
    "        raw_samples=512, # Number of random samples for initialization for optimizer\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        unique=True # Whether to return only unique candidates\n",
    "    )\n",
    "\n",
    "    # observe new values. In this case just retreieve 1 value\n",
    "    x_new = candidates.detach()\n",
    "    # Find index where it matches in x_test\n",
    "    match_idx = (x_test == x_new).all(dim = 1)\n",
    "    # Find where it matches in y_test\n",
    "    y_new = y_test[match_idx]\n",
    "\n",
    "    # Remove from the candidate from the test\n",
    "    x_test = x_test[~match_idx]\n",
    "    y_test = y_test[~match_idx]\n",
    "\n",
    "    return x_new, y_new, match_idx, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# Code adopted from PAL2 acquition_function_bo.ipynb. MAKE SURE TO ACK AND CITE !!!\n",
    "####################################################################################\n",
    "\n",
    "# First load the model and get the latents and their predictions\n",
    "x_torch_all = torch.from_numpy(dataset[:, 0:(len(descriptors + latent_col_names))]).to(dtype=torch.float32)\n",
    "# Using the true values for prediction\n",
    "y_true = torch.from_numpy(dataset[:, len(descriptors + latent_col_names)]).to(dtype=torch.float32)\n",
    "\n",
    "# Check for nan in dataset\n",
    "if np.isnan(x_torch_all).any():\n",
    "    print('Dataset has NaN values. Please check.')\n",
    "else:\n",
    "    print('Dataset has no NaN values.')\n",
    "\n",
    "maximize = False\n",
    "verbose = False   \n",
    "bo_n_trials = 200\n",
    "bo_n_updates = 10\n",
    "test_size = 0.9\n",
    "print(test_size)\n",
    "bo_n_batches = 100\n",
    "print(f'bo_n_batches = {bo_n_batches}')\n",
    "epochs_train_GP = 100\n",
    "lr_GP = 0.1\n",
    "save_filename1 = 'bo_results_no_feat_sel_min.csv'\n",
    "save_filename2 = 'batches_to_reach_optimal_no_feat_sel_min.csv'\n",
    "save_filename3 = 'selected_latent_hist_no_feat_sel_min.npy'\n",
    "total_number_of_materials_in_dataset = x_torch_all.shape[0]\n",
    "\n",
    "# loaded_ae.eval()\n",
    "# with torch.no_grad():\n",
    "#     ae_out = loaded_ae(x_torch_all)\n",
    "# latents = ae_out['z']\n",
    "\n",
    "latents_stand = x_torch_all\n",
    "# No need to standardize x_torch_all as it has already been standardized in the beginning\n",
    "# latents_stand = (latents - latents.mean(dim=0)) / latents.std(dim=0)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Sanity check : Identify row indices where the latent values are the same in latents\n",
    "duplicate_latent_indices = []\n",
    "for i in range(latents_stand.shape[0]):\n",
    "    for j in range(i + 1, latents_stand.shape[0]):\n",
    "        if torch.equal(latents_stand[i], latents_stand[j]):\n",
    "            duplicate_latent_indices.append((i, j))\n",
    "if len(duplicate_latent_indices) > 0:\n",
    "    print(f'Found duplicate latent representations at indices: {duplicate_latent_indices}')\n",
    "    raise ValueError(\"Duplicate latent representations found.\")\n",
    "\n",
    "best_obs_val_hist = []\n",
    "selected_latent_hist = []\n",
    "selected_obs_hist = []\n",
    "for trial_num in range(1, bo_n_trials + 1):\n",
    "    t0 = time.monotonic()\n",
    "\n",
    "    print(f' ----- Trial {trial_num} ----- ')\n",
    "    latents_train_stand, latents_test_stand, y_train, y_test = train_test_split(latents_stand, y_true, \n",
    "                                                                    test_size=test_size,\n",
    "                                                                    random_state=trial_num)\n",
    "    print(f'Train size: {latents_train_stand.shape[0]}, Test size: {latents_test_stand.shape[0]}')\n",
    "\n",
    "    # check for max in preds\n",
    "    if maximize : \n",
    "        best_obs_val = y_train.max()\n",
    "        best_obs_val_idx = y_train.argmax()\n",
    "        optimal_soln = torch.cat([y_train, y_test]).max()\n",
    "    else:\n",
    "        best_obs_val = y_train.min()\n",
    "        best_obs_val_idx = y_train.argmin()\n",
    "        optimal_soln = torch.cat([y_train, y_test]).min()\n",
    "\n",
    "    if best_obs_val.eq(optimal_soln):\n",
    "        if maximize:\n",
    "            print(f'Max in training set. Removing it. Max value is {optimal_soln}')\n",
    "        else:\n",
    "            print(f'Min in training set. Removing it. Min value is {optimal_soln}')\n",
    "        # removing from train and adding to test\n",
    "        latents_test_stand = torch.cat([latents_test_stand, latents_train_stand[best_obs_val_idx, :].unsqueeze(0)], dim=0)\n",
    "        latents_train_stand = torch.cat([latents_train_stand[0:best_obs_val_idx, :], latents_train_stand[best_obs_val_idx + 1:, :]], dim=0)\n",
    "        y_test = torch.cat([y_test, y_train[best_obs_val_idx].unsqueeze(0)], dim=0)\n",
    "        y_train = torch.cat([y_train[0:best_obs_val_idx], y_train[best_obs_val_idx + 1:]], dim=0)\n",
    "        if maximize:\n",
    "            # Upadting best obs value\n",
    "            print('Updating best observed value')\n",
    "            best_obs_val = y_train.max()\n",
    "        else:\n",
    "            print('Updating best observed value')\n",
    "            best_obs_val = y_train.min()\n",
    "\n",
    "    best_obs_val_batch = []\n",
    "    best_obs_val_batch.append(best_obs_val)\n",
    "\n",
    "    selected_latent_batch = []\n",
    "    selected_obs_batch = []\n",
    "\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = ExactGPModel(latents_train_stand, y_train.squeeze(), likelihood, 'Matern')\n",
    "    acq_fn = ExpectedImprovement(model=model, best_f=best_obs_val_batch[-1], maximize=maximize)\n",
    "\n",
    "    for batch_num in range(1, bo_n_batches + 1):\n",
    "        # Train the model on the initial training data. Then update it with points chosen by the acq. fn from the test set\n",
    "        if (batch_num - 1)%bo_n_updates == 0:\n",
    "            if verbose: print(f'Updating the model.')\n",
    "            model, likelihood = train_model(latents_train_stand, y_train, epochs_train_GP, lr_GP)\n",
    "\n",
    "        # Here we get the new latent, new observation and the updated latent and observation test sets with that latent and obs removed.\n",
    "        latent_new, y_new, match_idx, latents_test_stand, y_test = optimize_acq_fn_and_get_observation(acq_fn, latents_test_stand, y_test)\n",
    "        \n",
    "        # Add the latent and observation to training set.\n",
    "        latents_train_stand = torch.cat([latents_train_stand, latent_new], dim=0)\n",
    "        y_train = torch.cat([y_train, y_new], dim=0)\n",
    "\n",
    "        # Track the latent and corresponding obs\n",
    "        selected_latent_batch.append(latent_new)\n",
    "        selected_obs_batch.append(y_new)\n",
    "\n",
    "        if maximize:\n",
    "            best_obs_val_batch.append(y_train.max())\n",
    "        else:\n",
    "            best_obs_val_batch.append(y_train.min())\n",
    "\n",
    "        # Define acquisition function with updated GP model and best observed obs so far.\n",
    "        acq_fn = ExpectedImprovement(model=model, best_f=best_obs_val_batch[-1], maximize=maximize)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Batch {batch_num}, best obs val {best_obs_val_batch[-1]}')\n",
    "\n",
    "    t1 = time.monotonic()\n",
    "    print(f' Time = {round(t1 - t0, 3)}')\n",
    "    best_obs_val_hist.append(best_obs_val_batch)\n",
    "    selected_latent_hist.append(selected_latent_batch)\n",
    "    selected_obs_hist.append(selected_obs_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec8cdc",
   "metadata": {},
   "source": [
    "### Comparing against other bayesian optimization models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e9acb",
   "metadata": {},
   "source": [
    "#### This is Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55128cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the model and get the latents and their predictions\n",
    "x_torch_all = torch.from_numpy(dataset[:, 0:(len(descriptors + latent_col_names))]).to(dtype=torch.float32)\n",
    "# Using the true values for prediction\n",
    "y_true = torch.from_numpy(dataset[:, len(descriptors + latent_col_names)]).to(dtype=torch.float32)\n",
    "\n",
    "maximize = True\n",
    "verbose = False\n",
    "bo_n_trials = 200\n",
    "save_filename1 = 'bo_results_random_sampling_max.csv'\n",
    "save_filename2 = 'batches_to_reach_optimal_random_sampling_max.csv'\n",
    "save_filename3 = 'selected_latent_hist_random_sampling_max.npy'\n",
    "total_number_of_materials_in_dataset = 240\n",
    "bo_n_batches = total_number_of_materials_in_dataset\n",
    "print(f'bo_n_batches = {bo_n_batches}')\n",
    "\n",
    "best_obs_val_hist = []\n",
    "selected_latent_hist = []\n",
    "selected_obs_hist = []\n",
    "for trial_num in range(1, bo_n_trials + 1):\n",
    "    t0 = time.monotonic()\n",
    "\n",
    "    best_obs_val_batch = []\n",
    "    selected_latent_batch = []\n",
    "    selected_obs_batch = []\n",
    "\n",
    "    x_torch_all_test = copy.deepcopy(x_torch_all)\n",
    "    y_true_test = copy.deepcopy(y_true)\n",
    "\n",
    "    for batch_num in range(1, bo_n_batches + 1):\n",
    "        print(np.arange(0, x_torch_all_test.shape[0]))\n",
    "        random_idx = random.choice(np.arange(0, x_torch_all_test.shape[0]))\n",
    "        latent_new = x_torch_all_test[random_idx, :].unsqueeze(0)\n",
    "        y_new = y_true_test[random_idx].unsqueeze(0)\n",
    "        match_idx = (x_torch_all_test == latent_new).all(dim = 1)\n",
    "        # Find where it matches in y_test\n",
    "        x_torch_all_test = x_torch_all_test[~match_idx]\n",
    "        y_true_test = y_true_test[~match_idx]\n",
    "\n",
    "        selected_latent_batch.append(latent_new)\n",
    "        selected_obs_batch.append(y_new)\n",
    "\n",
    "        if maximize:\n",
    "            best_obs_val_batch.append(max(selected_obs_batch))\n",
    "        else:\n",
    "            best_obs_val_batch.append(min(selected_obs_batch))\n",
    "\n",
    "    t1 = time.monotonic()\n",
    "    print(f' Time = {round(t1 - t0, 3)}')\n",
    "    best_obs_val_hist.append(best_obs_val_batch)\n",
    "    selected_latent_hist.append(selected_latent_batch)\n",
    "    selected_obs_hist.append(selected_obs_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0118a8",
   "metadata": {},
   "source": [
    "#### This is using SMAC BlackBoxFacade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the model and get the latents and their predictions\n",
    "x_torch_all = torch.from_numpy(dataset[:, 0:(len(descriptors + latent_col_names))]).to(dtype=torch.float32)\n",
    "# Using the true values for prediction\n",
    "y_true = torch.from_numpy(dataset[:, len(descriptors + latent_col_names)]).to(dtype=torch.float32)\n",
    "\n",
    "latents_stand = x_torch_all\n",
    "\n",
    "# Sanity check : Identify row indices where the latent values are the same in latents\n",
    "duplicate_latent_indices = []\n",
    "for i in range(latents_stand.shape[0]):\n",
    "    for j in range(i + 1, latents_stand.shape[0]):\n",
    "        if torch.equal(latents_stand[i], latents_stand[j]):\n",
    "            duplicate_latent_indices.append((i, j))\n",
    "if len(duplicate_latent_indices) > 0:\n",
    "    print(f'Found duplicate latent representations at indices: {duplicate_latent_indices}')\n",
    "    raise ValueError(\"Duplicate latent representations found.\")\n",
    "\n",
    "best_obs_val_hist = []\n",
    "selected_latent_hist = []\n",
    "selected_obs_hist = []\n",
    "for trial_num in range(1, bo_n_trials + 1):\n",
    "    t0 = time.monotonic()\n",
    "\n",
    "    print(f' ----- Trial {trial_num} ----- ')\n",
    "    latents_train_stand, latents_test_stand, y_train, y_test = train_test_split(latents_stand, y_true, \n",
    "                                                                    test_size=test_size,\n",
    "                                                                    random_state=trial_num)\n",
    "    print(f'Train size: {latents_train_stand.shape[0]}, Test size: {latents_test_stand.shape[0]}')\n",
    "\n",
    "    # check for max in preds\n",
    "    if maximize : \n",
    "        best_obs_val = y_train.max()\n",
    "        best_obs_val_idx = y_train.argmax()\n",
    "        optimal_soln = torch.cat([y_train, y_test]).max()\n",
    "    else:\n",
    "        best_obs_val = y_train.min()\n",
    "        best_obs_val_idx = y_train.argmin()\n",
    "        optimal_soln = torch.cat([y_train, y_test]).min()\n",
    "\n",
    "    if best_obs_val.eq(optimal_soln):\n",
    "        if maximize:\n",
    "            print(f'Max in training set. Removing it. Max value is {optimal_soln}')\n",
    "        else:\n",
    "            print(f'Min in training set. Removing it. Min value is {optimal_soln}')\n",
    "        # removing from train and adding to test\n",
    "        latents_test_stand = torch.cat([latents_test_stand, latents_train_stand[best_obs_val_idx, :].unsqueeze(0)], dim=0)\n",
    "        latents_train_stand = torch.cat([latents_train_stand[0:best_obs_val_idx, :], latents_train_stand[best_obs_val_idx + 1:, :]], dim=0)\n",
    "        y_test = torch.cat([y_test, y_train[best_obs_val_idx].unsqueeze(0)], dim=0)\n",
    "        y_train = torch.cat([y_train[0:best_obs_val_idx], y_train[best_obs_val_idx + 1:]], dim=0)\n",
    "        if maximize:\n",
    "            # Upadting best obs value\n",
    "            print('Updating best observed value')\n",
    "            best_obs_val = y_train.max()\n",
    "        else:\n",
    "            print('Updating best observed value')\n",
    "            best_obs_val = y_train.min()\n",
    "\n",
    "    best_obs_val_batch = []\n",
    "    best_obs_val_batch.append(best_obs_val)\n",
    "\n",
    "    selected_latent_batch = []\n",
    "    selected_obs_batch = []\n",
    "\n",
    "    scenario = Scenario(\n",
    "        configspace=None,\n",
    "        name='smac',\n",
    "        output_directory='smac_output',\n",
    "        deterministic=False,\n",
    "        objectives=None,\n",
    "        n_trials=100,\n",
    "        seed=trial_num,\n",
    "        n_workers=1\n",
    "    )\n",
    "\n",
    "    # Initialize SMAC Gaussian Process model\n",
    "    train_x = latents_train_stand.numpy()\n",
    "    train_y = y_train.numpy().reshape(-1, 1)\n",
    "    test_x = latents_test_stand.numpy()\n",
    "    test_y = y_test.numpy().reshape(-1, 1)\n",
    "\n",
    "    \n",
    "    \n",
    "    bbf = BlackBoxFacade(scenario)\n",
    "\n",
    "    # smac_gp = GaussianProcess(scenario)\n",
    "    # acq_fn = EI(model=smac_gp, best_f=best_obs_val_batch[-1], maximize=maximize)\n",
    "\n",
    "    for batch_num in range(1, bo_n_batches + 1):\n",
    "        \n",
    "        if (batch_num - 1)%bo_n_updates == 0:\n",
    "            if verbose: print(f'Updating the model.')\n",
    "            # smac_gp.train(train_x, train_y)\n",
    "\n",
    "\n",
    "        # Get new latent and observation\n",
    "        candidates = acq_fn.optimize(test_x)\n",
    "        latent_new = torch.from_numpy(candidates[0]).to(dtype=torch.float32).unsqueeze(0)\n",
    "        # Find index where it matches in x_test\n",
    "        match_idx = (torch.from_numpy(test_x).to(dtype=torch.float32) == latent_new).all(dim = 1)\n",
    "        # Find where it matches in y_test\n",
    "        y_new = torch.from_numpy(test_y).to(dtype=torch.float32)[match_idx] \n",
    "        # Remove from the candidate from the test\n",
    "        test_x = test_x[~match_idx.numpy()]\n",
    "        test_y = test_y[~match_idx.numpy()]\n",
    "\n",
    "        # Add the latent and observation to training set.\n",
    "        train_x = np.vstack([train_x, latent_new.numpy()])\n",
    "        train_y = np.vstack([train_y, y_new.numpy()])\n",
    "        latents_train_stand = torch.from_numpy(train_x).to(dtype=torch.float32)\n",
    "        y_train = torch.from_numpy(train_y).to(dtype=torch.float32).squeeze()\n",
    "\n",
    "        if maximize:\n",
    "            best_obs_val_batch.append(y_train.max())\n",
    "        else:\n",
    "            best_obs_val_batch.append(y_train.min())\n",
    "        \n",
    "        # Track the latent and corresponding obs\n",
    "        selected_latent_batch.append(latent_new)\n",
    "        selected_obs_batch.append(y_new)\n",
    "\n",
    "        # Update acquisition function with updated model and best observed value so far\n",
    "        acq_fn = EI(model=smac_gp, best_f=best_obs_val_batch[-1], maximize=maximize)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Batch {batch_num}, best obs val {best_obs_val_batch[-1]}')\n",
    "\n",
    "    t1 = time.monotonic()\n",
    "    print(f' Time = {round(t1 - t0, 3)}')\n",
    "    best_obs_val_hist.append(best_obs_val_batch)\n",
    "    selected_latent_hist.append(selected_latent_batch)\n",
    "    selected_obs_hist.append(selected_obs_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset x_torch_all and selected_latent_hist to numpy arays\n",
    "np.save('latents_stand.npy', latents_stand.numpy())\n",
    "np.save(save_filename3, np.array(selected_latent_hist, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay the 100 trials in line plot with varying shades of blue\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(best_obs_val_hist)):\n",
    "    plt.plot(range(1, len(best_obs_val_hist[i]) + 1), best_obs_val_hist[i], color='blue', alpha=0.1)\n",
    "    # Mark the initial best observed value\n",
    "    plt.scatter(1, best_obs_val_hist[i][0], color='red', s=10)\n",
    "\n",
    "best_obs_val_hist_arr = np.array(best_obs_val_hist)\n",
    "print(np.mean(best_obs_val_hist_arr, axis=0).shape)\n",
    "plt.plot(range(1, len(best_obs_val_hist_arr[0]) + 1), np.mean(best_obs_val_hist_arr, axis=0), color='orange', label='Mean of 200 trials', linewidth=2)\n",
    "plt.legend()\n",
    "\n",
    "# for i in range(len(selected_obs_hist)):\n",
    "#     selected_obs_concat = torch.cat(selected_obs_hist[i]).numpy()\n",
    "#     plt.scatter(range(1, len(selected_obs_concat) + 1), selected_obs_concat, color='blue', s=10, alpha=0.1)\n",
    "plt.xlabel('Number of evaluated design candidates', fontsize=14)\n",
    "plt.ylabel('Best Observed Value', fontsize=14)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87138c46",
   "metadata": {},
   "source": [
    "### Percent space explored plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the hist of best obs val in a numpy array. \n",
    "best_obs_val_each_batch_df = pd.DataFrame()\n",
    "# Store the index of max val in each trial in a list\n",
    "batches_to_reach_optimal = []\n",
    "for trial in range(1, bo_n_trials + 1):\n",
    "    best_obs_val_each_batch_df = pd.concat([best_obs_val_each_batch_df, \n",
    "                                            pd.DataFrame(best_obs_val_hist[trial - 1], columns=[f'trial{trial}'])], axis=1)\n",
    "    if maximize:\n",
    "        # Need to add the initial training set size. \n",
    "        # Another point is that we add the best obs val from the initial training set. So the first entry is not actually selected by the acq. fn.\n",
    "        # This is fine since anyways argmax index counting starts from 0.\n",
    "        # print(np.round((1 - test_size)*latents_stand.shape[0], 1))\n",
    "        batches_to_reach_optimal.append((np.round((1 - test_size)*latents_stand.shape[0], 1)\n",
    "                                    + best_obs_val_each_batch_df[f'trial{trial}'].argmax())/total_number_of_materials_in_dataset*100)\n",
    "        \n",
    "        # # This is only for random sampling\n",
    "        # batches_to_reach_optimal.append((best_obs_val_each_batch_df[f'trial{trial}'].argmax())/total_number_of_materials_in_dataset*100)\n",
    "    else:\n",
    "        batches_to_reach_optimal.append((np.round((1 - test_size)*latents_stand.shape[0], 1)\n",
    "                                         + best_obs_val_each_batch_df[f'trial{trial}'].argmin())/total_number_of_materials_in_dataset*100)\n",
    "        \n",
    "        # # This is only for random sampling\n",
    "        # batches_to_reach_optimal.append((best_obs_val_each_batch_df[f'trial{trial}'].argmin())/total_number_of_materials_in_dataset*100)\n",
    "\n",
    "best_obs_val_each_batch_df.to_csv(save_filename1, index=False)\n",
    "    \n",
    "# Plot box plot and strip plot of iter_to_reach_max\n",
    "batches_to_reach_optimal_df = pd.DataFrame(batches_to_reach_optimal, columns=['value'])\n",
    "batches_to_reach_optimal_df['method'] = 'SimpleBO'\n",
    "batches_to_reach_optimal_df.to_csv(save_filename2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc203336",
   "metadata": {},
   "source": [
    "### Decode the latents to find the design conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last 4 latents before the max/min obs value\n",
    "choose_idx = 23\n",
    "if maximize:\n",
    "    last_latents = chosen_latents[max_idx+choose_idx:max_idx+choose_idx+1]\n",
    "else:\n",
    "    last_latents = chosen_latents[min_idx-5:min_idx-4]\n",
    "\n",
    "# Decode the latents using AE1 decoder\n",
    "loaded_ae.eval()\n",
    "inp = torch.from_numpy(last_latents).to(dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    for i, layer in enumerate(loaded_ae.ae_modules['latents_predictor']):\n",
    "        if i == 0:\n",
    "            out1_ae2 = layer(inp)\n",
    "        else:\n",
    "            out1_ae2 = layer(out1_ae2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, layer in enumerate(loaded_ae.ae_modules['solvent_predictor']):\n",
    "        if i == 0:\n",
    "            out2_ae2 = layer(inp)\n",
    "        else:\n",
    "            out2_ae2 = layer(out2_ae2)\n",
    "\n",
    "print(\"Solvent Categories : 'DMSO', 'THTO', 'DMF', 'NMP', 'ACETONE', 'METHA', 'GBL', 'NITRO'\")\n",
    "print(torch.round(torch.softmax(out2_ae2, dim=1), decimals=3))\n",
    "print(chosen_obs_vals[max_idx+choose_idx:max_idx+choose_idx+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f9d71",
   "metadata": {},
   "source": [
    "### Loading AE1 to get the deisgn conditions for the perovskite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64334928",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 5\n",
    "saved_module_name = 'AE1'\n",
    "latent_dim = 9\n",
    "module_params = {'name':'AE1', \n",
    "                    'modules':{\n",
    "\n",
    "                        'encoder':{\n",
    "                            'input_dim':15,\n",
    "                            'output_dim':latent_dim, \n",
    "                            'hidden_dim':25, \n",
    "                            'hidden_layers':1, \n",
    "                            'hidden_activation':None, \n",
    "                            'output_activation':None, \n",
    "                            'layer_kernel_init':'xavier_normal', \n",
    "                            'layer_bias_init':'zeros', \n",
    "                            },\n",
    "\n",
    "                        'bandgaps_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':1,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':torch.nn.ReLU(),\n",
    "                            'output_activation':torch.nn.ReLU(),\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'},\n",
    "\n",
    "                        'A_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':5,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':None,\n",
    "                            'output_activation':None,\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        },\n",
    "\n",
    "                        'B_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':6,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':None,\n",
    "                            'output_activation':None,\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        },\n",
    "\n",
    "                        'X_predictor':{\n",
    "                            'input_dim':latent_dim,\n",
    "                            'output_dim':3,\n",
    "                            'hidden_dim':25,\n",
    "                            'hidden_layers':1,\n",
    "                            'hidden_activation':None,\n",
    "                            'output_activation':None,\n",
    "                            'layer_kernel_init':'xavier_normal',\n",
    "                            'layer_bias_init':'zeros'\n",
    "                        }\n",
    "\n",
    "                    }}\n",
    "\n",
    "loaded_state_dict = f'runs/nestedae_{saved_module_name}_bandgaps_THEN_perov_solv_BE/nestedae_fold{fold_num}'\n",
    "loaded_ae2 = AE(module_params)\n",
    "loaded_ae2.load_state_dict(torch.load(loaded_state_dict))\n",
    "loaded_ae2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f958fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = out1_ae2\n",
    "with torch.no_grad():\n",
    "    for i, layer in enumerate(loaded_ae2.ae_modules['A_predictor']):\n",
    "        if i == 0:\n",
    "            out1_ae1 = layer(inp)\n",
    "        else:\n",
    "            out1_ae1 = layer(out1_ae1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, layer in enumerate(loaded_ae2.ae_modules['B_predictor']):\n",
    "        if i == 0:\n",
    "            out2_ae1 = layer(inp)\n",
    "        else:\n",
    "            out2_ae1 = layer(out2_ae1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, layer in enumerate(loaded_ae2.ae_modules['X_predictor']):\n",
    "        if i == 0:\n",
    "            out3_ae1 = layer(inp)\n",
    "        else:\n",
    "            out3_ae1 = layer(out3_ae1)\n",
    "\n",
    "print(\"A site Categories : 'K', 'Rb', 'Cs', 'MA', 'FA'\")\n",
    "print(torch.round(torch.softmax(out1_ae1, dim=1), decimals=3))\n",
    "print(\"B site Categories : 'Ca', 'Sr', 'Ba', 'Ge', 'Sn', 'Pb'\")\n",
    "print(torch.round(torch.softmax(out2_ae1, dim=1), decimals=3))\n",
    "print(\"X site Categories : 'Cl', 'Br', 'I'\")\n",
    "print(torch.round(torch.softmax(out3_ae1, dim=1), decimals=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NestedAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
