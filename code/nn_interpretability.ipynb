{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67fc96aa-67d8-4aca-8ff5-77f162088b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "import copy\n",
    "import joblib\n",
    "import shutil\n",
    "import shap\n",
    "import seaborn as sns\n",
    "\n",
    "# User defined libraries\n",
    "from utils.custom_utils import set_global_random_seed\n",
    "from utils.dataset_utils import create_preprocessed_datasets\n",
    "# from inputs_3scale_perov_data import nn_inputs, dataset_inputs, train_inputs\n",
    "from inputs_syn_data.dataset_inputs import list_of_nn_datasets_dict\n",
    "from inputs_syn_data.nn_inputs import list_of_nn_params_dict\n",
    "from inputs_syn_data.train_inputs import list_of_nn_train_params_dict\n",
    "\n",
    "seed = 10\n",
    "set_global_random_seed(seed)\n",
    "\n",
    "# Plotting parameters\n",
    "fig_aspect_ratio = 1/1.3\n",
    "# Font style\n",
    "plt.rcParams.update({\n",
    "\"text.usetex\":True,\n",
    "\"font.family\":\"serif\",\n",
    "\"font.serif\":[\"Computer Modern Roman\"]})\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.major.size'] = 5\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.minor.size'] = 5\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.major.size'] = 5\n",
    "plt.rcParams['ytick.major.width'] = 1\n",
    "plt.rcParams['ytick.minor.size'] = 5\n",
    "plt.rcParams['ytick.minor.width'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "# Create a Directed Graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "z_labels = ['$z_1$', '$z_2$', '$z_3$', '$z_4$', '$z_5$', '$z_6$', '$z_7$', '$z_8$']\n",
    "f1_labels = ['$f_1$', '$f_2$', '$f_3$', '$f_4$']\n",
    "\n",
    "for z in z_labels:\n",
    "    for f in f1_labels:\n",
    "        G.add_edge(z, f)\n",
    "\n",
    "G.add_edge('$f_1$', '$f_5$')\n",
    "G.add_edge('$f_2$', '$f_5$')\n",
    "G.add_edge('$f_3$', '$f_5$')\n",
    "G.add_edge('$f_4$', '$f_5$')\n",
    "\n",
    "# Create a layout for our nodes \n",
    "pos = {'$z_1$': (1, 1), '$z_2$': (1, 2), '$z_3$': (1, 3), '$z_4$': (1, 4), \n",
    "       '$z_5$': (1, 5), '$z_6$': (1, 6), '$z_7$': (1, 7), '$z_8$': (1, 8),\n",
    "       '$f_1$': (2, 1.5), '$f_2$': (2, 3.5), '$f_3$': (2, 5.5), '$f_4$': (2, 7.5),\n",
    "       '$f_5$': (3, 4.5)}\n",
    "\n",
    "# Draw the graph using the layout\n",
    "nx.draw(G, pos, with_labels=True, arrowsize=15, node_size=1000, font_size=20, node_color='skyblue', font_color='black', font_weight='bold')\n",
    "\n",
    "plt.subplots_adjust(left=0, right=2, top=2, bottom=0)\n",
    "# Save the plot to pdf \n",
    "plt.savefig('multiscale_graph.pdf', dpi=300, bbox_inches='tight', )\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f4600c",
   "metadata": {},
   "source": [
    "## Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276352fd",
   "metadata": {},
   "source": [
    "### Plotting model performance vs latent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0cc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Store the below loss values in .npy arrays and move them to the model directory\n",
    "\n",
    "# For RL paper\n",
    "# # Grid data - UnsupervisedSimpleAE 1\n",
    "# latent_space = [1, 2, 4, 6, 8]\n",
    "# total_val_loss = [0.001025053068588022, 0.001063714546035044, 0.0011066086881328374, 0.0013437549932859838, 0.0009386805177200586]\n",
    "# total_train_loss = [0.007932848995551467, 0.008279352798126638, 0.004237618821207434, 0.004374776501208544, 0.004906855116132647]\n",
    "\n",
    "# # Grid data - Nonlinf5 SupervisedSimpleAE 2\n",
    "# latent_space = [1, 2, 4, 6]\n",
    "# total_val_loss = [0.7463283464312553, 0.2002287097275257, 0.1977713629603386, 0.1788929458707571]\n",
    "# total_train_loss = [0.4705591835081578, 0.14468571869656444, 0.1250611103605479, 0.13816878804937005]\n",
    "\n",
    "# # Grid data - Sumf5 - SupervisedSimpleAE 2\n",
    "# latent_space = [1, 2, 4, 6]\n",
    "# total_val_loss = [0.49156802892684937, 0.019491535145789385, 0.02250012196600437, 0.02030603913590312]\n",
    "# total_train_loss = [0.4140172880142927, 0.03587072214577347, 0.02835194836370647, 0.03037486143875867]\n",
    "\n",
    "# # Random data - UnsupervisedSimpleAE 1\n",
    "# latent_space = [2, 4, 6, 8]\n",
    "# total_val_loss = [0.6480237692594528, 0.4781555384397506, 0.24337586015462875, 0.002268550335429609]\n",
    "# total_train_loss = [0.6661303304135799, 0.4441600125283003, 0.20833437889814377, 0.007954166503623128]\n",
    "\n",
    "# # Random data - Nonlinf5 - SupervisedSimpleAE 2\n",
    "# latent_space = [1, 2, 4, 6, 8, 10, 11, 12]\n",
    "# total_val_loss = [0.7916551381349564, 0.6637141406536102, 0.5525364577770233, 0.3102440983057022, 0.19166426360607147, 0.051493472419679165, 0.021430929424241185, 0.007555923308245838]\n",
    "# total_train_loss = [0.7504490427672863, 0.6266670003533363, 0.4413919039070606, 0.30615816451609135, 0.18379461765289307, 0.05851957411505282, 0.028894496499560773, 0.016192137030884624]\n",
    "\n",
    "# # Random data - Sumf5 - SupervisedSimpleAE 2\n",
    "# # Will have to change the predictor as only prediction loss is high, reconstruction is good.\n",
    "# latent_space = [1, 2, 4, 6, 8, 10, 11, 12]\n",
    "# total_val_loss = [1.4578111469745636, 1.4702374935150146, 1.383190006017685, 1.16259, 0.99866, 0.81431, 0.75846, 0.83324]\n",
    "# total_train_loss = [1.4289479702711103, 1.2744147181510923, 1.0854754857718945, 0.96673, 0.84347, 0.72946, 0.69708, 0.66834]\n",
    "\n",
    "# Arun2024 (Input dim = 15 + 4 = 19)\n",
    "# latent_space =      [2,     4,     6,     8,     10,    12,    14]\n",
    "# pred_val_loss =     [0.206, 0.178, 0.244, 0.227, 0.148, 0.233, 0.231]\n",
    "# pred_train_loss =   [0.11,  0.076, 0.082, 0.067, 0.074, 0.071, 0.067]\n",
    "# recont_val_loss =   [0.561, 0.069, 0.055, 0.050, 0.038, 0.046, 0.056]\n",
    "# recont_train_loss = [0.234, 0.055, 0.037, 0.038, 0.037, 0.042, 0.039]\n",
    "# total_val_loss = [0.76786, 0.24755343379718917, \t0.24418, 0.27646, 0.18622, 0.27943, 0.28719]\n",
    "# total_train_loss = [0.38542, 0.16735203887741917, \t0.15132, 0.13772, 0.14288, 0.14528, 0.13803]\n",
    "\n",
    "# for 10 this is the best that can be done. Adding more layers or increasing the hidden dim or using non linea act does not work.\n",
    "# val : 0.7632711380720139\n",
    "# train : 0.7363427169620991\n",
    "run_dir = 'results_for_RL_paper'\n",
    "train_loss_matrix_path = f'../runs/{run_dir}/SupSimpleAE_2_ldim10_randomData_nonlinf5/train_total_loss.npy'\n",
    "val_loss_matrix_path = f'../runs/{run_dir}/SupSimpleAE_2_ldim10_randomData_nonlinf5/val_total_loss.npy'\n",
    "\n",
    "train_loss_matrix = np.load(train_loss_matrix_path)\n",
    "val_loss_matrix = np.load(val_loss_matrix_path)\n",
    "train_mean_losses = np.mean(train_loss_matrix, axis=0)\n",
    "train_std_losses = np.std(train_loss_matrix, axis=0)\n",
    "val_mean_losses = np.mean(val_loss_matrix, axis=0)\n",
    "val_std_losses = np.std(val_loss_matrix, axis=0)\n",
    "\n",
    "latent_space_to_sweep = [2, 4, 6, 8, 10, 12]\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "\n",
    "# Plot the latent space vs total validation loss\n",
    "plt.errorbar(latent_space_to_sweep, train_mean_losses, yerr=train_std_losses, label='train', marker='o', linestyle='-')\n",
    "plt.errorbar(latent_space_to_sweep, val_mean_losses, yerr=val_std_losses, label='val', marker='o', linestyle='-')\n",
    "# # Label the mean values on top with red color\n",
    "# for i, txt in enumerate(val_mean_losses):\n",
    "#     # plt.annotate(f'{txt:.3f}', (latent_space_to_sweep[i], val_mean_losses[i]), textcoords=\"offset points\", xytext=(0,15), ha='center', color='r')\n",
    "#     print(f'{latent_space_to_sweep[i]} : Val Std dev. : {val_std_losses[i]}')  \n",
    "#     print(f'{latent_space_to_sweep[i]} : Val Mean : {val_mean_losses[i]}')\n",
    "# for i, txt in enumerate(train_mean_losses):\n",
    "#     # plt.annotate(f'{txt:.3f}', (latent_space_to_sweep[i], train_mean_losses[i]), textcoords=\"offset points\", xytext=(0,-15), ha='center', color='b')\n",
    "#     print(f'{latent_space_to_sweep[i]} : Train Std dev. : {train_std_losses[i]}')  \n",
    "#     print(f'{latent_space_to_sweep[i]} : Train Mean : {train_mean_losses[i]}')\n",
    "# plt.plot(latent_space, total_train_loss, marker='o', linestyle='', label='train')\n",
    "# plt.plot(latent_space, total_val_loss, marker='o', linestyle='', label='val')\n",
    "plt.xlabel('Latent space dimension')\n",
    "plt.xticks(latent_space_to_sweep)\n",
    "# Set the tick labels\n",
    "plt.ylabel('Total MAE (Reconst. + Pred.)')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust the subplot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('latent_space_vs_total_loss.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52d60a",
   "metadata": {},
   "source": [
    "### Loading the torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841d9ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "                'input_dim':{\n",
    "                    'value':12\n",
    "                },\n",
    "                'hidden_dim':{\n",
    "                    'value':None\n",
    "                },\n",
    "                'latent_dim':{\n",
    "                    'value':10\n",
    "                },\n",
    "                'y_dim':{\n",
    "                    'value':0\n",
    "                },\n",
    "                'dropout':{\n",
    "                    'value':0\n",
    "                },\n",
    "                'l1_reg':{\n",
    "                    'value':0\n",
    "                },\n",
    "                'l2_reg':{\n",
    "                    'value':0.001\n",
    "                },\n",
    "                'num_layers':{\n",
    "                    'value':1\n",
    "                },\n",
    "                'activation_fn':{\n",
    "                    'value':None\n",
    "                },\n",
    "                'dec_activation_fn':{\n",
    "                    'value':None\n",
    "                },\n",
    "                'pred_activation_fn':{\n",
    "                    'value':None\n",
    "                },\n",
    "                'batch_size':{\n",
    "                    'value':10\n",
    "                },\n",
    "                'learning_rate':{\n",
    "                    'value':1e-3\n",
    "                },\n",
    "                'epochs':{\n",
    "                    'value':1000\n",
    "                }\n",
    "}\n",
    "\n",
    "from nn.models import SupervisedSimpleAE, UnsupervisedSimpleAE, UnsupervisedVAE, GMVAE\n",
    "\n",
    "# Load torch model\n",
    "ldim = parameters['latent_dim']['value']\n",
    "# model_name = f'{ae_name}_ldim{ldim}_{dataset_name}'\n",
    "# model_name = 'UnsupSimpleAE_3_ldim12_peroveff2_device_none_3_75_e2000'\n",
    "# model_name = 'SupSimpleAE_2_ldim2_gridData_nonlinf5'\n",
    "# model_name = 'best_SupSimpleAE_2_ldim2_gridData_sumf5'\n",
    "rand_sumf5_model_name = 'SupSimpleAE_2_ldim10_randomData_sumf5'\n",
    "rand_nonlinf5_model_name = 'SupSimpleAE_2_ldim10_randomData_nonlinf5'\n",
    "run_dir = 'results_for_RL_paper'\n",
    "model_type = 'SupervisedSimpleAE'\n",
    "rand_sumf5_model_path = f'../runs/{run_dir}/{rand_sumf5_model_name}/' + rand_sumf5_model_name + '.pth'\n",
    "rand_nonlinf5_model_path = f'../runs/{run_dir}/{rand_nonlinf5_model_name}/' + rand_nonlinf5_model_name + '.pth'\n",
    "if model_type == 'UnsupervisedSimpleAE':\n",
    "    model = UnsupervisedSimpleAE(parameters['input_dim']['value'],\n",
    "                                parameters['hidden_dim']['value'], \n",
    "                                parameters['dropout']['value'], \n",
    "                                parameters['latent_dim']['value'], \n",
    "                                parameters['num_layers']['value'], \n",
    "                                parameters['activation_fn']['value'],\n",
    "                                parameters['dec_activation_fn']['value'])\n",
    "elif model_type == 'SupervisedSimpleAE':\n",
    "    rand_sumf5_model = SupervisedSimpleAE(parameters['input_dim']['value'],\n",
    "                                parameters['hidden_dim']['value'], \n",
    "                                parameters['dropout']['value'], \n",
    "                                parameters['latent_dim']['value'], \n",
    "                                parameters['num_layers']['value'], \n",
    "                                parameters['activation_fn']['value'],\n",
    "                                parameters['pred_activation_fn']['value'],\n",
    "                                parameters['dec_activation_fn']['value'])\n",
    "    rand_nonlinf5_model = SupervisedSimpleAE(parameters['input_dim']['value'],\n",
    "                                parameters['hidden_dim']['value'], \n",
    "                                parameters['dropout']['value'], \n",
    "                                parameters['latent_dim']['value'], \n",
    "                                parameters['num_layers']['value'], \n",
    "                                parameters['activation_fn']['value'],\n",
    "                                parameters['pred_activation_fn']['value'],\n",
    "                                parameters['dec_activation_fn']['value'])\n",
    "elif model_type == 'VAE':\n",
    "    model = UnsupervisedVAE(parameters['input_dim']['value'],\n",
    "                            parameters['hidden_dim']['value'], \n",
    "                            parameters['dropout']['value'], \n",
    "                            parameters['latent_dim']['value'], \n",
    "                            parameters['num_layers']['value'], \n",
    "                            parameters['activation_fn']['value'])\n",
    "elif model_type == 'GMVAE':\n",
    "    model = GMVAE(parameters['input_dim']['value'],\n",
    "                parameters['y_dim']['value'],\n",
    "                parameters['hidden_dim']['value'], \n",
    "                parameters['dropout']['value'], \n",
    "                parameters['latent_dim']['value'], \n",
    "                parameters['num_layers']['value'], \n",
    "                parameters['activation_fn']['value'])\n",
    "else:\n",
    "    pass\n",
    "# print(torch.load(model_path))\n",
    "\n",
    "rand_sumf5_model.load_state_dict(torch.load(rand_sumf5_model_path))\n",
    "rand_nonlinf5_model.load_state_dict(torch.load(rand_nonlinf5_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce370e",
   "metadata": {},
   "source": [
    "#### Save the latents to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the latents to .csv file\n",
    "# z, pred, _, _ = model(torch.cat((torch.from_numpy(X1_final), torch.from_numpy(X2_final)), dim=1))\n",
    "z, _, _, _, _ = model(torch.concat((torch.from_numpy(X1_final), torch.from_numpy(X2_final), torch.from_numpy(X3_final), torch.from_numpy(X4_final)), dim=1))\n",
    "latent_filename = f'latents_from_{dataset_name}'\n",
    "\n",
    "# Data folder paths and file names\n",
    "dataset_folder_name = 'synthetic_dataset'\n",
    "dataset_file_name = 'synthetic_data_gridSamples_200_sumf5.csv'\n",
    "new_dataset_file_name = 'synthetic_data_gridSamples_200_sumf5_with_ae1_latents_concat.csv'\n",
    "run_folder_name = 'perovskite_multiscale_dataset_v2'\n",
    "model_folder_name = 'best_SupSimpleAE_1_ldim4_arun2024'\n",
    "AE_number = '1'\n",
    "\n",
    "dataset_folder = '../datasets/' + dataset_folder_name\n",
    "dataset_file = dataset_folder + '/' + dataset_file_name\n",
    "concatenated_dataset_file = dataset_folder + '/' + new_dataset_file_name\n",
    "\n",
    "run_folder = '../runs/' + run_dir\n",
    "latent_file = run_folder + '/' + model_name + '/' + latent_filename + '.csv'\n",
    "\n",
    "# Save the latents to .csv file\n",
    "z_df = pd.DataFrame(z.detach().numpy())\n",
    "# pred_df = pd.DataFrame(pred.detach().numpy())\n",
    "# Conccaetnate the latents and the predictions\n",
    "# z_pred_df = pd.concat([z_df, pred_df], axis=1)\n",
    "# z_pred_df = pd.DataFrame(z.detach().numpy())\n",
    "z_df.to_csv(latent_file, index=False, header=False) \n",
    "\n",
    "# latents = pd.read_csv(latent_file, header=None, skiprows=None)\n",
    "# data = pd.read_csv(dataset_file)\n",
    "\n",
    "# for i in range(len(latents.columns)):\n",
    "#     data['AE'+ AE_number +'_latent_'+str(i)] = latents[i]\n",
    "\n",
    "# data.to_csv(concatenated_dataset_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba29d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find which 'z'is closest to the 'z' provided below\n",
    "# z_query = torch.tensor([0.26077228, 3.95919058, 0., 0., 0., 0., 1.19784881, 1.33860231], dtype=torch.float32)\n",
    "# z_query = z_query.unsqueeze(dim=0)\n",
    "# z_query = z_query.repeat(z.shape[0], 1)\n",
    "# dist = torch.nn.PairwiseDistance(p=1)\n",
    "# distances = dist(z, z_query)\n",
    "# closest_idx = torch.argmin(distances).item()\n",
    "# print(torch.min(distances))\n",
    "# print(f'Closest z to the query z is at index : {closest_idx}')\n",
    "# print(f'Closest z to the query z is : {z[closest_idx]}')\n",
    "# print(f'Reconst for query z is : {reconst[closest_idx]}')\n",
    "# print(f'INvert scaling for reconst : {scaler_X.inverse_transform(reconst[closest_idx].detach().numpy().reshape(1, -1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor(X_scaled_np32[1, :], requires_grad=True)\n",
    "print(input)\n",
    "baseline = torch.zeros_like(input)\n",
    "print(baseline)\n",
    "print(model(baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee664e",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9a2f2",
   "metadata": {},
   "source": [
    "### 1. Using Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1fe308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, DeepLift, InputXGradient, Saliency\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def model_wrapper(input):\n",
    "    z, pred, reconst = model(input)\n",
    "    # z, pred, reconst1, reconst2 = model(input)\n",
    "    # z, recon1, recon2, recon3, recon4 = model(input)\n",
    "    return pred\n",
    "\n",
    "# Create an instance of the IntegratedGradients class\n",
    "sal = Saliency(model_wrapper)\n",
    "ig = IntegratedGradients(model_wrapper)\n",
    "ixg = InputXGradient(model_wrapper)\n",
    "dl = DeepLift(model_wrapper)\n",
    "\n",
    "input = torch.from_numpy(X1_final)\n",
    "# input = torch.cat((torch.from_numpy(X1_scaled_np32), torch.from_numpy(X2_np32)), dim=1)\n",
    "# input = torch.concat((torch.from_numpy(X1_final), torch.from_numpy(X2_final), torch.from_numpy(X3_final), torch.from_numpy(X4_final)), dim=1)\n",
    "\n",
    "# attr_sal = sal.attribute(input, target=None)\n",
    "# attr_sal_np = attr_sal.detach().numpy()\n",
    "# attr_ixg = ig.attribute(input, baselines=0, target=None)\n",
    "# attr_ixg_np = attr_ixg.detach().numpy()\n",
    "attr_ig = ig.attribute(input, baselines=0, target=None)\n",
    "attr_ig_np = attr_ig.detach().numpy()\n",
    "# attr_dl = dl.attribute(input, baselines=0, target=None)\n",
    "# attr_dl_np = attr_dl.detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "# # Plot the attributions\n",
    "using_boxplot = True\n",
    "if using_boxplot:\n",
    "    # Label the means on top of the bar in the boxplot\n",
    "    for i in range(attr_ig_np.shape[1]):\n",
    "        # plt.text(i+1, 1.5, '{:0.1f}'.format(np.mean(attr_ig_np[:, i])), ha='center', va='bottom')\n",
    "        # Plot the standard deviation\n",
    "        plt.text(i+1, 1.6, '{:0.1f}'.format(np.std(attr_ig_np[:, i])), ha='center', va='bottom')\n",
    "    # Plot the means and standard deviations of the attributions for each feature\n",
    "    plt.boxplot(attr_ig_np, showmeans=True, meanline=True)\n",
    "    ax.set_xticklabels(descriptors1, rotation=90)\n",
    "    plt.title('Integrated Gradients calc. wrt pred')\n",
    "    plt.show()\n",
    "else:\n",
    "    means = np.mean(attr_ig_np, axis=0)\n",
    "    std_dev = np.std(attr_ig_np, axis=0)\n",
    "    num_feats = attr_ig_np.shape[1]\n",
    "    # ax.bar(np.arange(num_feats), means, yerr=std_dev, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "    ax.scatter(np.arange(num_feats), means, label='mean', color='r', marker='.')\n",
    "    ax.errorbar(np.arange(num_feats), means, yerr=std_dev, fmt='o', capsize=5)\n",
    "    for i in range(attr_ig_np.shape[1]):\n",
    "        # plt.text(i+1, 1.5, '{:0.1f}'.format(np.mean(attr_ig_np[:, i])), ha='center', va='bottom')\n",
    "        # Plot the standard deviation\n",
    "        plt.text(i, 0.8, '{:0.1f}'.format(np.std(attr_ig_np[:, i])), ha='center', va='bottom')\n",
    "    ax.set_xticks(np.arange(num_feats))\n",
    "    # descriptors = ['DMF', 'DMF; DMSO', 'DMSO', 'DMSO; GBL', \n",
    "    #                '1', '1; 1', '1; 4', '2; 1', '3; 1', '3; 7', '4; 1', '7; 3',\n",
    "    #                '8; 1', '9; 1', 'Anisole', 'Chlorobenzene', 'Diethyl ether', \n",
    "    #                'Ethyl acetate', 'N2', 'Toluene', \n",
    "    #                'AE1_latent_0', 'AE1_latent_1', 'AE1_latent_2', 'AE1_latent_3']\n",
    "    descriptors = ['PET | ITO', 'SLG | FTO', 'SLG | ITO', \n",
    "                   'NiO-c', 'PEDOT:PSS', 'PTAA', 'Spiro-MeOTAD',\n",
    "                   'C60 | BCP', 'PCBM-60', 'PCBM-60 | BCP', 'PCBM-60 | ZnO-np',\n",
    "                   'SnO2-c', 'SnO2-np', 'TiO2-c', 'TiO2-c | TiO2-mp',\n",
    "                   'AE2_latent_0', 'AE2_latent_1', 'AE2_latent_2', 'AE2_latent_3',\n",
    "                   'AE2_latent_4', 'AE2_latent_5', 'AE2_latent_6', 'AE2_latent_7']\n",
    "    ax.set_xticklabels(descriptors, rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "\n",
    "# # Matrix Plot model weights\n",
    "# for name, param in model.encoder.named_parameters():\n",
    "#     if name == 'layers.0.weight':\n",
    "#         ax = plt.figure(figsize=(9, 8))\n",
    "#         ax = sns.heatmap(param.detach().numpy(), annot=True, fmt='.3f', cmap='coolwarm')\n",
    "#         # Remove y axis labels\n",
    "#         ax.yticks([])\n",
    "#         ax.set_title('Encoder Layer 1 Weights')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f252e",
   "metadata": {},
   "source": [
    "### 2. Pearson Correlation Coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7be71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_wrapper(input):\n",
    "    z, pred, reconst = model(input)\n",
    "    # z, pred, reconst1, reconst2 = model(input)\n",
    "    # z, recon1, recon2, recon3, recon4 = model(input)\n",
    "    return pred\n",
    "\n",
    "\n",
    "pred = model_wrapper(torch.from_numpy(X1_final))\n",
    "corrcoefs = np.corrcoef(X1_final, y=pred.detach().numpy(), rowvar=False)\n",
    "\n",
    "# In the corr coefs change all values that lie between +- threhold to 0\n",
    "threshold = 0.1\n",
    "corrcoefs[np.abs(corrcoefs) < threshold] = 0\n",
    "\n",
    "# Make a color bar with 0 value as dark gray\n",
    "# cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "# cmap.set_bad(color='black')\n",
    "import matplotlib.colors as colors\n",
    "cmap = colors.LinearSegmentedColormap.from_list('mycmap', ['blue', 'gray', 'red'])\n",
    "norm = colors.Normalize(vmin=-1, vmax=1)\n",
    "\n",
    "# Matrix plot of corr coefs \n",
    "# plt.figure(figsize=(12,10))\n",
    "# Set figure size in inches\n",
    "plt.figure(figsize=(5, 3.85))\n",
    "\n",
    "sns.heatmap(corrcoefs, annot=False, cmap=cmap, norm=norm, fmt='.2f', annot_kws={\"color\": 'black', 'size':1})\n",
    "plt.tick_params(axis='x', labelbottom=False, labeltop=True)\n",
    "\n",
    "plt.gca().xaxis.tick_top()\n",
    "\n",
    "plt.xticks(np.arange(X1_final.shape[1] + 1) + 0.5, labels=descriptors1 + ['f5_pred'], rotation=90)\n",
    "plt.yticks(np.arange(X1_final.shape[1] + 1) + 0.5, labels=descriptors1 + ['f5_pred'], rotation=0)\n",
    "\n",
    "#Adjust subplot size\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to pdf\n",
    "plt.savefig('corr_coefs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd2c57",
   "metadata": {},
   "source": [
    "### 3. Using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680a64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the inputs into one dataset, if only one input then skip\n",
    "combined = False\n",
    "background_dataset_gen = 'all_samples' # 'kmeans' or 'all_samples'\n",
    "# descriptors1 = ['$z_1$', '$z_2$', '$z_3$', '$z_4$', '$z_5$', '$z_6$', '$z_7$', '$z_8$']\n",
    "# descriptors2=['AE1_l0', 'AE1_l1', 'AE1_l2', 'AE1_l3', 'AE1_l4', 'AE1_l5', \n",
    "#                  'AE1_l6', 'AE1_l7', 'AE1_l8', 'AE1_l9', 'AE1_l10', 'AE1_l11']+\\\n",
    "#                 list(ae.datasets.variable_preprocessors['etm'].categories_[0]) +\\\n",
    "#                 list(ae.datasets.variable_preprocessors['htm'].categories_[0])\n",
    "\n",
    "# if combined:    \n",
    "#     dataset = torch.concat((ae.all_samples['latents'], ae.all_samples['etm'], ae.all_samples['htm']), dim=1)\n",
    "# else:\n",
    "#     dataset = ae.all_samples['all_props']\n",
    "\n",
    "cols = ['f1', 'f2', 'f3', 'f4', 'AE1_latent_0', 'AE1_latent_1', 'AE1_latent_2', 'AE1_latent_3', 'AE1_latent_4', 'AE1_latent_5', 'AE1_latent_6', 'AE1_latent_7']\n",
    "cols_for_plot = [r'$f_1$', r'$f_2$', r'$f_3$', r'$f_4$', r'$l^0_1$', r'$l^1_1$', r'$l^2_1$', r'$l^3_1$', r'$l^4_1$', r'$l^5_1$', r'$l^6_1$', r'$l^7_1$']\n",
    "\n",
    "rand_sumf5_dataset = pd.read_csv('../datasets/synthetic_dataset/synthetic_data_randomSamples_200_sumf5_with_ae1_latents_concat.csv', usecols=cols).to_numpy()\n",
    "rand_nonlinf5_dataset = pd.read_csv('../datasets/synthetic_dataset/synthetic_data_randomSamples_200_with_ae1_latents_concat.csv', usecols=cols).to_numpy()\n",
    "\n",
    "# # Using kmeans to cluster the data\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# if background_dataset_gen == 'kmeans':\n",
    "#     krange = np.arange(1, 10)\n",
    "#     inertia = np.zeros(len(krange))\n",
    "\n",
    "#     for k in krange:\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(dataset)\n",
    "#         if k == krange[-1]:\n",
    "#             background_dataset = kmeans.cluster_centers_\n",
    "#             # Change dtypes to np.float32\n",
    "#             background_dataset = background_dataset.astype(np.float32)\n",
    "#         inertia[k-1] = kmeans.inertia_\n",
    "    \n",
    "#     # Plot elbow plot\n",
    "#     plt.plot(krange, inertia, marker='o')\n",
    "#     plt.xlabel('Number of clusters')\n",
    "#     plt.ylabel('Inertia')\n",
    "# elif background_dataset_gen == 'all_samples':\n",
    "#     # X100 = shap.utils.sample(combined_dataset.detach().numpy(), 100)\n",
    "#     # background_dataset = shap.utils.sample(ae.all_samples['all_props'].detach().numpy(), len(ae.all_samples['all_props']))\n",
    "#     background_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc6538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_sumf5_model_wrapper(input):\n",
    "    input = torch.from_numpy(input)\n",
    "    input = torch.tensor(input, requires_grad=True, dtype=torch.float32)\n",
    "    z, pred, recon1 = rand_sumf5_model(input)\n",
    "    # z, pred, reconst1, reconst2 = model(input)\n",
    "    # z, recon1, recon2, recon3, recon4 = model(input)\n",
    "    return pred.detach().numpy()\n",
    "\n",
    "def rand_nonlinf5_model_wrapper(input):\n",
    "    input = torch.from_numpy(input)\n",
    "    input = torch.tensor(input, requires_grad=True, dtype=torch.float32)\n",
    "    z, pred, recon1 = rand_nonlinf5_model(input)\n",
    "    # z, pred, reconst1, reconst2 = model(input)\n",
    "    # z, recon1, recon2, recon3, recon4 = model(input)\n",
    "    return pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ebfce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/lw7ggcln7zx_cnkbk2zbgvm80000gn/T/ipykernel_8390/2386829701.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input, requires_grad=True, dtype=torch.float32)\n",
      "/var/folders/k2/lw7ggcln7zx_cnkbk2zbgvm80000gn/T/ipykernel_8390/2386829701.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input, requires_grad=True, dtype=torch.float32)\n",
      "/var/folders/k2/lw7ggcln7zx_cnkbk2zbgvm80000gn/T/ipykernel_8390/2386829701.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(input, requires_grad=True, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "rand_sumf5_explainer = shap.Explainer(rand_sumf5_model_wrapper, rand_sumf5_dataset, algorithm='exact', output_names='f5')\n",
    "rand_sumf5_explainer_shap = rand_sumf5_explainer(rand_sumf5_dataset)\n",
    "rand_sumf5_explainer_shap.feature_names = cols\n",
    "\n",
    "rand_nonlinf5_explainer = shap.Explainer(rand_nonlinf5_model_wrapper, rand_nonlinf5_dataset, algorithm='exact', output_names='f5')\n",
    "rand_nonlinf5_explainer_shap = rand_nonlinf5_explainer(rand_nonlinf5_dataset)\n",
    "rand_nonlinf5_explainer_shap.feature_names = cols\n",
    "\n",
    "# Kernel Explainer\n",
    "# kernel_explainer = shap.KernelExplainer(model_wrapper, data=background_dataset, link='identity', feature_names=cols)\n",
    "# shap_values = kernel_explainer.shap_values(dataset)\n",
    "# shap_values = np.squeeze(shap_values, axis=-1)\n",
    "# shap.initjs()\n",
    "# shap.summary_plot(shap_values, features=dataset, feature_names=cols, max_display=10, plot_type='dot', show=False, plot_size=(5.8, 3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['text.usetex'] = False\n",
    "shap.plots.waterfall(rand_sumf5_explainer_shap[seed], max_display=12, show=True)\n",
    "# shap.waterfall_plot(shap_values_from_explainer[seed], max_display=12, show=True)\n",
    "# Save SHAP waterfall plot to pdf\n",
    "plt.savefig('shap_waterfall_random_sumf5.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87057be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(rand_nonlinf5_explainer_shap[seed], max_display=12, show=True)\n",
    "# shap.waterfall_plot(shap_values_from_explainer[seed], max_display=12, show=True)\n",
    "# Save SHAP waterfall plot to pdf\n",
    "plt.savefig('shap_waterfall_random_nonlinf5.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e331ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f1', 'f2', 'f3', 'f4', 'AE1_latent_0', 'AE1_latent_1', 'AE1_latent_2', 'AE1_latent_3', 'AE1_latent_4', 'AE1_latent_5', 'AE1_latent_6', 'AE1_latent_7']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 316.667x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "figure_scale = 3\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9.5/figure_scale, 12/figure_scale))\n",
    "\n",
    "# Calculate the mean SHAP values\n",
    "rand_sumf5_mean_shap = np.mean(np.abs(rand_sumf5_explainer_shap.values), axis=0)\n",
    "rand_nonlinf5_mean_shap = np.mean(np.abs(rand_nonlinf5_explainer_shap.values), axis=0)\n",
    "\n",
    "print(rand_sumf5_explainer_shap.feature_names)\n",
    "\n",
    "# Sort the bars according to the magnitude of the SHAP values\n",
    "bars1 = ax1.barh(cols_for_plot, rand_sumf5_mean_shap , capsize=8)\n",
    "bars2 = ax2.barh(cols_for_plot, rand_nonlinf5_mean_shap, capsize=8)\n",
    "\n",
    "# # Add the mean SHAP values to the bars\n",
    "# for i, bar in enumerate(bars1):\n",
    "#     ax1.text(bar.get_width() + 0.75, bar.get_y() + bar.get_height()/2, f'{rand_sumf5_mean_shap[i]:.2f}', va='center')\n",
    "\n",
    "# for i, bar in enumerate(bars2):\n",
    "#     ax2.text(bar.get_width() + 500, bar.get_y() + bar.get_height()/2, f'{rand_nonlinf5_mean_shap[i]:.2f}', va='center')\n",
    "\n",
    "# Put a text box underneath the first subplot with label (a)\n",
    "ax1.text(0.96, 0.96, 'A', transform=ax1.transAxes, va='top', fontsize=15, ha='right')\n",
    "ax2.text(0.96, 0.96, 'B', transform=ax2.transAxes, va='top', fontsize=15, ha='right')\n",
    "\n",
    "# Set combined y label for both subplots\n",
    "ax1.set_ylabel('Features')\n",
    "ax2.set_ylabel('Features')\n",
    "ax2.set_xlabel('Mean ($| \\mathrm{SHAP} |$)')\n",
    "plt.savefig('mean_shap_values.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7d39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
