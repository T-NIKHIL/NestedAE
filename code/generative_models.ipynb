{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fc96aa-67d8-4aca-8ff5-77f162088b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'generative_models.ipynb'\n",
    "\n",
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccd275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../datasets/PSC_bandgaps/PSC_bandgaps_dataset.csv'\n",
    "\n",
    "dataset_df = pd.read_csv(dataset_path)\n",
    "\n",
    "bandgaps = dataset_df['Gap'].values\n",
    "elemental_properties = dataset_df.iloc[:, 19:].values\n",
    "\n",
    "# Creating a pytorch dataset\n",
    "class BandgapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, bandgaps, elemental_properties):\n",
    "        self.bandgaps = bandgaps\n",
    "        self.elemental_properties = elemental_properties\n",
    "    def __len__(self):\n",
    "        return len(self.bandgaps)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.bandgaps[idx], self.elemental_properties[idx]\n",
    "    \n",
    "# Standardize the dataset\n",
    "elemental_properties = (elemental_properties - elemental_properties.mean(axis=0)) / elemental_properties.std(axis=0)\n",
    "\n",
    "# Change the dtype to torch.float32\n",
    "bandgaps = bandgaps.astype(np.float32)\n",
    "elemental_properties = elemental_properties.astype(np.float32)\n",
    "\n",
    "# Shuffle and split the dataset into train and validation sets\n",
    "indices = np.arange(len(bandgaps))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:int(0.8*len(bandgaps))]\n",
    "val_indices = indices[int(0.8*len(bandgaps)):]\n",
    "bandgaps_train = bandgaps[train_indices]\n",
    "bandgaps_val = bandgaps[val_indices]\n",
    "elemental_properties_train = elemental_properties[train_indices]\n",
    "elemental_properties_val = elemental_properties[val_indices]\n",
    "    \n",
    "train_dataset = BandgapDataset(torch.from_numpy(bandgaps_train), torch.from_numpy(elemental_properties_train))\n",
    "val_dataset = BandgapDataset(torch.from_numpy(bandgaps_val), torch.from_numpy(elemental_properties_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3255788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "        self.mu = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.predictor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, 1),\n",
    "            self.activation\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.mu(h)\n",
    "        logvar = self.logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        y = self.predictor(z)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, y, z, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824e64a",
   "metadata": {},
   "source": [
    "Architecture notes:\n",
    "\n",
    "- Using the sampled z for both prediction and decoding helps in reconstrutcion but the bandgap prediction is high (MAE ~ 0.76 eV)\n",
    "- Epoch [900/1500], KL Loss: 0.3239647190140775, Reconst Loss: 0.9181102482466236, Pred Loss : 0.7544278908946401,  Total Loss: 1.9965028582664568\n",
    "- Using the $\\mu (X)$ instead as input to predictor does not affect the reconstruction loss much but the bandgap prediction is much better (MAE ~ 0.15 eV)\n",
    "- Epoch [1400/1500], KL Loss: 0.009583918224820227, Reconst Loss: 0.9987305798317714, Pred Loss : 0.1031322028556248,  Total Loss: 1.1114467009099127"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377d1ec",
   "metadata": {},
   "source": [
    "### Deriving the KL divergence loss for unit normal prior\n",
    "\n",
    "- Lets start with any arbitrary distribution Q(z) and minimize the KL divergence of it with a distribution P(z|X)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    D_{KL}(Q(z) || P(z|X)) & = \\int Q(z) \\log \\frac{Q(z)}{P(z|X)} dz \\\\ \\\\\n",
    "    D_{KL}(Q(z) || P(z|X)) & = E_{Q(z)}[ \\log \\frac{Q(z)}{P(z|X)} ] \\\\ \\\\\n",
    "    D_{KL}(Q(z) || P(z|X)) & = E_{Q(z)}[ \\log Q(z) - \\log P(z|X) ] \\\\ \\\\\n",
    "    D_{KL}(Q(z) || P(z|X)) & = E_{Q(z)}[ \\log Q(z) - \\log P(X|z) - \\log P(z) + \\log P(X) ] \\\\ \\\\\n",
    "    D_{KL}(Q(z) || P(z|X)) & = E_{Q(z)}[ \\log Q(z) - \\log P(X|z) - \\log P(z)] + \\log P(X) \\\\ \\\\\n",
    "    \\log P(X) - D_{KL}(Q(z) || P(z|X)) & =  E_{Q(z)}[\\log P(X|z)] - D_{KL}(Q(z) || P(z)) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Now instead of choosing any distribution for Q(z), it makes sense to choose a distribution for the z variables that depends on X. Hence we can replace Q(z) with Q(z|X).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\log P(X) - D_{KL}(Q(z|X) || P(z|X)) & =  E_{Q(z|X)}[\\log P(X|z)] - D_{KL}(Q(z|X) || P(z))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- The left hand side contains the terms that we want to maximize. The log probability density of X and an error term that measures the deviation between the approximate distribution (Q(z|X)) and the true probability distribution (P(z|X)). To note P(X) is a high dimensional intractable distribution and we don't have access to P(z|X). By having a large enough capacity for Q(z|X) we are pulling it closer to P(z|X), lower the KL divergence term until we are only optimizing for the log probability density of X. \n",
    "- The right hand side contains terms that can be optimized via gradient descent. The first term is the expected value of the log likelihood of the data given the latent variables. The second term is the KL divergence between the approximate distribution and the prior distribution. \n",
    "- Stochastic gradient descent can be performed on the right hand side by assuming some forms of the distribution. The most common form for the posterior and liklihood is a multivariate Gaussian distribution and for the prior is unit normal distribution. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    D_{KL}(N(\\mu_0, \\Sigma_0) || N(\\mu_1, \\Sigma_1)) = \\frac{1}{2} ( \\text{tr}(\\Sigma_1^{-1} \\Sigma_0) + (\\mu_1 - \\mu_0)^T \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - k + \\log \\frac{\\det \\Sigma_1}{\\det \\Sigma_0} )\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- 'k' is the dimensionality of the distribution. Substituting the prior as unit normal distribution, we get the KL divergence loss as\n",
    "$$\n",
    "\\begin{align*}\n",
    "    D_{KL}(N(\\mu (X), \\Sigma (X)) || N(O, I)) = \\frac{1}{2} ( \\text{tr}(\\Sigma (X)) + (\\mu (X))^T (\\mu (X)) - k - \\log \\det \\Sigma (X) )\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- To back propagate the errors to the the neural network that approximates Q(z|X), so that we get z's that correctly reproduce the data, we need to find a way that allows backpropagation to work. This is where the reparameterization trick comes in. It allows us to sample for 'z' while giving access to the neural networks that approximate the mean and covariance functions for  Q(z|X). $ z = \\mu (X) + \\Sigma (X) * \\epsilon $. Here $\\mu (X) and \\Sigma (X)$ are approximated by using neural networks and $\\epsilon$ is sampled from the unit normal distribution.\n",
    "- If any other distribution is to be modelled then the KL divergnce term must be modified accordingly and the appropriate reparameterization trick must be used.\n",
    "\n",
    "Reference:\n",
    "- Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014 https://arxiv.org/abs/1312.6114 (Appendix B)\n",
    "- Doersch, C. Tutorial on Variational Autoencoders. arXiv January 3, 2021. http://arxiv.org/abs/1606.05908.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5504d",
   "metadata": {},
   "source": [
    "### Trail 2 : Using a Gaussian Mixture Model as prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e7c81",
   "metadata": {},
   "source": [
    "Check out these github repos for how to code the model and the loss function :\n",
    "\n",
    "1. https://github.com/jariasf/GMVAE/tree/master\n",
    "2. https://github.com/RuiShu/vae-clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2840312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining the required layers for the GMVAE\n",
    "# # Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "# class GumbelSoftmax(torch.nn.Module):\n",
    "\n",
    "#   def __init__(self, f_dim, c_dim):\n",
    "#     super(GumbelSoftmax, self).__init__()\n",
    "#     self.logits = nn.Linear(f_dim, c_dim)\n",
    "#     self.f_dim = f_dim\n",
    "#     self.c_dim = c_dim\n",
    "     \n",
    "#   def sample_gumbel(self, shape, is_cuda=False, eps=1e-20):\n",
    "#     U = torch.rand(shape)\n",
    "#     if is_cuda:\n",
    "#       U = U.cuda()\n",
    "#     return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "#   def gumbel_softmax_sample(self, logits, temperature):\n",
    "#     y = logits + self.sample_gumbel(logits.size(), logits.is_cuda)\n",
    "#     return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "#   def gumbel_softmax(self, logits, temperature, hard=False):\n",
    "#     \"\"\"\n",
    "#     ST-gumple-softmax\n",
    "#     input: [*, n_class]\n",
    "#     return: flatten --> [*, n_class] an one-hot vector\n",
    "#     \"\"\"\n",
    "#     #categorical_dim = 10\n",
    "#     y = self.gumbel_softmax_sample(logits, temperature)\n",
    "\n",
    "#     if not hard:\n",
    "#         return y\n",
    "\n",
    "#     shape = y.size()\n",
    "#     _, ind = y.max(dim=-1)\n",
    "#     y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "#     y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "#     y_hard = y_hard.view(*shape)\n",
    "#     # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "#     y_hard = (y_hard - y).detach() + y\n",
    "#     return y_hard \n",
    "  \n",
    "#   def forward(self, x, temperature=1.0, hard=False):\n",
    "#     logits = self.logits(x).view(-1, self.c_dim)\n",
    "#     prob = F.softmax(logits, dim=-1)\n",
    "#     y = self.gumbel_softmax(logits, temperature, hard)\n",
    "#     return logits, prob, y\n",
    "\n",
    "# class GMVAE(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, latent_dim, num_components):\n",
    "#         super(GMVAE, self).__init__()\n",
    "\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.num_components = num_components\n",
    "#         self.activation = torch.nn.ReLU()\n",
    "#         # p(y) = Cat(y|pi)\n",
    "#         # p(z) = N(z|0, I)\n",
    "#         # p(x|y,z) = f(x;y,z,theta)\n",
    "\n",
    "#         # q(y|x) : Probability of seeing a label y given x\n",
    "#         self.qy_x = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_dim, hidden_dim),\n",
    "#             self.activation,\n",
    "#             torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "#             self.activation,\n",
    "#             torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "#             GumbelSoftmax(hidden_dim, num_components)\n",
    "#         )\n",
    "#         # q(z|x,y) : Probability of seeing a latent variable z given x and y\n",
    "#         self.qz_xy = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_dim + num_components, hidden_dim),\n",
    "#             self.activation,\n",
    "#             torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "#             self.activation,\n",
    "#             torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "#             self.activation\n",
    "#         )\n",
    "\n",
    "\n",
    "class GMVAE(torch.nn.Module):\n",
    "    def __init__(self, input_dim, y_dim, hidden_dim, latent_dim, model_activation_fn):\n",
    "        super(GMVAE, self).__init__()\n",
    "\n",
    "        # Note to self : This has elem props and bandgaps as input\n",
    "        self.input_dim = input_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.activation = model_activation_fn\n",
    "\n",
    "        # Inference Network\n",
    "        # q(y|x) : Based on the input, predict the class logit\n",
    "        self.qy_logit = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, y_dim)\n",
    "        )\n",
    "        self.qy = torch.nn.Softmax(dim=1)\n",
    "        # q(z|x,y) : Probability of seeing a latent based on X and class label\n",
    "        self.qz_xy = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim + y_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            self.activation\n",
    "        )\n",
    "        self.mu = torch.nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, latent_dim),\n",
    "            torch.nn.Softplus()\n",
    "        )\n",
    "        # Generative Network\n",
    "        # p(z|y) : Probability of seeing a latent given the class label\n",
    "        self.mu_prior = torch.nn.Linear(y_dim, latent_dim)\n",
    "        self.logvar_prior = torch.nn.Sequential(\n",
    "            torch.nn.Linear(y_dim, latent_dim),\n",
    "            torch.nn.Softplus()\n",
    "        )\n",
    "        # p(x|z) : Probability of seeing X given the latent\n",
    "        self.px_z = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            torch.nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        # # p(x|z,y) : Probability of seeing X given the latent and class label\n",
    "        # self.px_zy = torch.nn.Sequential(\n",
    "        #     torch.nn.Linear(latent_dim + y_dim, hidden_dim),\n",
    "        #     self.activation,\n",
    "        #     torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "        #     self.activation,\n",
    "        #     torch.nn.Linear(hidden_dim, input_dim)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Inference Net\n",
    "        # Shape : [batch_size, y_dim]\n",
    "        qy_logit = self.qy_logit(x)\n",
    "        # Note to self : This repo has Gumbel Softmax as last layer : https://github.com/jariasf/GMVAE\n",
    "        # Shape : [batch_size, y_dim]\n",
    "        qy = self.qy(qy_logit)\n",
    "\n",
    "        # Defining a tensor that will store the fixed class label for all members of the batch\n",
    "        y_ = torch.zeros([x.shape[0], self.y_dim])\n",
    "        z, mu, logvar, mu_prior, logvar_prior, reconst = [[None] * 10 for i in range(6)]\n",
    "        for i in range(self.y_dim):\n",
    "            # Add the class label to the tensor\n",
    "            y = y_ + torch.eye(self.y_dim)[i]\n",
    "            # Note to self : The generative model can take the predicted class label as input. This is what is done in the GMVAE repo\n",
    "            # Note to self : In the Rui Shu repo the class label (y) is provided as a one hot vector. \n",
    "            h = torch.cat([x, y], dim=1)\n",
    "            h = self.qz_xy(h)\n",
    "            # Shape : batch_size, latent_dim\n",
    "            mu[i] = self.mu(h)\n",
    "            logvar[i] = self.logvar(h)\n",
    "            # Note to self : Can use the reparameterization trick here instead. This gives modified M2 in Rui Shu's repo.\n",
    "            # Using the predicted mean and logvar sample from a gaussian distribution.\n",
    "            # z[i] = torch.normal(mu[i], logvar[i].exp().sqrt())\n",
    "            eps = torch.randn_like(mu[i])\n",
    "            z[i] = mu[i] + eps*torch.exp(0.5 * logvar[i])\n",
    "            # Generative Net\n",
    "            # Prior is a gaussian mixture. Using the fixed class label, the mean and logvar of the 'z' distribution are computed.\n",
    "            # The posterior distribution aims to match this distribution\n",
    "            mu_prior[i] = self.mu_prior(y)\n",
    "            logvar_prior[i] = self.logvar_prior(y)\n",
    "            # Using the sampled 'z' reconstruct the X\n",
    "            reconst[i] = self.px_z(z[i])\n",
    "        return z, mu, logvar, mu_prior, logvar_prior, reconst, qy_logit, qy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8cfc68",
   "metadata": {},
   "source": [
    "### Model training grounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf96a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnthota2\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nikhilthota/Desktop/lab/projects/SPIRAL/codes_and_datasets/T-NIKHIL/NestedAE/code/wandb/run-20240221_160050-8u8lolrw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nthota2/gmvae/runs/8u8lolrw' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/nthota2/gmvae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nthota2/gmvae' target=\"_blank\">https://wandb.ai/nthota2/gmvae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nthota2/gmvae/runs/8u8lolrw' target=\"_blank\">https://wandb.ai/nthota2/gmvae/runs/8u8lolrw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1500\n",
      "     Train Classif Loss: 0.3050380939599149, Train Labelled Loss: 40.06689712427839, Train Total Loss: 40.37193523358695\n",
      "    Val. Classif Loss: 0.004014177042464702, Val. Labelled Loss: 32.52751094163066, Val. Total Loss: 32.53152513985682\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 152\u001b[0m\n\u001b[1;32m    149\u001b[0m run \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmvae\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial1\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mhyperparams, job_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m, notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    150\u001b[0m model \u001b[38;5;241m=\u001b[39m GMVAE(hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_activation_fn\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 152\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_after\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m model_save_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmvae_trail1.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    155\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_save_name)\n",
      "Cell \u001b[0;32mIn[5], line 89\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, val_dataset, epochs, batch_size, learning_rate, print_after)\u001b[0m\n\u001b[1;32m     86\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m loss1 \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(loss2)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     87\u001b[0m     train_total_loss_per_step\u001b[38;5;241m.\u001b[39mappend(total_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 89\u001b[0m     \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# train_kl_loss_per_epoch.append(np.mean(train_kl_loss_per_step))\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# train_reconst_loss_per_epoch.append(np.mean(train_reconst_loss_per_step))\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# train_pred_loss_per_epoch.append(np.mean(train_pred_loss_per_step))\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nestedae/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nestedae/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "def log_normal(z, mu, logvar):\n",
    "    c = torch.tensor(2*np.pi, dtype=torch.float32) \n",
    "    return torch.tensor(-0.5, dtype=torch.float32)*torch.sum(torch.log(c) + logvar + (z - mu).pow(2) / logvar.exp(), dim=1)\n",
    "\n",
    "# Sum of reconstruction loss and KL divergence loss\n",
    "def labelled_loss(X, reconst, z, mu, logvar, mu_prior, logvar_prior):\n",
    "    c = torch.tensor(0.1, dtype=torch.float32)\n",
    "    return torch.nn.MSELoss(reduction='sum')(X, reconst) + log_normal(z, mu, logvar) - log_normal(z, mu_prior, logvar_prior) - torch.log(c)\n",
    "\n",
    "# Classification loss\n",
    "def cross_entropy_loss(qy_logit, qy):\n",
    "    return torch.nn.CrossEntropyLoss(reduction='mean')(qy_logit, qy)\n",
    "\n",
    "# Derived by assuming posterior is Gaussian and prior is unit normal distribution.\n",
    "def kl_divergence_loss_fn(mu, logvar):\n",
    "        return torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1), dim=0)\n",
    "\n",
    "mse_loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "mae_loss_fn = torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "train_reconst_loss_per_step = []\n",
    "train_reconst_loss_per_epoch = []\n",
    "train_pred_loss_per_step = []\n",
    "train_pred_loss_per_epoch = []\n",
    "train_kl_loss_per_step = []\n",
    "train_kl_loss_per_epoch = []\n",
    "\n",
    "val_reconst_loss_per_step = []\n",
    "val_reconst_loss_per_epoch = []\n",
    "val_pred_loss_per_step = []\n",
    "val_pred_loss_per_epoch = []\n",
    "val_kl_loss_per_step = []\n",
    "val_kl_loss_per_epoch = []\n",
    "\n",
    "train_classification_loss_per_step = []\n",
    "train_classification_loss_per_epoch = []\n",
    "train_labelled_loss_per_step = []\n",
    "train_labelled_loss_per_epoch = []\n",
    "\n",
    "val_classification_loss_per_step = []\n",
    "val_classification_loss_per_epoch = []\n",
    "val_labelled_loss_per_step = []\n",
    "val_labelled_loss_per_epoch = []\n",
    "\n",
    "train_total_loss_per_step = []\n",
    "train_total_loss_per_epoch = []\n",
    "\n",
    "val_total_loss_per_step = []\n",
    "val_total_loss_per_epoch = []\n",
    "\n",
    "def train(model, train_dataset, val_dataset, epochs, batch_size, learning_rate, print_after):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "        for i, (bandgaps, elem_props) in enumerate(train_dataloader):\n",
    "            input = torch.cat([elem_props, bandgaps.unsqueeze(dim=1)], dim=1)\n",
    "            #input = elem_props\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # z, mu, logvar, reconst, bandgap_preds = model(input)\n",
    "\n",
    "            # loss1 = kl_divergence_loss_fn(mu, logvar)\n",
    "            # loss2 = mse_loss_fn(reconst, elem_props)\n",
    "            # loss3 = mae_loss_fn(bandgap_preds, bandgaps.unsqueeze(dim=0))\n",
    "            # total_loss = loss1 + loss2 + loss3\n",
    "            \n",
    "            # train_kl_loss_per_step.append(loss1.item())\n",
    "            # train_reconst_loss_per_step.append(loss2.item())\n",
    "            # train_pred_loss_per_step.append(loss3.item())\n",
    "            # train_total_loss_per_step.append(total_loss.item())\n",
    "\n",
    "            z, mu, logvar, mu_prior, logvar_prior, reconst, qy_logit, qy = model(input)\n",
    "\n",
    "            loss1 = cross_entropy_loss(qy_logit, qy)\n",
    "            train_classification_loss_per_step.append(loss1.item())\n",
    "\n",
    "            loss2 = [None] * model.y_dim\n",
    "            for i in range(model.y_dim):\n",
    "                loss2[i] = torch.mean(qy[:, i]*labelled_loss(input, reconst[i], z[i], mu[i], logvar[i], mu_prior[i], logvar_prior[i]), dtype=torch.float32)\n",
    "            \n",
    "            train_labelled_loss_per_step.append(torch.stack(loss2).sum().item())\n",
    "            total_loss = loss1 + torch.stack(loss2).sum()\n",
    "            train_total_loss_per_step.append(total_loss.item())\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # train_kl_loss_per_epoch.append(np.mean(train_kl_loss_per_step))\n",
    "        # train_reconst_loss_per_epoch.append(np.mean(train_reconst_loss_per_step))\n",
    "        # train_pred_loss_per_epoch.append(np.mean(train_pred_loss_per_step))\n",
    "        train_classification_loss_per_epoch.append(np.mean(train_classification_loss_per_step))\n",
    "        train_labelled_loss_per_epoch.append(np.mean(train_labelled_loss_per_step))\n",
    "        train_total_loss_per_epoch.append(np.mean(train_total_loss_per_step))\n",
    "        if epoch % print_after == 0:\n",
    "            print(f'Epoch [{epoch}/{epochs}')\n",
    "            print(f'     Train Classif Loss: {train_classification_loss_per_epoch[-1]}, Train Labelled Loss: {train_labelled_loss_per_epoch[-1]}, Train Total Loss: {train_total_loss_per_epoch[-1]}')\n",
    "\n",
    "        # Run the validation loop\n",
    "        for i, (bandgaps, elem_props) in enumerate(val_dataloader):\n",
    "            # Concatenate the bandgaps and elem_props \n",
    "            input = torch.cat([elem_props, bandgaps.unsqueeze(dim=1)], dim=1)\n",
    "\n",
    "            z, mu, logvar, mu_prior, logvar_prior, reconst, qy_logit, qy = model(input)\n",
    "\n",
    "            loss1 = cross_entropy_loss(qy_logit, qy)\n",
    "            val_classification_loss_per_step.append(loss1.item())\n",
    "\n",
    "            loss2 = [None] * model.y_dim\n",
    "            for i in range(model.y_dim):\n",
    "                loss2[i] = torch.mean(qy[:, i]*labelled_loss(input, reconst[i], z[i], mu[i], logvar[i], mu_prior[i], logvar_prior[i]), dtype=torch.float32)\n",
    "            \n",
    "            val_labelled_loss_per_step.append(torch.stack(loss2).sum().item())\n",
    "            total_loss = loss1 + torch.stack(loss2).sum()\n",
    "            val_total_loss_per_step.append(total_loss.item())\n",
    "\n",
    "        # val_kl_loss_per_epoch.append(np.mean(train_kl_loss_per_step))\n",
    "        # val_reconst_loss_per_epoch.append(np.mean(train_reconst_loss_per_step))\n",
    "        # val_pred_loss_per_epoch.append(np.mean(train_pred_loss_per_step))\n",
    "        val_classification_loss_per_epoch.append(np.mean(val_classification_loss_per_step))\n",
    "        val_labelled_loss_per_epoch.append(np.mean(val_labelled_loss_per_step))\n",
    "        val_total_loss_per_epoch.append(np.mean(val_total_loss_per_step))\n",
    "        if epoch % print_after == 0:\n",
    "            print(f'    Val. Classif Loss: {val_classification_loss_per_epoch[-1]}, Val. Labelled Loss: {val_labelled_loss_per_epoch[-1]}, Val. Total Loss: {val_total_loss_per_epoch[-1]}')\n",
    "\n",
    "# input_dim = elemental_properties.shape[1]\n",
    "# hidden_dim = 50\n",
    "# latent_dim = 2\n",
    "# batch_size = 1\n",
    "# learning_rate = 1e-3\n",
    "# epochs = 1500\n",
    "# print_after = 1\n",
    "# model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "hyperparams = {\n",
    "    'model_activation_fn': torch.nn.ReLU(),\n",
    "    'input_dim': elemental_properties.shape[1] + 1,\n",
    "    'y_dim':3,\n",
    "    'hidden_dim': 50,\n",
    "    'latent_dim':2,\n",
    "    'batch_size': 1,\n",
    "    'learning_rate': 1e-3,\n",
    "    'epochs': 1500,\n",
    "}\n",
    "print_after = 100\n",
    "run = wandb.init(project='gmvae', name='trial1', config=hyperparams, job_type='training', notes='NA')\n",
    "model = GMVAE(hyperparams['input_dim'], hyperparams['y_dim'], hyperparams['hidden_dim'], hyperparams['latent_dim'], hyperparams['model_activation_fn'])\n",
    "\n",
    "train(model, train_dataset, val_dataset, hyperparams['epochs'], hyperparams['batch_size'], hyperparams['learning_rate'], print_after)\n",
    "\n",
    "model_save_name = 'gmvae_trail1.pt'\n",
    "torch.save(model.state_dict(), model_save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d823e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the 2D latent space\n",
    "#elem_props_reconst, bandgap_preds, z, mu, logvar = model(elemental_properties)\n",
    "z, mu, logvar, mu_prior, logvar_prior, reconst, qy_logit, qy = model(torch.cat([torch.from_numpy(elemental_properties, dtype=torch.float32), torch.from_numpy(bandgaps, dtype=torch.float32)], dim=1))\n",
    "\n",
    "plt.scatter(z[:, 0].detach().numpy(), z[:, 1].detach().numpy(), c=bandgaps, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aadfbb5",
   "metadata": {},
   "source": [
    "### Observations for Unit normal prior\n",
    "- What we observe from the above example is that although multivariate Gaussian distribution are useful\n",
    "    as each dimension can encode a separate DOF which results in representations that are sturctured and disentangled, \n",
    "    they are unimodal and hence cannot encode complex representations. A natural extension is to then use a different\n",
    "    prior. Gaussain Mixture Model (GMM) is the next choice.\n",
    "- Latent space is segregated into different classes.\n",
    "- However, inference is non-trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e90f3ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979b4368-d0f6-11ee-a068-7a2a0020f23d\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "print(uuid.uuid1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3180e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
